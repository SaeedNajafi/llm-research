{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-26 15:26:58.768148: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-26 15:26:59.055258: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-26 15:26:59.055326: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-26 15:26:59.086602: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-26 15:26:59.163939: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-26 15:27:00.894678: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from typing import Any, Dict, Iterator, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from bitsandbytes.optim.adamw import PagedAdamW8bit\n",
    "from src.galore_torch import GaLoreAdamW8bit\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    ")\n",
    "\n",
    "from src.base_lm import BaseLM\n",
    "from src.general_utils import DictDataset, test_loop, train_loop\n",
    "from src.model_utils import clear_cache, llama2_log_of_labels, lm_logits, mlm_log_of_labels, set_random_seed\n",
    "from src.general_utils import white_space_fix\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr 26 15:27:06 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A40          On   | 00000000:06:00.0 Off |                    0 |\n",
      "|  0%   46C    P8    35W / 300W |      2MiB / 46068MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A40          On   | 00000000:2F:00.0 Off |                    0 |\n",
      "|  0%   37C    P8    30W / 300W |      2MiB / 46068MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (0.21.3)\n",
      "Requirement already satisfied: filelock in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from huggingface_hub) (2024.2.0)\n",
      "Requirement already satisfied: requests in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from huggingface_hub) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from huggingface_hub) (4.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from requests->huggingface_hub) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from requests->huggingface_hub) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /h/snajafi/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token=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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 2\n",
    "eval_batch_size = 2\n",
    "lm_input_max_length = 4096\n",
    "lm_output_max_length = 128\n",
    "lm_top_p = 0.9\n",
    "temperature = 0.6\n",
    "learning_rate = 0.00005\n",
    "\n",
    "# folder to store models and predictions.\n",
    "model_path = \"/scratch/ssd004/scratch/snajafi/checkpoints/gemma2b-lora-hotpotqa\"\n",
    "\n",
    "# related to metric\n",
    "metric_device = \"cuda:1\"\n",
    "metric_batch_size = 2\n",
    "\n",
    "# related to lora\n",
    "r = 16\n",
    "lora_alpha = 8\n",
    "lora_dropout = 0.05\n",
    "\n",
    "# multiple answer splitter\n",
    "answer_splitter = \"[<@>]\"\n",
    "\n",
    "# Natural Instructions format for QA on hotpotqa.\n",
    "# source: https://github.com/allenai/natural-instructions/blob/master/tasks/task170_hotpotqa_answer_generation.json\n",
    "# we remove the part about using the supporting facts as we won't give the model the supporting facts.\n",
    "hotpot_question_answer_instruction = \"In this task, you are given a set of context paragraph to answer a question. Your task is to generate answer for given question based on set of context paragraphs.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load LM efficiently.\"\"\"\n",
    "\n",
    "# Make sure we have some tokens defined for the LM, if not defined in the model.\n",
    "_EXTRA_TOKENS = {\n",
    "    \"pad_token\": \"<pad>\",\n",
    "    \"mask_token\": \"<mask>\",\n",
    "    \"bos_token\": \"<bos>\",\n",
    "    \"eos_token\": \"<eos>\",\n",
    "    \"unk_token\": \"<unk>\",\n",
    "    \"cls_token\": \"<CLS>\",\n",
    "}\n",
    "\n",
    "target_modules = [\"q_proj\", \"v_proj\", \"o_proj\", \"k_proj\"]\n",
    "\n",
    "\n",
    "def load_peft_model(\n",
    "    model: PreTrainedModel,\n",
    "    adapter_name: str = \"lora\",\n",
    "    is_trainable: bool = False,\n",
    "    model_type: str = \"causal_lm\",\n",
    "    lora_target_modules: List[str] = target_modules,\n",
    ") -> torch.nn.Module:\n",
    "    \"\"\"Load a trained PEFT adapter to the base model and return the PeftModel.\n",
    "\n",
    "    Args:\n",
    "    ----\n",
    "        model: the main model.\n",
    "        num_quantized_bits: number of bits in the loaded model.\n",
    "        adapter_name: e.g. lora.\n",
    "        is_trainable: train or inference mode.\n",
    "        model_type: causal lm or seq-to-seq.\n",
    "        lora_target_modules: which modules to train with lora.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "        The PEFT model and tokenizer.\n",
    "    \"\"\"\n",
    "    if model_type == \"causal_lm\":\n",
    "        task_type = TaskType.CAUSAL_LM\n",
    "    elif model_type == \"seq_to_seq_lm\":\n",
    "        task_type = TaskType.SEQ_2_SEQ_LM\n",
    "\n",
    "    if adapter_name == \"lora\":\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=task_type,\n",
    "            inference_mode=not is_trainable,\n",
    "            r=r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            lora_dropout=lora_dropout,\n",
    "            bias=\"none\",\n",
    "            init_lora_weights=True,\n",
    "            target_modules=lora_target_modules,\n",
    "        )\n",
    "\n",
    "    peft_model = get_peft_model(model, peft_config)\n",
    "    peft_model.print_trainable_parameters()\n",
    "    return peft_model\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer(\n",
    "    model_id: str, model_type: str, model_dtype: torch.dtype, attn_implementation: str, load_in_4bit: Optional[bool] = True\n",
    ") -> Tuple[PreTrainedModel, PreTrainedTokenizer]:\n",
    "    \"\"\"Load the model and tokenizer.\n",
    "\n",
    "    Args:\n",
    "    ----\n",
    "        model_id: the id for the pre-trained model.\n",
    "        model_type: causal lm or seq_to_seq_lm.\n",
    "        model_dtype: model data type.\n",
    "        load_in_4bit: Whether to load in 4 bit quantization.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "        The model and tokenizer.\n",
    "    \"\"\"\n",
    "    # load model\n",
    "    if model_type == \"causal_lm\":\n",
    "        ModelClass = AutoModelForCausalLM\n",
    "    elif model_type == \"seq_to_seq_lm\":\n",
    "        ModelClass = AutoModelForSeq2SeqLM\n",
    "    model_args: Dict[str, Any] = {\"use_cache\": False, \"torch_dtype\": model_dtype, \"attn_implementation\": attn_implementation}\n",
    "    if load_in_4bit:\n",
    "        quant_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=model_args[\"torch_dtype\"],\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "        model_args[\"quantization_config\"] = quant_config\n",
    "    model = ModelClass.from_pretrained(\n",
    "        model_id,\n",
    "        **model_args,\n",
    "    )\n",
    "\n",
    "    # load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.add_special_tokens(_EXTRA_TOKENS)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        # extend embeddings to a multiple so we use Tensor cores\n",
    "        multiple = 64 if \"A100\" in torch.cuda.get_device_name() else 8\n",
    "        model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=multiple)\n",
    "    else:\n",
    "        raise Exception(\"No CUDA Found!\")\n",
    "\n",
    "    # re-define token ids for the model.\n",
    "    for extra_token_key, extra_token_val in _EXTRA_TOKENS.items():\n",
    "        extra_token_id = tokenizer.convert_tokens_to_ids([extra_token_val])[0]\n",
    "        model.config.__setattr__(f\"{extra_token_key}_id\", extra_token_id)\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gemma(BaseLM):\n",
    "    \"\"\"Class to implement Gemma for QA task.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        mode: str,\n",
    "        device: str,\n",
    "        seed: int = 42,\n",
    "    ) -> None:\n",
    "        super().__init__(device, \"main_lm\", seed)\n",
    "        self.device = device\n",
    "        model, tokenizer = load_model_and_tokenizer(\n",
    "            model_id=\"/model-weights/gemma-2b-it\",\n",
    "            model_type=\"causal_lm\",\n",
    "            model_dtype=torch.bfloat16,\n",
    "            attn_implementation=\"flash_attention_2\",\n",
    "            load_in_4bit=True,\n",
    "        )\n",
    "        peft_model = load_peft_model(\n",
    "            model=model,\n",
    "            adapter_name=\"lora\",\n",
    "            is_trainable=mode == \"train\",\n",
    "            model_type=\"causal_lm\",\n",
    "        )\n",
    "        self.model = peft_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.optimizer = PagedAdamW8bit(self.model.parameters(), lr=learning_rate)\n",
    "        self.scheduler = CosineAnnealingWarmRestarts(self.optimizer, T_0=10, eta_min=learning_rate / 5.0)\n",
    "\n",
    "    def prepare_text(self, texts: List[str], output_texts: List[str], instructions: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Convert texts to ids and return the dataset required for training\n",
    "        and inference.\"\"\"\n",
    "        input_texts = [f\"{instructions[idx]} {texts[idx]}\" for idx in range(len(instructions))]\n",
    "        template = \"<bos><start_of_turn>user\\n{user_input}<end_of_turn>\\n<start_of_turn>model\"\n",
    "        inputs_for_training = [\n",
    "            f\"{template.format(user_input=input_texts[idx])}\\n{output_texts[idx]}<end_of_turn>\" for idx in range(len(input_texts))\n",
    "        ]\n",
    "        inputs_for_generation = [\n",
    "            template.format(user_input=input_texts[idx]) for idx in range(len(input_texts))\n",
    "        ]\n",
    "\n",
    "        input_encodings = self.tokenizer(\n",
    "            inputs_for_training,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=lm_input_max_length + lm_output_max_length,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        input_encodings_for_generation = self.tokenizer(\n",
    "            inputs_for_generation,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=lm_input_max_length,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        print(len(input_encodings.input_ids[0]))\n",
    "        data = {\n",
    "            \"output_texts\": output_texts,\n",
    "            \"lm_input_ids_for_train\": input_encodings.input_ids,\n",
    "            \"lm_attention_mask_for_train\": input_encodings.attention_mask,\n",
    "            \"lm_input_ids_for_generation\": input_encodings_for_generation.input_ids,\n",
    "            \"lm_attention_mask_for_generation\": input_encodings_for_generation.attention_mask,\n",
    "        }\n",
    "        return data\n",
    "\n",
    "    def train(self, batch: torch.utils.data.Dataset) -> torch.Tensor:\n",
    "        \"\"\"Using the Gemma, run a forward computation over the batch, compute\n",
    "        the log probability over the batch.\n",
    "\n",
    "        This will be used for training.\n",
    "        \"\"\"\n",
    "        self.train_mode_on()\n",
    "        loaded_batch = self.data_to_device(batch, keys=[\"lm_input_ids_for_train\", \"lm_attention_mask_for_train\",\n",
    "                                                        \"lm_attention_mask_for_generation\"])\n",
    "        input_ids = loaded_batch[\"lm_input_ids_for_train\"]\n",
    "        attention_mask = loaded_batch[\"lm_attention_mask_for_train\"]\n",
    "        original_len_without_answer = torch.sum(loaded_batch[\"lm_attention_mask_for_generation\"], dim=1)\n",
    "        with torch.set_grad_enabled(True):\n",
    "            logits = lm_logits(\n",
    "                model=self.model,\n",
    "                input_ids=input_ids,\n",
    "                input_mask=attention_mask,\n",
    "            )\n",
    "            batch_size, seq_len = input_ids.size()\n",
    "            masked_labels = input_ids.masked_fill(input_ids == self.tokenizer.pad_token_id, -100)\n",
    "            prompt_mask = torch.arange(seq_len, device=self.device).expand(batch_size, seq_len) < original_len_without_answer.unsqueeze(1)\n",
    "            masked_labels = masked_labels.masked_fill(prompt_mask == 1, -100)\n",
    "            return llama2_log_of_labels(logits=logits, labels=masked_labels, loss_func=self.loss_func)\n",
    "\n",
    "    def generation_pass(self, batch: torch.utils.data.Dataset) -> Tuple[List[str], torch.Tensor]:\n",
    "        \"\"\"Using the Gemma, generate new text.\n",
    "\n",
    "        This will be used for inference.\n",
    "        \"\"\"\n",
    "        self.predict_mode_on()\n",
    "        loaded_batch = self.data_to_device(batch, keys=[\"lm_input_ids_for_generation\", \"lm_attention_mask_for_generation\"])\n",
    "        input_ids = loaded_batch[\"lm_input_ids_for_generation\"]\n",
    "        attention_mask = loaded_batch[\"lm_attention_mask_for_generation\"]\n",
    "        with torch.no_grad():\n",
    "            predictions_output = self.model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                do_sample=True,\n",
    "                top_p=lm_top_p,\n",
    "                temperature=temperature,\n",
    "                max_length=lm_input_max_length + lm_output_max_length,\n",
    "                num_return_sequences=1,\n",
    "                output_logits=True,\n",
    "                return_dict_in_generate=True,\n",
    "                use_cache=True,\n",
    "                renormalize_logits=True,\n",
    "            )\n",
    "\n",
    "        prompt_len = input_ids.size()[1]\n",
    "        selected_samples = predictions_output.sequences[:, prompt_len:]\n",
    "        # all special tokens will be removed.\n",
    "        predictions_str = self.tokenizer.batch_decode(selected_samples, skip_special_tokens=True)\n",
    "        predictions_str = [pred.lstrip('\"').lstrip(\"'\").rstrip(\"'\").rstrip('\"').strip() for pred in predictions_str]\n",
    "\n",
    "        logits_list = list(predictions_output.logits)\n",
    "        logits = torch.stack(logits_list, dim=1)\n",
    "        labels_to_consider = selected_samples.masked_fill(selected_samples == self.tokenizer.pad_token_id, -100)\n",
    "        final_log_ps = mlm_log_of_labels(logits=logits, labels=labels_to_consider, loss_func=self.loss_func)\n",
    "        actual_lens = torch.sum(torch.where(labels_to_consider > 0, 1, 0), dim=1)\n",
    "        # Average log probs per token (length normalization).\n",
    "        return predictions_str, final_log_ps / actual_lens\n",
    "\n",
    "    def predict(self, batch: torch.utils.data.Dataset) -> Iterator[Dict[str, str]]:\n",
    "        \"\"\"The main prediction loop.\"\"\"\n",
    "        answers, log_ps = self.generation_pass(batch)\n",
    "        log_ps = log_ps.cpu().detach().numpy()\n",
    "        for idx, answer in enumerate(answers):\n",
    "            output_row = {\n",
    "                \"potential_answer\": answer,\n",
    "                \"prediction_score\": log_ps[idx],\n",
    "                \"gold_answer\": batch[\"output_texts\"][idx],\n",
    "            }\n",
    "            yield output_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram of context lengths and questions in the hotpotQA train and test splits.\n",
    "dataset = load_dataset(\"hotpot_qa\", \"distractor\")\n",
    "\n",
    "train_split = dataset[\"train\"]\n",
    "\n",
    "dev_split = dataset[\"validation\"]\n",
    "\n",
    "train_lengths = []\n",
    "for row in train_split:\n",
    "    sentences = row[\"context\"][\"sentences\"]\n",
    "    lens = 0.0\n",
    "    for para in sentences:\n",
    "        lens += sum([len(sent.split()) for sent in para])\n",
    "    train_lengths.append(lens)\n",
    "\n",
    "dev_lengths = []\n",
    "for row in dev_split:\n",
    "    sentences = row[\"context\"][\"sentences\"]\n",
    "    lens = 0.0\n",
    "    for para in sentences:\n",
    "        lens += sum([len(sent.split()) for sent in para])\n",
    "    dev_lengths.append(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.800e+01, 6.600e+01, 9.300e+01, 7.500e+01, 4.700e+01, 5.700e+01,\n",
       "        6.100e+01, 6.000e+01, 7.200e+01, 1.120e+02, 1.250e+02, 1.730e+02,\n",
       "        2.560e+02, 3.680e+02, 5.110e+02, 6.630e+02, 8.050e+02, 1.130e+03,\n",
       "        1.323e+03, 1.799e+03, 2.076e+03, 2.393e+03, 3.020e+03, 3.348e+03,\n",
       "        3.671e+03, 4.028e+03, 4.211e+03, 4.290e+03, 4.510e+03, 4.347e+03,\n",
       "        4.383e+03, 4.271e+03, 4.015e+03, 3.931e+03, 3.791e+03, 3.198e+03,\n",
       "        2.976e+03, 2.610e+03, 2.391e+03, 2.162e+03, 1.881e+03, 1.648e+03,\n",
       "        1.427e+03, 1.166e+03, 1.050e+03, 8.830e+02, 7.530e+02, 6.010e+02,\n",
       "        4.770e+02, 5.220e+02, 3.760e+02, 3.450e+02, 2.880e+02, 2.460e+02,\n",
       "        1.670e+02, 1.770e+02, 1.360e+02, 1.030e+02, 9.800e+01, 5.900e+01,\n",
       "        7.400e+01, 6.400e+01, 4.400e+01, 3.600e+01, 3.000e+01, 3.400e+01,\n",
       "        3.600e+01, 2.800e+01, 1.700e+01, 2.600e+01, 1.700e+01, 1.800e+01,\n",
       "        2.000e+01, 1.700e+01, 1.900e+01, 1.700e+01, 1.100e+01, 6.000e+00,\n",
       "        1.300e+01, 8.000e+00, 1.000e+01, 3.000e+00, 3.000e+00, 4.000e+00,\n",
       "        4.000e+00, 3.000e+00, 5.000e+00, 4.000e+00, 1.000e+00, 0.000e+00,\n",
       "        6.000e+00, 2.000e+00, 2.000e+00, 2.000e+00, 0.000e+00, 2.000e+00,\n",
       "        0.000e+00, 1.000e+00, 0.000e+00, 1.000e+00]),\n",
       " array([  29.  ,   56.63,   84.26,  111.89,  139.52,  167.15,  194.78,\n",
       "         222.41,  250.04,  277.67,  305.3 ,  332.93,  360.56,  388.19,\n",
       "         415.82,  443.45,  471.08,  498.71,  526.34,  553.97,  581.6 ,\n",
       "         609.23,  636.86,  664.49,  692.12,  719.75,  747.38,  775.01,\n",
       "         802.64,  830.27,  857.9 ,  885.53,  913.16,  940.79,  968.42,\n",
       "         996.05, 1023.68, 1051.31, 1078.94, 1106.57, 1134.2 , 1161.83,\n",
       "        1189.46, 1217.09, 1244.72, 1272.35, 1299.98, 1327.61, 1355.24,\n",
       "        1382.87, 1410.5 , 1438.13, 1465.76, 1493.39, 1521.02, 1548.65,\n",
       "        1576.28, 1603.91, 1631.54, 1659.17, 1686.8 , 1714.43, 1742.06,\n",
       "        1769.69, 1797.32, 1824.95, 1852.58, 1880.21, 1907.84, 1935.47,\n",
       "        1963.1 , 1990.73, 2018.36, 2045.99, 2073.62, 2101.25, 2128.88,\n",
       "        2156.51, 2184.14, 2211.77, 2239.4 , 2267.03, 2294.66, 2322.29,\n",
       "        2349.92, 2377.55, 2405.18, 2432.81, 2460.44, 2488.07, 2515.7 ,\n",
       "        2543.33, 2570.96, 2598.59, 2626.22, 2653.85, 2681.48, 2709.11,\n",
       "        2736.74, 2764.37, 2792.  ]),\n",
       " <BarContainer object of 100 artists>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAGdCAYAAAAMm0nCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAsj0lEQVR4nO3de3hU9YH/8U8CZEiESYQ0EygZNpZCgnJZYoHZVhYlS6Spqyu/36MtUbYiiht8irjosnXRYl18cBVvKOx6oV2kqF0vWxCUi4CXgJCSCiQEtewOW5mhIyXDZQiXfH9/+MvZTEiAXGe+k/freeYhc853Tr7n+8wMn5zzvSQZY4wAAAAskhzrCgAAALQUAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYJ3usa5AR6mrq9OXX36p3r17KykpKdbVAQAAF8EYo6NHj6p///5KTm7+OkvCBpgvv/xSOTk5sa4GAABohQMHDmjAgAHN7k/YANO7d29JXzeA2+2OcW0AAMDFCIfDysnJcf4fb07CBpj620Zut5sAAwCAZS7U/YNOvAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsk7CrUQMdwe/3KxQKOc8zMzPl9XpjWCMA6JoIMMBF8vv9ysvPU+RExNmWmpaqvVV7CTEA0MkIMMBFCoVCipyIqGRpiTyDPQruC2r5ncsVCoUIMADQyQgwQAt5BnuUMyIn1tUAgC6NTrwAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANZhGDXQzpitFwA6HgEGaEfM1gsAnYMAA7QjZusFgM5BgAHaqKqq6pyfma0XADoWAQZopXAwrKTkJJWUlMS6KgDQ5RBggFaK1ERk6oxzu0iSKtdXas0ja2JcMwBIfAQYoI0a3i4K7gvGuDYA0DUQYIBmNB4O3bCvCwAgtggwQBOaGg4NAIgfBBigCY2HQ0tt69/S+OoNk9sBQNsQYIDzaGv/luZGKjG5HQC0DQEG6EBNjVRicjsAaDsCDNAJmNgOANoXq1EDAADrcAUG+P8aDptmyDQAxDcCDCCGTQOAbQgwgM4dNs2SAAAQ3+gDAzRQ39m2r7dvrKsCADgPAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsE6bAsyjjz6qpKQkzZo1y9l28uRJlZaWqm/fvurVq5cmT56sYDAY9Tq/36/i4mKlpaUpKytLc+bM0ZkzZ6LKbNq0SaNGjZLL5dKgQYO0bNmytlQVAAAkkFavhbR9+3YtXbpUw4cPj9p+zz33aPXq1Xr99deVnp6umTNn6sYbb9RHH30kSTp79qyKi4uVnZ2tjz/+WAcPHtStt96qHj166J//+Z8lSfv371dxcbFmzJihV155RRs2bNDtt9+ufv36qaioqA2nC3yt4crTEqtPA4BtWhVgjh07pilTpujf/u3f9POf/9zZXlNToxdffFErVqzQNddcI0l6+eWXlZ+fr61bt2rs2LF67733VFlZqfXr18vj8WjkyJF6+OGHdf/99+uhhx5SSkqKlixZotzcXD3++OOSpPz8fH344YdatGgRAQZtFi8rTzcMTZmZmfJ6vTGsDQDYpVW3kEpLS1VcXKzCwsKo7eXl5Tp9+nTU9ry8PHm9XpWVlUmSysrKNGzYMHk8HqdMUVGRwuGw9uzZ45RpfOyioiLnGE2pra1VOByOegBNabjy9L3v36t7379Xk346qdN+fzgYVlJykkpKSlRQUKCCggLl5efJ7/d3Wh0AwHYtvgKzcuVK/fa3v9X27dvP2RcIBJSSkqKMjIyo7R6PR4FAwCnTMLzU76/fd74y4XBYkUhEqamp5/zuBQsW6Gc/+1lLTwddWP3K05IU3Be8QOn2E6mJyNQZlSwtkWewR8F9QS2/c7lCoRBXYQDgIrXoCsyBAwf0k5/8RK+88op69uzZUXVqlblz56qmpsZ5HDhwINZVAs6rPkB5BnsuXBgAEKVFAaa8vFyHDh3SqFGj1L17d3Xv3l2bN2/W008/re7du8vj8ejUqVM6cuRI1OuCwaCys7MlSdnZ2eeMSqp/fqEybre7yasvkuRyueR2u6MeAAAgMbUowEyYMEG7du1SRUWF87jyyis1ZcoU5+cePXpow4YNzmuqq6vl9/vl8/kkST6fT7t27dKhQ4ecMuvWrZPb7dbQoUOdMg2PUV+m/hgAAKBra1EfmN69e+uKK66I2nbJJZeob9++zvZp06Zp9uzZ6tOnj9xut+6++275fD6NHTtWkjRx4kQNHTpUt9xyixYuXKhAIKAHHnhApaWlcrlckqQZM2bo2Wef1X333afbbrtNGzdu1GuvvabVq1e3xzkDcanxUG5GJgFA81o9D0xzFi1apOTkZE2ePFm1tbUqKirSc8895+zv1q2bVq1apbvuuks+n0+XXHKJpk6dqvnz5ztlcnNztXr1at1zzz166qmnNGDAAL3wwgsMoUZCajgqqaHUtFTtrdpLiAGAJrQ5wGzatCnqec+ePbV48WItXry42dcMHDhQ77zzznmPO378eO3cubOt1QPiXuNRSZIYmQQAF9DuV2AAtE7DYd0AgPNjMUcAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHWYBwaIYw2XF2BpAQD4XwQYIA41tbwASwsAwP8iwABxqPHyAiwtAADRCDBAHGN5AQBoGp14AQCAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACswzBqdAl+v1+hUEhS9Oy2AAA7EWCQ8Px+v/Ly8xQ5EYl1VQAA7YQAg4QXCoUUORFxZrWtXF+pNY+siXW1AABtQB8YdBn1s9r29faNdVUAAG1EgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGCd7rGuANDe/H6/QqGQ87yqqiqGtQEAdAQCDBKK3+9XXn6eIicisa4KAKADEWCQUEKhkCInIipZWiLPYI8kqXJ9pdY8sibGNQMAtCcCDBKSZ7BHOSNyJEnBfcEY1wYA0N7oxAsAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIeJ7ACLNF7XKTMzU16vN0a1AYDYIcAAFggHw0pKTlJJSUnU9tS0VO2t2kuIAdDlEGAAC0RqIjJ1JmqNp+C+oJbfuVyhUIgAA6DLIcAAFmm4xhMAdGV04gUAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArNOiAPP8889r+PDhcrvdcrvd8vl8WrNmjbP/5MmTKi0tVd++fdWrVy9NnjxZwWAw6hh+v1/FxcVKS0tTVlaW5syZozNnzkSV2bRpk0aNGiWXy6VBgwZp2bJlrT9DAACQcFoUYAYMGKBHH31U5eXl2rFjh6655hpdf/312rNnjyTpnnvu0W9+8xu9/vrr2rx5s7788kvdeOONzuvPnj2r4uJinTp1Sh9//LF+8YtfaNmyZZo3b55TZv/+/SouLtbVV1+tiooKzZo1S7fffrvefffddjplAABguxYtJXDddddFPX/kkUf0/PPPa+vWrRowYIBefPFFrVixQtdcc40k6eWXX1Z+fr62bt2qsWPH6r333lNlZaXWr18vj8ejkSNH6uGHH9b999+vhx56SCkpKVqyZIlyc3P1+OOPS5Ly8/P14YcfatGiRSoqKmqn00Yi8fv9CoVCks5drRkAkJha3Qfm7NmzWrlypY4fPy6fz6fy8nKdPn1ahYWFTpm8vDx5vV6VlZVJksrKyjRs2DB5PB6nTFFRkcLhsHMVp6ysLOoY9WXqj9Gc2tpahcPhqAcSn9/vV15+ngoKClRQUHDOas0AgMTU4gCza9cu9erVSy6XSzNmzNCbb76poUOHKhAIKCUlRRkZGVHlPR6PAoGAJCkQCESFl/r99fvOVyYcDisSiTRbrwULFig9Pd155OSw4F1XEAqFFDkRUcnSEt37/r2a9NNJsa4SAKATtHg16iFDhqiiokI1NTX69a9/ralTp2rz5s0dUbcWmTt3rmbPnu08D4fDhJgupH6V5uC+4IULJ5iGt80yMzPl9XpjWBsA6BwtDjApKSkaNGiQJKmgoEDbt2/XU089pZtuukmnTp3SkSNHoq7CBINBZWdnS5Kys7P1ySefRB2vfpRSwzKNRy4Fg0G53W6lpqY2Wy+XyyWXy9XS0wGsFQ6GlZScFHXbLDUtVXur9hJiACS8Ns8DU1dXp9raWhUUFKhHjx7asGGDs6+6ulp+v18+n0+S5PP5tGvXLh06dMgps27dOrndbg0dOtQp0/AY9WXqjwHga5GaiEydcW6flSwtUeRExOnQDACJrEVXYObOnatJkybJ6/Xq6NGjWrFihTZt2qR3331X6enpmjZtmmbPnq0+ffrI7Xbr7rvvls/n09ixYyVJEydO1NChQ3XLLbdo4cKFCgQCeuCBB1RaWupcPZkxY4aeffZZ3Xfffbrtttu0ceNGvfbaa1q9enX7nz2QAOpvnwFAV9KiAHPo0CHdeuutOnjwoNLT0zV8+HC9++67+qu/+itJ0qJFi5ScnKzJkyertrZWRUVFeu6555zXd+vWTatWrdJdd90ln8+nSy65RFOnTtX8+fOdMrm5uVq9erXuuecePfXUUxowYIBeeOEFhlADAABHiwLMiy++eN79PXv21OLFi7V48eJmywwcOFDvvPPOeY8zfvx47dy5syVVAwAAXQhrIQEAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKzTPdYVANC+qqqqop5nZmbK6/XGqDYA0DEIMECCCAfDSkpOUklJSdT21LRU7a3aS4gBkFAIMECCiNREZOqMSpaWyDPYI0kK7gtq+Z3LFQqFCDAAEgoBBkgwnsEe5YzIiXU1AKBD0YkXAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHVYjRpW8fv9CoVCzvOqqqoY1sYeDdspMzNTXq83hrUBgLYjwMAafr9fefl5ipyIxLoq1ggHw0pKTlJJSYmzLTUtVXur9hJiAFiNAANrhEIhRU5EVLK0RJ7BHklS5fpKrXlkTYxrFr8iNRGZOuO0WXBfUMvvXK5QKESAAWA1Agys4xnsUc6IHElScF8wxrWxQ8M2A4BEQCdeAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOt0j3UFAHS+qqqqqOeZmZnyer0xqg0AtBwBBuhCwsGwkpKTVFJSErU9NS1Ve6v2EmIAWIMAA3QhkZqITJ1RydISeQZ7JEnBfUEtv3O5QqEQAQaANQgwQBfkGexRzoicWFcDAFqNTrwAAMA6BBgAAGAdbiEhrvn9foVCIUnnjpwBAHRdBBjELb/fr7z8PEVORGJdFQBAnCHAIG6FQiFFTkScETOV6yu15pE1sa4WACAOtKgPzIIFC/Sd73xHvXv3VlZWlm644QZVV1dHlTl58qRKS0vVt29f9erVS5MnT1YwGIwq4/f7VVxcrLS0NGVlZWnOnDk6c+ZMVJlNmzZp1KhRcrlcGjRokJYtW9a6M4T16kfM9PX2jXVVAABxokUBZvPmzSotLdXWrVu1bt06nT59WhMnTtTx48edMvfcc49+85vf6PXXX9fmzZv15Zdf6sYbb3T2nz17VsXFxTp16pQ+/vhj/eIXv9CyZcs0b948p8z+/ftVXFysq6++WhUVFZo1a5Zuv/12vfvuu+1wygAAwHYtuoW0du3aqOfLli1TVlaWysvLNW7cONXU1OjFF1/UihUrdM0110iSXn75ZeXn52vr1q0aO3as3nvvPVVWVmr9+vXyeDwaOXKkHn74Yd1///166KGHlJKSoiVLlig3N1ePP/64JCk/P18ffvihFi1apKKionY6dQAAYKs2DaOuqamRJPXp00eSVF5ertOnT6uwsNApk5eXJ6/Xq7KyMklSWVmZhg0bJo/H45QpKipSOBzWnj17nDINj1Ffpv4YTamtrVU4HI56AACAxNTqAFNXV6dZs2bpu9/9rq644gpJUiAQUEpKijIyMqLKejweBQIBp0zD8FK/v37f+cqEw2FFIk2PSFmwYIHS09OdR04Os4wCAJCoWh1gSktLtXv3bq1cubI969Nqc+fOVU1NjfM4cOBArKsEAAA6SKuGUc+cOVOrVq3Sli1bNGDAAGd7dna2Tp06pSNHjkRdhQkGg8rOznbKfPLJJ1HHqx+l1LBM45FLwWBQbrdbqampTdbJ5XLJ5XK15nQAAIBlWnQFxhijmTNn6s0339TGjRuVm5sbtb+goEA9evTQhg0bnG3V1dXy+/3y+XySJJ/Pp127dunQoUNOmXXr1sntdmvo0KFOmYbHqC9TfwwAANC1tegKTGlpqVasWKG3335bvXv3dvqspKenKzU1Venp6Zo2bZpmz56tPn36yO126+6775bP59PYsWMlSRMnTtTQoUN1yy23aOHChQoEAnrggQdUWlrqXEGZMWOGnn32Wd1333267bbbtHHjRr322mtavXp1O58+AACwUYuuwDz//POqqanR+PHj1a9fP+fx6quvOmUWLVqkH/zgB5o8ebLGjRun7OxsvfHGG87+bt26adWqVerWrZt8Pp9KSkp06623av78+U6Z3NxcrV69WuvWrdOIESP0+OOP64UXXmAINQAAkNTCKzDGmAuW6dmzpxYvXqzFixc3W2bgwIF65513znuc8ePHa+fOnS2pHgAA6CLaNA8MAABALLCYIwBJUlVVlfNzZmamvF5vDGsDAOdHgAG6uHAwrKTkJJWUlDjbUtNStbdqLyEGQNwiwABdXKQmIlNnVLK0RJ7BHgX3BbX8zuUKhUIEGABxiwADQJLkGexRzgiW4ABgBzrxAgAA6xBgAACAdQgwAADAOvSBQdzw+/0KhULO84bDegEAaIgAg7jg9/uVl5+nyIlIrKsCALAAAQZxIRQKKXIi4gzllaTK9ZVa88iaGNcMABCPCDCIKw2H8gb3BWNcGwBAvKITLwAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHSayA9CkxmtRZWZmyuv1xqg2ABCNAAMgSjgYVlJykkpKSqK2p6alam/VXkIMgLhAgAEQJVITkakzUetSBfcFtfzO5QqFQgQYAHGBAAOgSQ3XpQKAeEMnXgAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWYTVqABetqqrK+TkzM1NerzeGtQHQlRFgAFxQOBhWUnKSSkpKnG2paanaW7WXEAMgJggwAC4oUhORqTMqWVoiz2CPgvuCWn7ncoVCIQIMgJggwAC4aJ7BHuWMyIl1NQCATrwAAMA+BBgAAGAdAgwAALAOAQYAAFiHTryIGb/fr1AoJCl6fhEAAC6EAIOY8Pv9ysvPU+REJNZVAQBYiACDmAiFQoqciDjzilSur9SaR9bEuloAAEvQBwYxVT+vSF9v31hXBQBgEQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdZuIF0GqN17DKzMyU1+uNUW0AdCUEGAAtFg6GlZScpJKSkqjtqWmp2lu1lxADoMMRYAC0WKQmIlNnnLWsJCm4L6jldy5XKBQiwADocAQYdAq/369QKOQ8b3zrAXaqX8sKADobAQYdzu/3Ky8/T5ETkVhXBQCQIAgw6HChUEiRE5Go2w2V6yu15pE1Ma4ZAMBWBBh0moa3G4L7gjGuDQDAZswDAwAArEOAAQAA1uEWEoB21XCEGRPbAegoLb4Cs2XLFl133XXq37+/kpKS9NZbb0XtN8Zo3rx56tevn1JTU1VYWKjPPvssqszhw4c1ZcoUud1uZWRkaNq0aTp27FhUmU8//VRXXXWVevbsqZycHC1cuLDlZweg0zSc3K6goEAFBQXKy8+T3++PddUAJKAWB5jjx49rxIgRWrx4cZP7Fy5cqKefflpLlizRtm3bdMkll6ioqEgnT550ykyZMkV79uzRunXrtGrVKm3ZskV33HGHsz8cDmvixIkaOHCgysvL9dhjj+mhhx7Sv/7rv7biFAF0hoaT2937/r0qWVqiyIlI1Pw/ANBeWnwLadKkSZo0aVKT+4wxevLJJ/XAAw/o+uuvlyT98pe/lMfj0VtvvaWbb75ZVVVVWrt2rbZv364rr7xSkvTMM8/o+9//vv7lX/5F/fv31yuvvKJTp07ppZdeUkpKii6//HJVVFToiSeeiAo6AOIPk9sB6Azt2ol3//79CgQCKiwsdLalp6drzJgxKisrkySVlZUpIyPDCS+SVFhYqOTkZG3bts0pM27cOKWkpDhlioqKVF1drT/96U9N/u7a2lqFw+GoBwAASEztGmACgYAkyePxRG33eDzOvkAgoKysrKj93bt3V58+faLKNHWMhr+jsQULFig9Pd155OTwFyAAAIkqYYZRz507VzU1Nc7jwIEDsa4SAADoIO0aYLKzsyVJwWD0LKvBYNDZl52drUOHDkXtP3PmjA4fPhxVpqljNPwdjblcLrnd7qgHAABITO0aYHJzc5Wdna0NGzY428LhsLZt2yafzydJ8vl8OnLkiMrLy50yGzduVF1dncaMGeOU2bJli06fPu2UWbdunYYMGaJLL720PasMAAAs1OIAc+zYMVVUVKiiokLS1x13Kyoq5Pf7lZSUpFmzZunnP/+5/vM//1O7du3Srbfeqv79++uGG26QJOXn5+vaa6/V9OnT9cknn+ijjz7SzJkzdfPNN6t///6SpB/96EdKSUnRtGnTtGfPHr366qt66qmnNHv27HY7cQAAYK8WD6PesWOHrr76aud5faiYOnWqli1bpvvuu0/Hjx/XHXfcoSNHjuh73/ue1q5dq549ezqveeWVVzRz5kxNmDBBycnJmjx5sp5++mlnf3p6ut577z2VlpaqoKBAmZmZmjdvHkOoAQCApFYEmPHjx8sY0+z+pKQkzZ8/X/Pnz2+2TJ8+fbRixYrz/p7hw4frgw8+aGn1AABAF5Awo5AAAEDXwWKOADpUw8UdJRZ4BNA+CDAAOkTDxR0bSk1L1d6qvYQYAG1CgAHQIRou7ugZ/PVM2sF9QS2/c7lCoRABBkCbEGAAdCgWdwTQEejECwAArEOAAQAA1uEWEjqE3+9XKBSSdO4oFAAA2ooAg3bn9/uVl5+nyIlIrKsCAEhQBBi0u1AopMiJiDP6pHJ9pdY8sibW1QIAJBD6wKDD1I8+6evtG+uqAAASDAEGAABYh1tIADpdw47dLC0AoDUIMAA6TVPLC7C0AIDWIMAA6DSNlxdgaQEArUWAAdDpWF4AQFvRiRcAAFiHAAMAAKxDgAEAANahDwzarOG6RxJrHwEAOh4BBm3CukcAgFggwKBNGq97JIm1jwAAHY4Ag3bRcFhscF8wxrUBACQ6AgyAmGvcb4rlBQBcCAEGQMw0tbSAxPICAC6MAAMgZhovLSCJ5QUAXBQCDICYY2kBAC3FRHYAAMA6BBgAAGAdAgwAALAOfWDQYg2XDmDZAHSGxstVSAy1Bro6AgxahKUD0Nmae88x1Bro2ggwaJHGSwewbAA6WlPLVTDUGgABBq1SP+yVZQPQUepvT9b/y1BrAA0RYADEleZm5wWAhggwAOJK49l5uU0JoCkMowYQl+pvGfX19o11VQDEIQIMAACwDgEGAABYhwADAACsQ4ABAADWYRQSAGs1XMqCpQWAroUAg/NqvAYNax8hHjQ1VwxLCwBdCwEGzWLdI8SrxnPF1C8t8MEHHyg/P98px1UZIHERYNCsptagYVIxxJP6uWKam72XqzJA4iLA4IIarkHD2keIR42vyEgs+AgkOgIMgITBgo9A18EwagAAYB2uwABIaAy1BhITAQZAQmKoNZDYCDCI0nDeF+Z8gc2aG2pNp14gMRBg4GDeFySixh17GwdzbisBdiLAwNF43hfmfEEiYa4YILEQYHCO+r9YmfMFiYS5YoDEQoAB0KUwVwyQGJgHBgAAWIcrMADQQOMV2CU6+gLxiADThTX+ombYNLqq+vf+wYMH9X/+7//RycjJqP109AXiDwGmi2LINND8yCQ6+gLxjwDTRTUeMi2JYdPochqPTKr/DNDRF4h/BJguruEXNcOm0VVdzNQBrKkExBcCTBdBfxegdVhTCYhPBJgE1TCwNNcxEcCFNbem0gcffKD8/HynHFdlgM5FgOkgsRyK2VwHXfq7AK1Xf5upuY6/rp4u/cev/0P9+vWTRKABOhoBpgM0FyA66wuuuTWN6O8CtF1TSxL8fuvv9dZP39IPfvADpxy3mYCORYBpJw2vuFRVVZ0zwqcjv+Ca69/CmkZAx2n8B0FTt5kaD71u/Fmtra2Vy+WKOi5XboCLQ4BpB81dcemoL7iG2+jfAsSPxsOvG3aWb+qzmpScJFNnoo7BlRvg4sR1gFm8eLEee+wxBQIBjRgxQs8884xGjx4d62o1ecWjqVs2TWmPL7imttG/BYgfzfWTkXTO90RTk+Y17CDc2qs0LImARBe3AebVV1/V7NmztWTJEo0ZM0ZPPvmkioqKVF1draysrJjV63wz2Lbklk1rv+Aab6N/CxB/muon0/izWv85bfjZbep7oak/WBr3p2sccpq7Mnsx/fAaBx9CD+JV3AaYJ554QtOnT9ePf/xjSdKSJUu0evVqvfTSS/qHf/iHmNWrvWawbe0XXONthBUgfrX0D4vmZga+UH+6pkKOpAu+rnGgaSr4NC4jnRuYLnSruy1lGgeoi7myxNWnriEuA8ypU6dUXl6uuXPnOtuSk5NVWFiosrKyJl9TW1ur2tpa53lNTY0kKRwOt2vdjh079nUdI6dUe/zr33e69rQk6cDvDqj2eK0CnwWinks6Z1v98/Y6TlvLdOSxKUMZyrSsTP33Qv13QsPviaNfHZWpM7pm5jXKGJAh/2/92vHaDue5JGfb+V53sOqgyn5ZFhVo6l2wTJIkc57n7VjG1dOlf//lv8vj8SgYDOqWW29R7cnaNpWRvv4/pa6uLqpM422UOX+Z7OxsZWdnq73V/79tzLmhPIqJQ3/4wx+MJPPxxx9HbZ8zZ44ZPXp0k6958MEHjb5+6/PgwYMHDx48LH8cOHDgvFkhLq/AtMbcuXM1e/Zs53ldXZ0OHz6svn37KikpqdXHDYfDysnJ0YEDB+R2u9ujql0Obdg+aMe2ow3bB+3YPmjHphljdPToUfXv3/+85eIywGRmZqpbt24KBqPvGQeDwWYvV7lcrnPunWZkZLRbndxuN2+wNqIN2wft2Ha0YfugHdsH7Xiu9PT0C5ZJ7oR6tFhKSooKCgq0YcMGZ1tdXZ02bNggn88Xw5oBAIB4EJdXYCRp9uzZmjp1qq688kqNHj1aTz75pI4fP+6MSgIAAF1X3AaYm266SX/84x81b948BQIBjRw5UmvXrnV6kHcWl8ulBx988JzbU7h4tGH7oB3bjjZsH7Rj+6Ad2ybJmAuNUwIAAIgvcdkHBgAA4HwIMAAAwDoEGAAAYB0CDAAAsA4B5jwWL16sP/uzP1PPnj01ZswYffLJJ7GuUtx46KGHlJSUFPXIy8tz9p88eVKlpaXq27evevXqpcmTJ58zMaHf71dxcbHS0tKUlZWlOXPm6MyZM519Kp1qy5Ytuu6669S/f38lJSXprbfeitpvjNG8efPUr18/paamqrCwUJ999llUmcOHD2vKlClyu93KyMjQtGnTnDW66n366ae66qqr1LNnT+Xk5GjhwoUdfWqd5kJt+Ld/+7fnvDevvfbaqDJdvQ0lacGCBfrOd76j3r17KysrSzfccIOqq6ujyrTX53jTpk0aNWqUXC6XBg0apGXLlnX06XWKi2nD8ePHn/N+nDFjRlSZrtyGbdIuixcloJUrV5qUlBTz0ksvmT179pjp06ebjIwMEwwGY121uPDggw+ayy+/3Bw8eNB5/PGPf3T2z5gxw+Tk5JgNGzaYHTt2mLFjx5q/+Iu/cPafOXPGXHHFFaawsNDs3LnTvPPOOyYzM9PMnTs3FqfTad555x3z05/+1LzxxhtGknnzzTej9j/66KMmPT3dvPXWW+Z3v/ud+eu//muTm5trIpGIU+baa681I0aMMFu3bjUffPCBGTRokPnhD3/o7K+pqTEej8dMmTLF7N692/zqV78yqampZunSpZ11mh3qQm04depUc+2110a9Nw8fPhxVpqu3oTHGFBUVmZdfftns3r3bVFRUmO9///vG6/WaY8eOOWXa43P8+9//3qSlpZnZs2ebyspK88wzz5hu3bqZtWvXdur5doSLacO//Mu/NNOnT496P9bU1Dj7u3obtgUBphmjR482paWlzvOzZ8+a/v37mwULFsSwVvHjwQcfNCNGjGhy35EjR0yPHj3M66+/7myrqqoykkxZWZkx5uv/hJKTk00gEHDKPP/888btdpva2toOrXu8aPyfb11dncnOzjaPPfaYs+3IkSPG5XKZX/3qV8YYYyorK40ks337dqfMmjVrTFJSkvnDH/5gjDHmueeeM5deemlUO95///1myJAhHXxGna+5AHP99dc3+xrasGmHDh0ykszmzZuNMe33Ob7vvvvM5ZdfHvW7brrpJlNUVNTRp9TpGrehMV8HmJ/85CfNvoY2bD1uITXh1KlTKi8vV2FhobMtOTlZhYWFKisri2HN4stnn32m/v3767LLLtOUKVPk9/slSeXl5Tp9+nRU++Xl5cnr9TrtV1ZWpmHDhkVNTFhUVKRwOKw9e/Z07onEif379ysQCES1W3p6usaMGRPVbhkZGbryyiudMoWFhUpOTta2bducMuPGjVNKSopTpqioSNXV1frTn/7USWcTW5s2bVJWVpaGDBmiu+66S1999ZWzjzZsWk1NjSSpT58+ktrvc1xWVhZ1jPoyifhd2rgN673yyivKzMzUFVdcoblz5+rEiRPOPtqw9eJ2Jt5YCoVCOnv27Dmz/no8Hu3duzdGtYovY8aM0bJlyzRkyBAdPHhQP/vZz3TVVVdp9+7dCgQCSklJOWcxTY/Ho0AgIEkKBAJNtm/9vq6o/rybapeG7ZaVlRW1v3v37urTp09Umdzc3HOOUb/v0ksv7ZD6x4trr71WN954o3Jzc/XFF1/oH//xHzVp0iSVlZWpW7dutGET6urqNGvWLH33u9/VFVdcIUnt9jlurkw4HFYkElFqampHnFKna6oNJelHP/qRBg4cqP79++vTTz/V/fffr+rqar3xxhuSaMO2IMCgVSZNmuT8PHz4cI0ZM0YDBw7Ua6+91mU/TIgPN998s/PzsGHDNHz4cH3rW9/Spk2bNGHChBjWLH6VlpZq9+7d+vDDD2NdFWs114Z33HGH8/OwYcPUr18/TZgwQV988YW+9a1vdXY1Ewq3kJqQmZmpbt26ndPbPhgMKjs7O0a1im8ZGRkaPHiwPv/8c2VnZ+vUqVM6cuRIVJmG7Zednd1k+9bv64rqz/t877vs7GwdOnQoav+ZM2d0+PBh2rYZl112mTIzM/X5559Log0bmzlzplatWqX3339fAwYMcLa31+e4uTJutzth/thprg2bMmbMGEmKej/Shq1DgGlCSkqKCgoKtGHDBmdbXV2dNmzYIJ/PF8Oaxa9jx47piy++UL9+/VRQUKAePXpEtV91dbX8fr/Tfj6fT7t27Yr6j2TdunVyu90aOnRop9c/HuTm5io7Ozuq3cLhsLZt2xbVbkeOHFF5eblTZuPGjaqrq3O+GH0+n7Zs2aLTp087ZdatW6chQ4Yk3K2Pi/E///M/+uqrr9SvXz9JtGE9Y4xmzpypN998Uxs3bjznlll7fY59Pl/UMerLJMJ36YXasCkVFRWSFPV+7Mpt2Cax7kUcr1auXGlcLpdZtmyZqaysNHfccYfJyMiI6ineld17771m06ZNZv/+/eajjz4yhYWFJjMz0xw6dMgY8/XwS6/XazZu3Gh27NhhfD6f8fl8zuvrhw5OnDjRVFRUmLVr15pvfOMbCT+M+ujRo2bnzp1m586dRpJ54oknzM6dO81///d/G2O+HkadkZFh3n77bfPpp5+a66+/vslh1H/+539utm3bZj788EPz7W9/O2oI8JEjR4zH4zG33HKL2b17t1m5cqVJS0tLmCHA52vDo0ePmr//+783ZWVlZv/+/Wb9+vVm1KhR5tvf/rY5efKkc4yu3obGGHPXXXeZ9PR0s2nTpqghvidOnHDKtMfnuH4I8Jw5c0xVVZVZvHhxwgwBvlAbfv7552b+/Plmx44dZv/+/ebtt982l112mRk3bpxzjK7ehm1BgDmPZ555xni9XpOSkmJGjx5ttm7dGusqxY2bbrrJ9OvXz6SkpJhvfvOb5qabbjKff/65sz8SiZi/+7u/M5deeqlJS0szf/M3f2MOHjwYdYz/+q//MpMmTTKpqakmMzPT3Hvvveb06dOdfSqd6v333zeSznlMnTrVGPP1UOp/+qd/Mh6Px7hcLjNhwgRTXV0ddYyvvvrK/PCHPzS9evUybrfb/PjHPzZHjx6NKvO73/3OfO973zMul8t885vfNI8++mhnnWKHO18bnjhxwkycONF84xvfMD169DADBw4006dPP+cPj67ehsaYJttQknn55ZedMu31OX7//ffNyJEjTUpKirnsssuifofNLtSGfr/fjBs3zvTp08e4XC4zaNAgM2fOnKh5YIzp2m3YFknGGNN513sAAADajj4wAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFjn/wFsgYe6LlFeMwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(train_lengths, color='lightgreen', ec='black', bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2792.0\n"
     ]
    }
   ],
   "source": [
    "print(max(train_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  3.,   7.,   5.,   3.,   4.,   6.,   3.,   8.,   3.,   6.,   9.,\n",
       "         14.,  20.,  20.,  23.,  27.,  41.,  78.,  74., 103., 114., 160.,\n",
       "        169., 179., 229., 259., 266., 287., 324., 331., 329., 386., 299.,\n",
       "        301., 335., 288., 325., 237., 262., 236., 194., 170., 157., 164.,\n",
       "        115., 103., 106.,  87.,  80.,  65.,  62.,  57.,  38.,  30.,  32.,\n",
       "         21.,  24.,  19.,  10.,   9.,  12.,  14.,   3.,   6.,   8.,   3.,\n",
       "          3.,   7.,   9.,   1.,   3.,   0.,   0.,   1.,   1.,   2.,   0.,\n",
       "          0.,   0.,   1.,   3.,   1.,   0.,   0.,   3.,   2.,   0.,   0.,\n",
       "          0.,   1.,   2.,   0.,   0.,   1.,   0.,   0.,   1.,   0.,   0.,\n",
       "          1.]),\n",
       " array([  46.  ,   71.41,   96.82,  122.23,  147.64,  173.05,  198.46,\n",
       "         223.87,  249.28,  274.69,  300.1 ,  325.51,  350.92,  376.33,\n",
       "         401.74,  427.15,  452.56,  477.97,  503.38,  528.79,  554.2 ,\n",
       "         579.61,  605.02,  630.43,  655.84,  681.25,  706.66,  732.07,\n",
       "         757.48,  782.89,  808.3 ,  833.71,  859.12,  884.53,  909.94,\n",
       "         935.35,  960.76,  986.17, 1011.58, 1036.99, 1062.4 , 1087.81,\n",
       "        1113.22, 1138.63, 1164.04, 1189.45, 1214.86, 1240.27, 1265.68,\n",
       "        1291.09, 1316.5 , 1341.91, 1367.32, 1392.73, 1418.14, 1443.55,\n",
       "        1468.96, 1494.37, 1519.78, 1545.19, 1570.6 , 1596.01, 1621.42,\n",
       "        1646.83, 1672.24, 1697.65, 1723.06, 1748.47, 1773.88, 1799.29,\n",
       "        1824.7 , 1850.11, 1875.52, 1900.93, 1926.34, 1951.75, 1977.16,\n",
       "        2002.57, 2027.98, 2053.39, 2078.8 , 2104.21, 2129.62, 2155.03,\n",
       "        2180.44, 2205.85, 2231.26, 2256.67, 2282.08, 2307.49, 2332.9 ,\n",
       "        2358.31, 2383.72, 2409.13, 2434.54, 2459.95, 2485.36, 2510.77,\n",
       "        2536.18, 2561.59, 2587.  ]),\n",
       " <BarContainer object of 100 artists>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(dev_lengths, color='lightblue', ec='black', bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2587.0\n"
     ]
    }
   ],
   "source": [
    "print(max(dev_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAMetricModel:\n",
    "    \"\"\"Load and cache a model used for evaluating generative text\n",
    "    generation.\"\"\"\n",
    "\n",
    "    model_id = \"sentence-transformers/sentence-t5-xxl\"\n",
    "\n",
    "    def __init__(self, device: str = \"cuda:0\", batch_size: int = 16) -> None:\n",
    "        \"\"\"Save the gpu device and construct the model and cache it.\"\"\"\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.metric_model = SentenceTransformer(self.model_id, device=self.device).eval()\n",
    "\n",
    "    def compute_metric(self, predictions: List[str], references: List[List[str]]) -> float:\n",
    "        \"\"\"Compute the metric for the given predictions and multiple\n",
    "        references.\"\"\"\n",
    "        average_score = torch.tensor(0.0, device=self.device)\n",
    "        num_chunks = max(len(predictions) // self.batch_size, 1)\n",
    "        for chunk_i in range(num_chunks):\n",
    "            clear_cache()\n",
    "\n",
    "            if (chunk_i + 1) * self.batch_size <= len(predictions):\n",
    "                predictions_sub_arr = predictions[chunk_i * self.batch_size : (chunk_i + 1) * self.batch_size]\n",
    "                references_sub_arr = references[chunk_i * self.batch_size : (chunk_i + 1) * self.batch_size]\n",
    "            else:\n",
    "                predictions_sub_arr = predictions[chunk_i * self.batch_size :]\n",
    "                references_sub_arr = references[chunk_i * self.batch_size :]\n",
    "\n",
    "            # need to track multiple references.\n",
    "            ref_sub_arr_len = [len(ref_sub_arr) for ref_sub_arr in references_sub_arr]\n",
    "            references_sub_arr_flattened = []\n",
    "            for ref_sub_arr in references_sub_arr:\n",
    "                references_sub_arr_flattened.extend(ref_sub_arr)\n",
    "\n",
    "            prediction_embeddings = self.metric_model.encode(\n",
    "                predictions_sub_arr,\n",
    "                show_progress_bar=False,\n",
    "                batch_size=self.batch_size,\n",
    "                device=self.device,\n",
    "                normalize_embeddings=True,\n",
    "                convert_to_tensor=True,\n",
    "            )\n",
    "\n",
    "            references_embeddings = self.metric_model.encode(\n",
    "                references_sub_arr_flattened,\n",
    "                show_progress_bar=False,\n",
    "                batch_size=self.batch_size,\n",
    "                device=self.device,\n",
    "                normalize_embeddings=True,\n",
    "                convert_to_tensor=True,\n",
    "            )\n",
    "            dot_products = torch.matmul(prediction_embeddings, references_embeddings.t())\n",
    "            score_collector = torch.zeros_like(dot_products)\n",
    "            i = 0\n",
    "            j = 0\n",
    "            while i < len(predictions_sub_arr):\n",
    "                j_len = ref_sub_arr_len[i]\n",
    "                score_collector[i][j : j + j_len] = 1.0 / j_len\n",
    "                i += 1\n",
    "                j += j_len\n",
    "\n",
    "            average_score += torch.sum(dot_products * score_collector)\n",
    "        return (average_score / len(predictions)).item()\n",
    "\n",
    "\n",
    "qa_metric_model = None\n",
    "\n",
    "\n",
    "def postprocess_qa(label: str) -> str:\n",
    "    label = str(label)\n",
    "    label = label.lower()\n",
    "    label = label.replace(\"\\n\", \" \")\n",
    "    label = label.removesuffix(\"</s>\")\n",
    "    label = label.removeprefix(\"<s>\")\n",
    "    label = label.removeprefix(\"\\n\")\n",
    "    label = label.removesuffix(\"\\n\")\n",
    "    label = label.removeprefix(\".\")\n",
    "    label = label.removesuffix(\".\")\n",
    "    label = label.removeprefix(\"answer:\")\n",
    "    label = label.removeprefix(\",\")\n",
    "    label = label.strip()\n",
    "    if \"no answer\" in label or \"no_answer\" in label:\n",
    "        label = \"no answer\"\n",
    "    return label\n",
    "\n",
    "\n",
    "def qa_metric(prediction_file: str) -> Dict[str, float]:\n",
    "    \"\"\"Compute the metric for the qa task.\"\"\"\n",
    "    global qa_metric_model\n",
    "    if qa_metric_model is None:\n",
    "        qa_metric_model = QAMetricModel(device=metric_device, batch_size=metric_batch_size)\n",
    "\n",
    "    df = pd.read_csv(prediction_file, delimiter=\",\")\n",
    "\n",
    "    gold_answers = [postprocess_qa(label) for label in df[\"gold_answer\"].tolist()]\n",
    "\n",
    "    multiple_gold_answers = []\n",
    "    for answer in gold_answers:\n",
    "        multiple_gold_answers.append(answer.split(answer_splitter))\n",
    "\n",
    "    return_metrics: Dict[str, float] = {}\n",
    "    metrics = {\n",
    "        \"potential_answer\": \"qa_score\",\n",
    "    }\n",
    "\n",
    "    for metric_column, metric in metrics.items():\n",
    "        if metric_column in df.columns:\n",
    "            predictions = [postprocess_qa(pred) for pred in df[metric_column].tolist()]\n",
    "            score = qa_metric_model.compute_metric(predictions, multiple_gold_answers)\n",
    "            return_metrics[metric] = score\n",
    "\n",
    "    return return_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hotpotqa_dataloader(\n",
    "    model: Gemma,\n",
    "    dataset: Any,\n",
    "    batch_size: int,\n",
    "    shuffle: bool\n",
    ") -> DataLoader:\n",
    "    \"\"\"Function to create the required dataloader to train/test the LM models.\"\"\"\n",
    "\n",
    "    # look at the datacard here:\n",
    "    # https://huggingface.co/datasets/hotpot_qa?row=0#curation-rationale\n",
    "    input_texts = []\n",
    "    output_texts = []\n",
    "    for row in dataset:\n",
    "        sentences = row[\"context\"][\"sentences\"]\n",
    "        context = []\n",
    "        for idx, para in enumerate(sentences):\n",
    "            context.append(f\"Context_{idx+1} :\")\n",
    "            context.extend(para)\n",
    "        context.append(\"Question:\")\n",
    "        context.append(row[\"question\"])\n",
    "        input_texts.append(' '.join(context))\n",
    "        output_texts.append(f\"Answer: {row['answer']}\")\n",
    "\n",
    "    input_texts = input_texts[0:128]\n",
    "    output_texts = output_texts[0:128]\n",
    "    instructions = [hotpot_question_answer_instruction] * len(input_texts)\n",
    "    data = model.prepare_text(input_texts, output_texts, instructions)\n",
    "    dataset = DictDataset(data)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|| 2/2 [00:04<00:00,  2.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,686,400 || all params: 2,509,875,200 || trainable%: 0.14687582872646418\n",
      "4224\n",
      "4224\n"
     ]
    }
   ],
   "source": [
    "# Create model and start training.\n",
    "set_random_seed(42)\n",
    "\n",
    "model = Gemma(mode=\"train\", device=\"cuda:0\", seed=42)\n",
    "model.to_device()\n",
    "train_dataloader = create_hotpotqa_dataloader(model, train_split, train_batch_size, True)\n",
    "dev_dataloader = create_hotpotqa_dataloader(model, dev_split, eval_batch_size, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Prediction Step: 9.\n",
      "Prediction Step: 10.\n",
      "Prediction Step: 11.\n",
      "Prediction Step: 12.\n",
      "Prediction Step: 13.\n",
      "Prediction Step: 14.\n",
      "Prediction Step: 15.\n",
      "Prediction Step: 16.\n",
      "Prediction Step: 17.\n",
      "Prediction Step: 18.\n",
      "Prediction Step: 19.\n",
      "Prediction Step: 20.\n",
      "Prediction Step: 21.\n",
      "Prediction Step: 22.\n",
      "Prediction Step: 23.\n",
      "Prediction Step: 24.\n",
      "Prediction Step: 25.\n",
      "Prediction Step: 26.\n",
      "Prediction Step: 27.\n",
      "Prediction Step: 28.\n",
      "Prediction Step: 29.\n",
      "Prediction Step: 30.\n",
      "Prediction Step: 31.\n",
      "Prediction Step: 32.\n",
      "Prediction Step: 33.\n",
      "Prediction Step: 34.\n",
      "Prediction Step: 35.\n",
      "Prediction Step: 36.\n",
      "Prediction Step: 37.\n",
      "Prediction Step: 38.\n",
      "Prediction Step: 39.\n",
      "Prediction Step: 40.\n",
      "Prediction Step: 41.\n",
      "Prediction Step: 42.\n",
      "Prediction Step: 43.\n",
      "Prediction Step: 44.\n",
      "Prediction Step: 45.\n",
      "Prediction Step: 46.\n",
      "Prediction Step: 47.\n",
      "Prediction Step: 48.\n",
      "Prediction Step: 49.\n",
      "Prediction Step: 50.\n",
      "Prediction Step: 51.\n",
      "Prediction Step: 52.\n",
      "Prediction Step: 53.\n",
      "Prediction Step: 54.\n",
      "Prediction Step: 55.\n",
      "Prediction Step: 56.\n",
      "Prediction Step: 57.\n",
      "Prediction Step: 58.\n",
      "Prediction Step: 59.\n",
      "Prediction Step: 60.\n",
      "Prediction Step: 61.\n",
      "Prediction Step: 62.\n",
      "Prediction Step: 63.\n",
      "Prediction Step: 64.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /model-weights/gemma-2b-it - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:0\n",
      "\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 8.06 GiB. GPU 0 has a total capacity of 44.56 GiB of which 7.11 GiB is free. Including non-PyTorch memory, this process has 37.44 GiB memory in use. Of the allocated memory 35.98 GiB is allocated by PyTorch, and 172.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_to_save\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mqa_score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# not important\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqa_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdev_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/src/general_utils.py:86\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(model, mode, model_path, metric_to_save, max_epochs, training_steps, steps_per_checkpoint, metric, train_dataloader, eval_dataloader)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch:\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch))\n\u001b[1;32m     85\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, loss \u001b[38;5;129;01min\u001b[39;00m start_training(model, train_dataloader):\n\u001b[1;32m     87\u001b[0m     global_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;66;03m# do optimization stuff here.\u001b[39;00m\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/src/general_utils.py:50\u001b[0m, in \u001b[0;36mstart_training\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m     48\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m---> 50\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m step, loss\n",
      "Cell \u001b[0;32mIn[10], line 77\u001b[0m, in \u001b[0;36mGemma.train\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     75\u001b[0m original_len_without_answer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(loaded_batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlm_attention_mask_for_generation\u001b[39m\u001b[38;5;124m\"\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 77\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mlm_logits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     batch_size, seq_len \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m     83\u001b[0m     masked_labels \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mmasked_fill(input_ids \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/src/model_utils.py:152\u001b[0m, in \u001b[0;36mlm_logits\u001b[0;34m(model, input_ids, input_mask)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlm_logits\u001b[39m(\n\u001b[1;32m    145\u001b[0m     model: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[1;32m    146\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    147\u001b[0m     input_mask: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    148\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    149\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Do a forward computation and compute the logits for the given input_ids\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m    for a language model.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/peft/peft_model.py:1091\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mpeft_type \u001b[38;5;241m==\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mPOLY:\n\u001b[1;32m   1090\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m task_ids\n\u001b[0;32m-> 1091\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1102\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:160\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/transformers/models/gemma/modeling_gemma.py:1099\u001b[0m, in \u001b[0;36mGemmaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1097\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1098\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n\u001b[0;32m-> 1099\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;66;03m# Shift so that tokens < n predict n\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 8.06 GiB. GPU 0 has a total capacity of 44.56 GiB of which 7.11 GiB is free. Including non-PyTorch memory, this process has 37.44 GiB memory in use. Of the allocated memory 35.98 GiB is allocated by PyTorch, and 172.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "train_loop(\n",
    "    model=model,\n",
    "    mode=\"train\",\n",
    "    model_path=model_path,\n",
    "    metric_to_save=\"qa_score\",\n",
    "    max_epochs=10,\n",
    "    training_steps=10000000,  # not important\n",
    "    steps_per_checkpoint=64,\n",
    "    metric=qa_metric,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=dev_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Prediction Step: 9.\n",
      "Prediction Step: 10.\n",
      "Prediction Step: 11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Step: 12.\n",
      "Prediction Step: 13.\n",
      "Prediction Step: 14.\n",
      "Prediction Step: 15.\n",
      "Prediction Step: 16.\n",
      "Prediction Step: 17.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdev.predicted.tsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdev_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/src/general_utils.py:148\u001b[0m, in \u001b[0;36mtest_loop\u001b[0;34m(model, mode, model_path, prediction_file_name, test_dataloader, metric)\u001b[0m\n\u001b[1;32m    146\u001b[0m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicting...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    147\u001b[0m prediction_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_path, prediction_file_name)\n\u001b[0;32m--> 148\u001b[0m \u001b[43mstart_predicting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metric \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    150\u001b[0m     scores \u001b[38;5;241m=\u001b[39m metric(prediction_file)\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/src/general_utils.py:32\u001b[0m, in \u001b[0;36mstart_predicting\u001b[0;34m(model, dataloader, prediction_file)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 32\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m ret_row \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mpredict(batch):\n\u001b[1;32m     33\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m header_written:\n\u001b[1;32m     34\u001b[0m                 headers \u001b[38;5;241m=\u001b[39m ret_row\u001b[38;5;241m.\u001b[39mkeys()\n",
      "Cell \u001b[0;32mIn[11], line 150\u001b[0m, in \u001b[0;36mLlama2QA.predict\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]:\n\u001b[1;32m    149\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The main prediction loop.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m     answers, log_ps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneration_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     log_ps \u001b[38;5;241m=\u001b[39m log_ps\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, answer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(answers):\n",
      "Cell \u001b[0;32mIn[11], line 120\u001b[0m, in \u001b[0;36mLlama2QA.generation_pass\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    116\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m loaded_batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlm_attention_mask_for_generation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# more look here:\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;66;03m# https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L130\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m     predictions_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlm_top_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlm_input_max_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlm_output_max_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrenormalize_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m prompt_len \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    135\u001b[0m selected_samples \u001b[38;5;241m=\u001b[39m predictions_output\u001b[38;5;241m.\u001b[39msequences[:, prompt_len:]\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/peft/peft_model.py:1148\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgeneration_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1148\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prepare_inputs_for_generation\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/transformers/generation/utils.py:1570\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1562\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1563\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1564\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1565\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1566\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1567\u001b[0m     )\n\u001b[1;32m   1569\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1570\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1576\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1577\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1586\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1587\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1588\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1589\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1594\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1595\u001b[0m     )\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/transformers/generation/utils.py:2705\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2702\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2704\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2705\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2706\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2708\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2709\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2710\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2712\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2713\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1190\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1187\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1190\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1203\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1025\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1014\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1015\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1016\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m         cache_position,\n\u001b[1;32m   1023\u001b[0m     )\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1025\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1035\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:754\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[1;32m    753\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 754\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_attention_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    755\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[1;32m    756\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:90\u001b[0m, in \u001b[0;36mLlamaRMSNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m     88\u001b[0m variance \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     89\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrsqrt(variance \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariance_epsilon)\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m*\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_loop(\n",
    "    model=model,\n",
    "    mode=\"test\",\n",
    "    model_path=model_path,\n",
    "    prediction_file_name=\"dev.predicted.tsv\",\n",
    "    test_dataloader=dev_dataloader,\n",
    "    metric=qa_metric,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
