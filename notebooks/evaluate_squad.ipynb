{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d618f29-5738-43e5-836f-92a4d478c2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "from src.metrics import QAMetricModel, compute_f1_precision_recall, get_tokens, normalize_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a31e9362-d6d8-4f08-96d3-15da95094e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun 17 23:27:36 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A40          On   | 00000000:86:00.0 Off |                    0 |\n",
      "|  0%   27C    P8    29W / 300W |      0MiB / 46068MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38603a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hg_token = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "561e18d8-7113-4d44-8dc6-45cc59772905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /h/snajafi/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token hg_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7f0c0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 8.92k/8.92k [00:00<00:00, 14.9MB/s]\n",
      "Downloading data: 100%|██████████| 16.4M/16.4M [00:01<00:00, 15.2MB/s]\n",
      "Downloading data: 100%|██████████| 1.35M/1.35M [00:00<00:00, 4.77MB/s]\n",
      "Generating train split: 100%|██████████| 130319/130319 [00:00<00:00, 363124.98 examples/s]\n",
      "Generating validation split: 100%|██████████| 11873/11873 [00:00<00:00, 364517.86 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11873\n",
      "11873\n",
      "assistant\n",
      "\n",
      "France\n",
      "['France', 'France', 'France', 'France']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/scratch/ssd004/scratch/snajafi/checkpoints/llama3-squadv2.0_normal_icl/llama3.squad2.dev.results.csv\")\n",
    "data = df.to_dict(orient=\"records\")\n",
    "\n",
    "dataset = load_dataset(\"rajpurkar/squad_v2\", split=\"validation\")\n",
    "gold_answers = []\n",
    "for row in dataset:\n",
    "    if row[\"answers\"][\"text\"]:\n",
    "        gold_answers.append(row[\"answers\"][\"text\"])\n",
    "    else:\n",
    "        gold_answers.append([\"<no_answer>\"])\n",
    "\n",
    "predictions = []\n",
    "for row in data:\n",
    "    predictions.append(row[\"potential_answer\"])\n",
    "\n",
    "print(len(predictions))\n",
    "print(len(gold_answers))\n",
    "\n",
    "print(predictions[0])\n",
    "print(gold_answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e90876d6-ae47-4f0a-91fc-8bda10eadeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_t5_metric_model = QAMetricModel(device=\"cuda:0\", batch_size=16, metric_type=\"sentence_t5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95e9700a-1bbd-4ad9-b391-c6d646f51e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = [[normalize_answer(an) for an in ans] for ans in gold_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e40bdc14-4705-4c11-8f44-e8b79b4ab879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['assistant\\n\\nFrance', 'assistant\\n\\nThe correct answer is \"in the 10th and 11th centuries\".', 'assistant\\n\\nBased on the passage, the correct answer is:\\n\\nDenmark, Iceland, and Norway', 'assistant\\n\\nRollo', 'assistant\\n\\nThe shortest continuous text span from the passage that serves as an answer to the question is:\\n\\n\"...the distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century...\"\\n\\nTherefore, the correct answer is:\\n\\n10th century', 'assistant\\n\\nThe shortest continuous text span that serves as an answer to the given question is:\\n\\n\"The Normans\"\\n\\nThis answer is justified by the passage, which states that the Normans were the people who gave their name to Normandy in the 10th and 11th centuries.', 'assistant\\n\\nBased on the passage, the correct answer is:\\n\\nFinal Answer_11: Normandy', 'assistant\\n\\nThe correct answer is Rollo.', 'assistant\\n\\nThe passage does not mention the Frankish identity emerging at a specific time. It only mentions that the Normans, who were descended from Norse raiders and pirates, agreed to swear fealty to King Charles III of West Francia, and that their descendants would gradually merge with the Carolingian-based cultures of West Francia. Therefore, the correct answer is:\\n\\nFinal Answer_11: <no_answer>', 'assistant\\n\\nThe correct answer is William the Conqueror.']\n",
      "['france', 'in 10th and 11th centuries', 'denmark iceland and norway', 'rollo', 'distinct cultural and ethnic identity of normans emerged initially in first half of 10th century therefore correct answer is 10th century', 'normans this answer is justified by passage which states that normans were people who gave their name to normandy in 10th and 11th centuries', 'final answer11 normandy', 'rollo', 'This question is not answerable', 'william conqueror']\n",
      "[['france', 'france', 'france', 'france'], ['10th and 11th centuries', 'in 10th and 11th centuries', '10th and 11th centuries', '10th and 11th centuries'], ['denmark iceland and norway', 'denmark iceland and norway', 'denmark iceland and norway', 'denmark iceland and norway'], ['rollo', 'rollo', 'rollo', 'rollo'], ['10th century', 'first half of 10th century', '10th', '10th'], ['This question is not answerable'], ['This question is not answerable'], ['This question is not answerable'], ['This question is not answerable'], ['william conqueror', 'william conqueror', 'william conqueror']]\n",
      "[['France', 'France', 'France', 'France'], ['10th and 11th centuries', 'in the 10th and 11th centuries', '10th and 11th centuries', '10th and 11th centuries'], ['Denmark, Iceland and Norway', 'Denmark, Iceland and Norway', 'Denmark, Iceland and Norway', 'Denmark, Iceland and Norway'], ['Rollo', 'Rollo', 'Rollo', 'Rollo'], ['10th century', 'the first half of the 10th century', '10th', '10th'], ['<no_answer>'], ['<no_answer>'], ['<no_answer>'], ['<no_answer>'], ['William the Conqueror', 'William the Conqueror', 'William the Conqueror']]\n"
     ]
    }
   ],
   "source": [
    "preds = [normalize_answer(pred) for pred in predictions]\n",
    "print(predictions[0:10])\n",
    "print(preds[0:10])\n",
    "print(answers[:10])\n",
    "print(gold_answers[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "599f197e-a477-4d9b-878b-4ab5be10fd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = sentence_t5_metric_model.compute_metric(preds[:10], answers[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bcfde5d-a211-48f8-8faa-257a35a7c656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 0.9862, 1.0000, 1.0000, 0.7817, 0.5782, 0.6214, 0.6651, 1.0000,\n",
      "        1.0000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57216e44-d5cd-42f2-9657-2034ade0b393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'question', 'is', 'not', 'answerable']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tokens(\"This question is not answerable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc6a3ea8-11e0-4059-ba69-b383b919d633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_f1_precision_recall(preds[0], answers[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1d761b0-6ebf-4877-904f-74a77aa1a1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0]\n",
      "william conqueror\n",
      "william conqueror\n"
     ]
    }
   ],
   "source": [
    "print(compute_f1_precision_recall(preds[9], answers[9][0]))\n",
    "print(preds[9])\n",
    "print(answers[9][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bc94aaa-7717-4c5e-a8fa-0aeb34d7fdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1]\n",
      "This question is not answerable\n",
      "This question is not answerable\n"
     ]
    }
   ],
   "source": [
    "print(compute_f1_precision_recall(preds[8], answers[8][0]))\n",
    "print(preds[8])\n",
    "print(answers[8][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c073b55-0338-4e80-8228-63c353e2645d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "A new version of the following files was downloaded from https://huggingface.co/McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp:\n",
      "- attn_mask_utils.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp:\n",
      "- modeling_llama_encoder.py\n",
      "- attn_mask_utils.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Downloading shards: 100%|██████████| 4/4 [00:31<00:00,  7.76s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:34<00:00,  8.62s/it]\n",
      "Some weights of the model checkpoint at meta-llama/Meta-Llama-3-8B-Instruct were not used when initializing LlamaEncoderModel: ['lm_head.weight']\n",
      "- This IS expected if you are initializing LlamaEncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaEncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# load the new llm2vec sentence similarity metrics.\n",
    "llm2vec_metric_model = QAMetricModel(device=\"cuda:0\", batch_size=16, metric_type=\"llm2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccddf122-cfb0-4184-a655-954e08b164f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.99it/s]\n"
     ]
    }
   ],
   "source": [
    "scores = llm2vec_metric_model.compute_metric(preds[:10], answers[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36eaa294-ca87-420f-b6f9-d112a4e84c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9999, 0.9904, 0.9999, 0.9998, 0.5406, 0.3153, 0.2691, 0.1912, 0.9999,\n",
      "        0.9999])\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-research",
   "language": "python",
   "name": "llm-research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
