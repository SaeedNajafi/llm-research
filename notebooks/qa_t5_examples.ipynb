{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n",
      "2024-03-05 00:13:28.475597: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-05 00:13:28.581410: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-05 00:13:28.921397: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-05 00:13:28.921513: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-05 00:13:28.948119: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-05 00:13:29.050788: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-05 00:13:33.774117: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from typing import Any, Dict, Iterator, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "from src.base_lm import BaseLM\n",
    "from src.general_utils import DictDataset, test_loop, train_loop\n",
    "from src.model_utils import clear_cache, encoder_decoder_log_of_labels, mlm_log_of_labels, set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 8\n",
    "eval_batch_size = 8\n",
    "lm_input_max_length = 1024\n",
    "lm_output_max_length = 128\n",
    "lm_top_p = 0.9\n",
    "temperature = 0.6\n",
    "metric_device = \"cuda:1\"\n",
    "metric_batch_size = 16\n",
    "learning_rate = 0.001\n",
    "train_file_name = \"128-shot-datasets/squad/128-42-train.tsv\"\n",
    "dev_file_name = \"128-shot-datasets/squad/128-42-dev.tsv\"\n",
    "test_file_name = \"128-shot-datasets/squad/test.tsv\"\n",
    "\n",
    "# folder to store models and predictions.\n",
    "model_path = \"/scratch/ssd004/scratch/snajafi/checkpoints/t5-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"QA Model based on T5 base and without any optimizations for large-scale training.\"\"\"\n",
    "\n",
    "\n",
    "class T5BaseQA(BaseLM):\n",
    "    \"\"\"Class to implement T5-base for QA task.\"\"\"\n",
    "\n",
    "    def __init__(self, device: str, seed: int) -> None:\n",
    "        super().__init__(device=device, model_name=\"t5_base\", seed=seed)\n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"google/t5-base-lm-adapt\")\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(\"google/t5-base-lm-adapt\")\n",
    "        # to train the main lm, we update all of its parameters.\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "        self.scheduler = CosineAnnealingWarmRestarts(self.optimizer, T_0=10, eta_min=learning_rate / 10.0)\n",
    "\n",
    "    def prepare_text(self, texts: List[str], output_texts: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Convert texts to ids and return the dataset required for training\n",
    "        and inference.\"\"\"\n",
    "        instruction = \"In this task, you are given a context and question. \\\n",
    "            Provide a short phrase as the answer for the given question using only the information from the context. \\\n",
    "            If you do not know the answer from the context, generate 'no_answer' in the output. \\\n",
    "            Do not repeat the question in the output.\"\n",
    "        inputs = [f\"{instruction} {text}\" for text in texts]\n",
    "        # sample of the answers if possible.\n",
    "        sampled_answers = [random.choice(text.split(\"[<@>]\")) for text in output_texts]\n",
    "        answers = [f\"Answer: {answer}\" for answer in sampled_answers]\n",
    "        input_encodings = self.tokenizer(\n",
    "            inputs,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=lm_input_max_length,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        answer_encodings = self.tokenizer(\n",
    "            answers,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=lm_output_max_length,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        data = {\n",
    "            \"input_ids\": input_encodings.input_ids,\n",
    "            \"attention_mask\": input_encodings.attention_mask,\n",
    "            \"labels\": answer_encodings.input_ids,\n",
    "            \"target_attention_mask\": answer_encodings.attention_mask,\n",
    "            \"input_texts\": texts,\n",
    "            \"output_texts\": output_texts,\n",
    "        }\n",
    "        return data\n",
    "\n",
    "    def train(self, batch: torch.utils.data.Dataset) -> torch.Tensor:\n",
    "        \"\"\"Using the T5-base, run a forward computation over the batch, compute\n",
    "        the log probability over the batch.\n",
    "\n",
    "        This will be used for training.\n",
    "        \"\"\"\n",
    "        self.train_mode_on()\n",
    "        loaded_batch = self.data_to_device(batch, keys=[\"input_ids\", \"attention_mask\", \"target_attention_mask\", \"labels\"])\n",
    "        orig_labels = loaded_batch[\"labels\"]\n",
    "        labels = orig_labels.masked_fill(orig_labels == self.tokenizer.pad_token_id, -100)\n",
    "        with torch.set_grad_enabled(True):\n",
    "            class_log_p = encoder_decoder_log_of_labels(\n",
    "                model=self.model,\n",
    "                input_ids=loaded_batch[\"input_ids\"],\n",
    "                input_mask=loaded_batch[\"attention_mask\"],\n",
    "                decoder_mask=loaded_batch[\"target_attention_mask\"],\n",
    "                labels=labels,\n",
    "                loss_func=self.loss_func,\n",
    "            )\n",
    "        return class_log_p\n",
    "\n",
    "    def generation_pass(self, batch: torch.utils.data.Dataset) -> Tuple[List[str], torch.Tensor]:\n",
    "        \"\"\"This will be used for inference.\"\"\"\n",
    "        self.predict_mode_on()\n",
    "        loaded_batch = self.data_to_device(batch, keys=[\"input_ids\", \"attention_mask\"])\n",
    "        input_ids = loaded_batch[\"input_ids\"]\n",
    "        attention_mask = loaded_batch[\"attention_mask\"]\n",
    "        with torch.no_grad():\n",
    "            # more look here:\n",
    "            # https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L130\n",
    "            predictions_output = self.model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                do_sample=True,\n",
    "                top_p=lm_top_p,\n",
    "                temperature=temperature,\n",
    "                max_length=lm_output_max_length + lm_input_max_length,\n",
    "                num_return_sequences=1,\n",
    "                output_logits=True,\n",
    "                return_dict_in_generate=True,\n",
    "                use_cache=True,\n",
    "                renormalize_logits=True,\n",
    "            )\n",
    "\n",
    "        selected_samples = predictions_output.sequences\n",
    "        # all special tokens will be removed.\n",
    "        predictions_str = self.tokenizer.batch_decode(selected_samples, skip_special_tokens=True)\n",
    "        predictions_str = [pred.lstrip('\"').lstrip(\"'\").rstrip(\"'\").rstrip('\"').strip() for pred in predictions_str]\n",
    "\n",
    "        logits_list = list(predictions_output.logits)\n",
    "        logits = torch.stack(logits_list, dim=1)\n",
    "        ignore_first_token_samples = selected_samples[:, 1:]\n",
    "        labels_to_consider = ignore_first_token_samples.masked_fill(\n",
    "            ignore_first_token_samples == self.tokenizer.pad_token_id, -100\n",
    "        )\n",
    "        final_log_ps = mlm_log_of_labels(logits=logits, labels=labels_to_consider, loss_func=self.loss_func)\n",
    "        actual_lens = torch.sum(torch.where(labels_to_consider > 0, 1, 0), dim=1)\n",
    "        # Average log probs per token (length normalization).\n",
    "        return predictions_str, final_log_ps / actual_lens\n",
    "\n",
    "    def predict(self, batch: torch.utils.data.Dataset) -> Iterator[Dict[str, str]]:\n",
    "        \"\"\"The main prediction loop.\"\"\"\n",
    "        answers, log_ps = self.generation_pass(batch)\n",
    "        log_ps = log_ps.cpu().detach().numpy()\n",
    "        for idx, answer in enumerate(answers):\n",
    "            output_row = {\n",
    "                \"potential_answer\": answer,\n",
    "                \"prediction_score\": log_ps[idx],\n",
    "                \"gold_answer\": batch[\"output_texts\"][idx],\n",
    "            }\n",
    "            yield output_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gen_fewshot_file(file_path: str) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Load the fewshot files for QA task.\"\"\"\n",
    "    df = pd.read_csv(file_path, sep=\"\\t\")\n",
    "    input_texts = df.article.tolist()\n",
    "    output_texts = df.answer.tolist()\n",
    "    return input_texts, output_texts\n",
    "\n",
    "\n",
    "def create_dataloader(\n",
    "    model: T5BaseQA,\n",
    "    train_file_name: Optional[str] = None,\n",
    "    dev_file_name: Optional[str] = None,\n",
    "    test_file_name: Optional[str] = None,\n",
    ") -> DataLoader:\n",
    "    \"\"\"Function to create the required dataloader to train the LM models.\"\"\"\n",
    "    if train_file_name is not None:\n",
    "        input_texts, output_texts = read_gen_fewshot_file(train_file_name)\n",
    "        shuffle = True\n",
    "        batch_size = train_batch_size\n",
    "\n",
    "    if dev_file_name is not None:\n",
    "        input_texts, output_texts = read_gen_fewshot_file(dev_file_name)\n",
    "        shuffle = False\n",
    "        batch_size = eval_batch_size\n",
    "\n",
    "    if test_file_name is not None:\n",
    "        input_texts, output_texts = read_gen_fewshot_file(test_file_name)\n",
    "        shuffle = False\n",
    "        batch_size = eval_batch_size\n",
    "\n",
    "    data = model.prepare_text(input_texts, output_texts)\n",
    "    dataset = DictDataset(data)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAMetricModel:\n",
    "    \"\"\"Load and cache a model used for evaluating generative text\n",
    "    generation.\"\"\"\n",
    "\n",
    "    model_id = \"sentence-transformers/sentence-t5-xl\"\n",
    "\n",
    "    def __init__(self, device: str = \"cuda:0\", batch_size: int = 16) -> None:\n",
    "        \"\"\"Save the gpu device and construct the model and cache it.\"\"\"\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.metric_model = SentenceTransformer(self.model_id, device=self.device).eval()\n",
    "\n",
    "    def compute_metric(self, predictions: List[str], references: List[List[str]]) -> float:\n",
    "        \"\"\"Compute the metric for the given predictions and multiple\n",
    "        references.\"\"\"\n",
    "        average_score = torch.tensor(0.0, device=self.device)\n",
    "        num_chunks = max(len(predictions) // self.batch_size, 1)\n",
    "        for chunk_i in range(num_chunks):\n",
    "            clear_cache()\n",
    "\n",
    "            if (chunk_i + 1) * self.batch_size <= len(predictions):\n",
    "                predictions_sub_arr = predictions[chunk_i * self.batch_size : (chunk_i + 1) * self.batch_size]\n",
    "                references_sub_arr = references[chunk_i * self.batch_size : (chunk_i + 1) * self.batch_size]\n",
    "            else:\n",
    "                predictions_sub_arr = predictions[chunk_i * self.batch_size :]\n",
    "                references_sub_arr = references[chunk_i * self.batch_size :]\n",
    "\n",
    "            # need to track multiple references.\n",
    "            ref_sub_arr_len = [len(ref_sub_arr) for ref_sub_arr in references_sub_arr]\n",
    "            references_sub_arr_flattened = []\n",
    "            for ref_sub_arr in references_sub_arr:\n",
    "                references_sub_arr_flattened.extend(ref_sub_arr)\n",
    "\n",
    "            prediction_embeddings = self.metric_model.encode(\n",
    "                predictions_sub_arr,\n",
    "                show_progress_bar=False,\n",
    "                batch_size=self.batch_size,\n",
    "                device=self.device,\n",
    "                normalize_embeddings=True,\n",
    "                convert_to_tensor=True,\n",
    "            )\n",
    "\n",
    "            references_embeddings = self.metric_model.encode(\n",
    "                references_sub_arr_flattened,\n",
    "                show_progress_bar=False,\n",
    "                batch_size=self.batch_size,\n",
    "                device=self.device,\n",
    "                normalize_embeddings=True,\n",
    "                convert_to_tensor=True,\n",
    "            )\n",
    "            dot_products = torch.matmul(prediction_embeddings, references_embeddings.t())\n",
    "            score_collector = torch.zeros_like(dot_products)\n",
    "            i = 0\n",
    "            j = 0\n",
    "            while i < len(predictions_sub_arr):\n",
    "                j_len = ref_sub_arr_len[i]\n",
    "                score_collector[i][j : j + j_len] = 1.0 / j_len\n",
    "                i += 1\n",
    "                j += j_len\n",
    "\n",
    "            average_score += torch.sum(dot_products * score_collector)\n",
    "        return (average_score / len(predictions)).item()\n",
    "\n",
    "\n",
    "qa_metric_model = None\n",
    "\n",
    "\n",
    "def postprocess_qa(label: str) -> str:\n",
    "    label = str(label)\n",
    "    label = label.lower()\n",
    "    label = label.replace(\"\\n\", \" \")\n",
    "    label = label.removesuffix(\"</s>\")\n",
    "    label = label.removeprefix(\"<s>\")\n",
    "    label = label.removeprefix(\"\\n\")\n",
    "    label = label.removesuffix(\"\\n\")\n",
    "    label = label.removeprefix(\".\")\n",
    "    label = label.removesuffix(\".\")\n",
    "    label = label.removeprefix(\"answer:\")\n",
    "    label = label.removeprefix(\",\")\n",
    "    label = label.strip()\n",
    "    if \"no answer\" in label or \"no_answer\" in label:\n",
    "        label = \"no answer\"\n",
    "    return label\n",
    "\n",
    "\n",
    "def qa_metric(prediction_file: str) -> Dict[str, float]:\n",
    "    \"\"\"Compute the metric for the qa task.\"\"\"\n",
    "    global qa_metric_model\n",
    "    if qa_metric_model is None:\n",
    "        qa_metric_model = QAMetricModel(device=metric_device, batch_size=metric_batch_size)\n",
    "\n",
    "    df = pd.read_csv(prediction_file, delimiter=\",\")\n",
    "\n",
    "    gold_answers = [postprocess_qa(label) for label in df[\"gold_answer\"].tolist()]\n",
    "\n",
    "    multiple_gold_answers = []\n",
    "    for answer in gold_answers:\n",
    "        multiple_gold_answers.append(answer.split(\"[<@>]\"))\n",
    "\n",
    "    return_metrics: Dict[str, float] = {}\n",
    "    metrics = {\n",
    "        \"potential_answer\": \"qa_score\",\n",
    "    }\n",
    "\n",
    "    for metric_column, metric in metrics.items():\n",
    "        if metric_column in df.columns:\n",
    "            predictions = [postprocess_qa(pred) for pred in df[metric_column].tolist()]\n",
    "            score = qa_metric_model.compute_metric(predictions, multiple_gold_answers)\n",
    "            return_metrics[metric] = score\n",
    "\n",
    "    return return_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and start training.\n",
    "set_random_seed(42)\n",
    "\n",
    "model = T5BaseQA(device=\"cuda:0\", seed=42)\n",
    "model.to_device()\n",
    "train_dataloader = create_dataloader(model, train_file_name=train_file_name)\n",
    "dev_dataloader = create_dataloader(model, dev_file_name=dev_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Prediction Step: 9.\n",
      "Prediction Step: 10.\n",
      "Prediction Step: 11.\n",
      "Prediction Step: 12.\n",
      "Prediction Step: 13.\n",
      "Prediction Step: 14.\n",
      "Prediction Step: 15.\n",
      "Prediction Step: 16.\n",
      "\n",
      "Epoch:0\n",
      "\n",
      "Epoch: 0 | Batch: 1 | Mean Loss: 45.189735412597656 | Epoch Loss: 45.189735412597656 | Loss: 45.189735412597656\n",
      "\n",
      "Epoch: 0 | Batch: 2 | Mean Loss: 31.89446449279785 | Epoch Loss: 31.89446449279785 | Loss: 18.599193572998047\n",
      "\n",
      "Epoch: 0 | Batch: 3 | Mean Loss: 25.977359453837078 | Epoch Loss: 25.977359453837078 | Loss: 14.143149375915527\n",
      "\n",
      "Epoch: 0 | Batch: 4 | Mean Loss: 22.50176763534546 | Epoch Loss: 22.50176763534546 | Loss: 12.074992179870605\n",
      "\n",
      "Epoch: 0 | Batch: 5 | Mean Loss: 19.720742797851564 | Epoch Loss: 19.720742797851564 | Loss: 8.596643447875977\n",
      "\n",
      "Epoch: 0 | Batch: 6 | Mean Loss: 18.24715518951416 | Epoch Loss: 18.24715518951416 | Loss: 10.879217147827148\n",
      "\n",
      "Epoch: 0 | Batch: 7 | Mean Loss: 16.645410469600133 | Epoch Loss: 16.645410469600133 | Loss: 7.034942150115967\n",
      "\n",
      "Epoch: 0 | Batch: 8 | Mean Loss: 15.468935370445251 | Epoch Loss: 15.468935370445251 | Loss: 7.233609676361084\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Prediction Step: 9.\n",
      "Prediction Step: 10.\n",
      "Prediction Step: 11.\n",
      "Prediction Step: 12.\n",
      "Prediction Step: 13.\n",
      "Prediction Step: 14.\n",
      "Prediction Step: 15.\n",
      "Prediction Step: 16.\n",
      "Epoch: 0 | Batch: 9 | Mean Loss: 14.488691647847494 | Epoch Loss: 14.488691647847494 | Loss: 6.64674186706543\n",
      "\n",
      "Epoch: 0 | Batch: 10 | Mean Loss: 13.498787689208985 | Epoch Loss: 13.498787689208985 | Loss: 4.589652061462402\n",
      "\n",
      "Epoch: 0 | Batch: 11 | Mean Loss: 12.908428755673496 | Epoch Loss: 12.908428755673496 | Loss: 7.0048394203186035\n",
      "\n",
      "Epoch: 0 | Batch: 12 | Mean Loss: 12.305516560872396 | Epoch Loss: 12.305516560872396 | Loss: 5.673482418060303\n",
      "\n",
      "Epoch: 0 | Batch: 13 | Mean Loss: 11.951250039614164 | Epoch Loss: 11.951250039614164 | Loss: 7.700051784515381\n",
      "\n",
      "Epoch: 0 | Batch: 14 | Mean Loss: 11.354745524270195 | Epoch Loss: 11.354745524270195 | Loss: 3.600186824798584\n",
      "\n",
      "Epoch: 0 | Batch: 15 | Mean Loss: 10.99602559407552 | Epoch Loss: 10.99602559407552 | Loss: 5.973946571350098\n",
      "\n",
      "Epoch: 0 | Batch: 16 | Mean Loss: 10.614920169115067 | Epoch Loss: 10.614920169115067 | Loss: 4.898338794708252\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Prediction Step: 9.\n",
      "Prediction Step: 10.\n",
      "Prediction Step: 11.\n",
      "Prediction Step: 12.\n",
      "Prediction Step: 13.\n",
      "Prediction Step: 14.\n",
      "Prediction Step: 15.\n",
      "Prediction Step: 16.\n",
      "\n",
      "Epoch:1\n",
      "\n",
      "Epoch: 1 | Batch: 1 | Mean Loss: 10.298819401684929 | Epoch Loss: 5.241207122802734 | Loss: 5.241207122802734\n",
      "\n",
      "Epoch: 1 | Batch: 2 | Mean Loss: 9.759246249993643 | Epoch Loss: 2.9138548970222473 | Loss: 0.5865026712417603\n",
      "\n",
      "Epoch: 1 | Batch: 3 | Mean Loss: 9.37050937978845 | Epoch Loss: 2.7336518367131553 | Loss: 2.3732457160949707\n",
      "\n",
      "Epoch: 1 | Batch: 4 | Mean Loss: 9.055096071958541 | Epoch Loss: 2.8157996833324432 | Loss: 3.0622432231903076\n",
      "\n",
      "Epoch: 1 | Batch: 5 | Mean Loss: 8.746964971224466 | Epoch Loss: 2.7695083379745484 | Loss: 2.5843429565429688\n",
      "\n",
      "Epoch: 1 | Batch: 6 | Mean Loss: 8.450638321312992 | Epoch Loss: 2.6792200605074563 | Loss: 2.227778673171997\n",
      "\n",
      "Epoch: 1 | Batch: 7 | Mean Loss: 8.209353576535763 | Epoch Loss: 2.710915650640215 | Loss: 2.9010891914367676\n",
      "\n",
      "Epoch: 1 | Batch: 8 | Mean Loss: 7.9838149299224215 | Epoch Loss: 2.7216044515371323 | Loss: 2.7964260578155518\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Prediction Step: 9.\n",
      "Prediction Step: 10.\n",
      "Prediction Step: 11.\n",
      "Prediction Step: 12.\n",
      "Prediction Step: 13.\n",
      "Prediction Step: 14.\n",
      "Prediction Step: 15.\n",
      "Prediction Step: 16.\n",
      "Epoch: 1 | Batch: 9 | Mean Loss: 7.844640955924988 | Epoch Loss: 2.919700132475959 | Loss: 4.504465579986572\n",
      "\n",
      "Epoch: 1 | Batch: 10 | Mean Loss: 7.687179194046901 | Epoch Loss: 3.0027936339378356 | Loss: 3.7506351470947266\n",
      "\n",
      "Epoch: 1 | Batch: 11 | Mean Loss: 7.508989576940183 | Epoch Loss: 2.991272351958535 | Loss: 2.8760595321655273\n",
      "\n",
      "Epoch: 1 | Batch: 12 | Mean Loss: 7.3791914369378775 | Epoch Loss: 3.0648864607016244 | Loss: 3.8746416568756104\n",
      "\n",
      "Epoch: 1 | Batch: 13 | Mean Loss: 7.272534999354132 | Epoch Loss: 3.1588301750329824 | Loss: 4.286154747009277\n",
      "\n",
      "Epoch: 1 | Batch: 14 | Mean Loss: 7.152374311288198 | Epoch Loss: 3.195179045200348 | Loss: 3.6677143573760986\n",
      "\n",
      "Epoch: 1 | Batch: 15 | Mean Loss: 7.036709327851573 | Epoch Loss: 3.219951097170512 | Loss: 3.5667598247528076\n",
      "\n",
      "Epoch: 1 | Batch: 16 | Mean Loss: 6.933599550276995 | Epoch Loss: 3.252278931438923 | Loss: 3.737196445465088\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Prediction Step: 9.\n",
      "Prediction Step: 10.\n",
      "Prediction Step: 11.\n",
      "Prediction Step: 12.\n",
      "Prediction Step: 13.\n",
      "Prediction Step: 14.\n",
      "Prediction Step: 15.\n",
      "Prediction Step: 16.\n",
      "\n",
      "Epoch:2\n",
      "\n",
      "Epoch: 2 | Batch: 1 | Mean Loss: 6.749407401590636 | Epoch Loss: 0.8552586436271667 | Loss: 0.8552586436271667\n",
      "\n",
      "Epoch: 2 | Batch: 2 | Mean Loss: 6.586242021883235 | Epoch Loss: 1.028521567583084 | Loss: 1.2017844915390015\n",
      "\n",
      "Epoch: 2 | Batch: 3 | Mean Loss: 6.487264181886401 | Epoch Loss: 1.7263535857200623 | Loss: 3.1220176219940186\n",
      "\n",
      "Epoch: 2 | Batch: 4 | Mean Loss: 6.348020240664482 | Epoch Loss: 1.6633857637643814 | Loss: 1.4744822978973389\n",
      "\n",
      "Epoch: 2 | Batch: 5 | Mean Loss: 6.227199572163659 | Epoch Loss: 1.7062397122383117 | Loss: 1.8776555061340332\n",
      "\n",
      "Epoch: 2 | Batch: 6 | Mean Loss: 6.12186893507054 | Epoch Loss: 1.7926389873027802 | Loss: 2.224635362625122\n",
      "\n",
      "Epoch: 2 | Batch: 7 | Mean Loss: 6.044305054041056 | Epoch Loss: 1.978958785533905 | Loss: 3.0968775749206543\n",
      "\n",
      "Epoch: 2 | Batch: 8 | Mean Loss: 5.91577549725771 | Epoch Loss: 1.8444792851805687 | Loss: 0.9031227827072144\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Prediction Step: 9.\n",
      "Prediction Step: 10.\n",
      "Prediction Step: 11.\n",
      "Prediction Step: 12.\n",
      "Prediction Step: 13.\n",
      "Prediction Step: 14.\n",
      "Prediction Step: 15.\n",
      "Prediction Step: 16.\n",
      "Epoch: 2 | Batch: 9 | Mean Loss: 5.807217576154849 | Epoch Loss: 1.8023038903872173 | Loss: 1.4649007320404053\n",
      "\n",
      "Epoch: 2 | Batch: 10 | Mean Loss: 5.685299502951758 | Epoch Loss: 1.6907393515110016 | Loss: 0.686658501625061\n",
      "\n",
      "Epoch: 2 | Batch: 11 | Mean Loss: 5.614644348621368 | Epoch Loss: 1.777683761986819 | Loss: 2.647127866744995\n",
      "\n",
      "Epoch: 2 | Batch: 12 | Mean Loss: 5.509646790948781 | Epoch Loss: 1.7124394327402115 | Loss: 0.9947518110275269\n",
      "\n",
      "Epoch: 2 | Batch: 13 | Mean Loss: 5.42003667751948 | Epoch Loss: 1.6943434522702143 | Loss: 1.477191686630249\n",
      "\n",
      "Epoch: 2 | Batch: 14 | Mean Loss: 5.329993047144102 | Epoch Loss: 1.6646067542689187 | Loss: 1.2780296802520752\n",
      "\n",
      "Epoch: 2 | Batch: 15 | Mean Loss: 5.244236549164387 | Epoch Loss: 1.6402621467908223 | Loss: 1.2994376420974731\n",
      "\n",
      "Epoch: 2 | Batch: 16 | Mean Loss: 5.1673220582306385 | Epoch Loss: 1.634767074137926 | Loss: 1.5523409843444824\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Prediction Step: 9.\n",
      "Prediction Step: 10.\n",
      "Prediction Step: 11.\n",
      "Prediction Step: 12.\n",
      "Prediction Step: 13.\n",
      "Prediction Step: 14.\n",
      "Prediction Step: 15.\n",
      "Prediction Step: 16.\n",
      "\n",
      "Epoch:3\n",
      "\n",
      "Epoch: 3 | Batch: 1 | Mean Loss: 5.06772211741428 | Epoch Loss: 0.28692495822906494 | Loss: 0.28692495822906494\n",
      "\n",
      "Epoch: 3 | Batch: 2 | Mean Loss: 4.983929278850556 | Epoch Loss: 0.5825025737285614 | Loss: 0.8780801892280579\n",
      "\n",
      "Epoch: 3 | Batch: 3 | Mean Loss: 4.891827499749613 | Epoch Loss: 0.4839145640532176 | Loss: 0.2867385447025299\n",
      "\n",
      "Epoch: 3 | Batch: 4 | Mean Loss: 4.806097666231485 | Epoch Loss: 0.4714049622416496 | Loss: 0.4338761568069458\n",
      "\n",
      "Epoch: 3 | Batch: 5 | Mean Loss: 4.740816152883026 | Epoch Loss: 0.6463594615459443 | Loss: 1.3461774587631226\n",
      "\n",
      "Epoch: 3 | Batch: 6 | Mean Loss: 4.678460702300072 | Epoch Loss: 0.7675698548555374 | Loss: 1.3736218214035034\n",
      "\n",
      "Epoch: 3 | Batch: 7 | Mean Loss: 4.624496059526097 | Epoch Loss: 0.90226063983781 | Loss: 1.7104053497314453\n",
      "\n",
      "Epoch: 3 | Batch: 8 | Mean Loss: 4.570713314094713 | Epoch Loss: 0.9910608492791653 | Loss: 1.6126623153686523\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Prediction Step: 9.\n",
      "Prediction Step: 10.\n",
      "Prediction Step: 11.\n",
      "Prediction Step: 12.\n",
      "Prediction Step: 13.\n",
      "Prediction Step: 14.\n",
      "Prediction Step: 15.\n",
      "Prediction Step: 16.\n",
      "Epoch: 3 | Batch: 9 | Mean Loss: 4.520452497821105 | Epoch Loss: 1.0704815089702606 | Loss: 1.7058467864990234\n",
      "\n",
      "Epoch: 3 | Batch: 10 | Mean Loss: 4.457263390051907 | Epoch Loss: 1.0489817827939987 | Loss: 0.8554842472076416\n",
      "\n",
      "Epoch: 3 | Batch: 11 | Mean Loss: 4.3939801338365525 | Epoch Loss: 1.019397191025994 | Loss: 0.7235512733459473\n",
      "\n",
      "Epoch: 3 | Batch: 12 | Mean Loss: 4.329857963820299 | Epoch Loss: 0.9800015861789385 | Loss: 0.5466499328613281\n",
      "\n",
      "Epoch: 3 | Batch: 13 | Mean Loss: 4.274190374573723 | Epoch Loss: 0.9764733887635745 | Loss: 0.9341350197792053\n",
      "\n",
      "Epoch: 3 | Batch: 14 | Mean Loss: 4.2281975414483775 | Epoch Loss: 1.0083420553377695 | Loss: 1.4226347208023071\n",
      "\n",
      "Epoch: 3 | Batch: 15 | Mean Loss: 4.179856178779451 | Epoch Loss: 1.0199653645356497 | Loss: 1.1826916933059692\n",
      "\n",
      "Epoch: 3 | Batch: 16 | Mean Loss: 4.185423641931266 | Epoch Loss: 1.2397283930331469 | Loss: 4.5361738204956055\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Prediction Step: 9.\n",
      "Prediction Step: 10.\n",
      "Prediction Step: 11.\n",
      "Prediction Step: 12.\n",
      "Prediction Step: 13.\n",
      "Prediction Step: 14.\n",
      "Prediction Step: 15.\n",
      "Prediction Step: 16.\n",
      "\n",
      "Epoch:4\n",
      "\n",
      "Epoch: 4 | Batch: 1 | Mean Loss: 4.123711399390147 | Epoch Loss: 0.17412787675857544 | Loss: 0.17412787675857544\n",
      "\n",
      "Epoch: 4 | Batch: 2 | Mean Loss: 4.063668029552156 | Epoch Loss: 0.1674884334206581 | Loss: 0.16084899008274078\n",
      "\n",
      "Epoch: 4 | Batch: 3 | Mean Loss: 4.0088818782745905 | Epoch Loss: 0.2426575869321823 | Loss: 0.3929958939552307\n",
      "\n",
      "Epoch: 4 | Batch: 4 | Mean Loss: 4.003549691289663 | Epoch Loss: 1.0935664810240269 | Loss: 3.6462931632995605\n",
      "\n",
      "Epoch: 4 | Batch: 5 | Mean Loss: 3.957177305351133 | Epoch Loss: 1.035624197125435 | Loss: 0.8038550615310669\n",
      "\n",
      "Epoch: 4 | Batch: 6 | Mean Loss: 3.9029653155377932 | Epoch Loss: 0.8900765006740888 | Loss: 0.1623380184173584\n",
      "\n",
      "Epoch: 4 | Batch: 7 | Mean Loss: 3.853424860767915 | Epoch Loss: 0.8180074329887118 | Loss: 0.3855930268764496\n",
      "\n",
      "Epoch: 4 | Batch: 8 | Mean Loss: 3.8099097901334367 | Epoch Loss: 0.805798975750804 | Loss: 0.7203397750854492\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Prediction Step: 9.\n",
      "Prediction Step: 10.\n",
      "Prediction Step: 11.\n",
      "Prediction Step: 12.\n",
      "Prediction Step: 13.\n",
      "Prediction Step: 14.\n",
      "Prediction Step: 15.\n",
      "Prediction Step: 16.\n",
      "Epoch: 4 | Batch: 9 | Mean Loss: 3.7623911955176967 | Epoch Loss: 0.7541604654656516 | Loss: 0.341052383184433\n",
      "\n",
      "Epoch: 4 | Batch: 10 | Mean Loss: 3.7240958300393983 | Epoch Loss: 0.7715978339314461 | Loss: 0.9285341501235962\n",
      "\n",
      "Epoch: 4 | Batch: 11 | Mean Loss: 3.6883353890975314 | Epoch Loss: 0.7961855544285341 | Loss: 1.042062759399414\n",
      "\n",
      "Epoch: 4 | Batch: 12 | Mean Loss: 3.653422818960328 | Epoch Loss: 0.8160850964486599 | Loss: 1.034980058670044\n",
      "\n",
      "Epoch: 4 | Batch: 13 | Mean Loss: 3.6364930842991 | Epoch Loss: 0.9340657236484381 | Loss: 2.3498332500457764\n",
      "\n",
      "Epoch: 4 | Batch: 14 | Mean Loss: 3.5916537506840167 | Epoch Loss: 0.8772771049823079 | Loss: 0.13902506232261658\n",
      "\n",
      "Epoch: 4 | Batch: 15 | Mean Loss: 3.5587470012752314 | Epoch Loss: 0.8849266678094864 | Loss: 0.9920205473899841\n",
      "\n",
      "Epoch: 4 | Batch: 16 | Mean Loss: 3.5314380103722214 | Epoch Loss: 0.915495484136045 | Loss: 1.3740277290344238\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Prediction Step: 9.\n",
      "Prediction Step: 10.\n",
      "Prediction Step: 11.\n",
      "Prediction Step: 12.\n",
      "Prediction Step: 13.\n",
      "Prediction Step: 14.\n",
      "Prediction Step: 15.\n",
      "Prediction Step: 16.\n",
      "\n",
      "Epoch:5\n",
      "\n",
      "Epoch: 5 | Batch: 1 | Mean Loss: 3.495850346154637 | Epoch Loss: 0.6488372087478638 | Loss: 0.6488372087478638\n",
      "\n",
      "Epoch: 5 | Batch: 2 | Mean Loss: 3.4594081227735773 | Epoch Loss: 0.5782126188278198 | Loss: 0.5075880289077759\n",
      "\n",
      "Epoch: 5 | Batch: 3 | Mean Loss: 3.422282022345497 | Epoch Loss: 0.5114556749661764 | Loss: 0.3779417872428894\n",
      "\n",
      "Epoch: 5 | Batch: 4 | Mean Loss: 3.3930407154063382 | Epoch Loss: 0.6250948160886765 | Loss: 0.9660122394561768\n",
      "\n",
      "Epoch: 5 | Batch: 5 | Mean Loss: 3.353433837916921 | Epoch Loss: 0.5053670786321163 | Loss: 0.026456128805875778\n",
      "\n",
      "Epoch: 5 | Batch: 6 | Mean Loss: 3.3295866517393393 | Epoch Loss: 0.6382352033009132 | Loss: 1.3025758266448975\n",
      "\n",
      "Epoch: 5 | Batch: 7 | Mean Loss: 3.2923344195145985 | Epoch Loss: 0.5597219525703362 | Loss: 0.08864244818687439\n",
      "\n",
      "Epoch: 5 | Batch: 8 | Mean Loss: 3.256449065382846 | Epoch Loss: 0.5065596154890954 | Loss: 0.13442325592041016\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Prediction Step: 9.\n",
      "Prediction Step: 10.\n",
      "Prediction Step: 11.\n",
      "Prediction Step: 12.\n",
      "Prediction Step: 13.\n",
      "Prediction Step: 14.\n",
      "Prediction Step: 15.\n",
      "Prediction Step: 16.\n",
      "Epoch: 5 | Batch: 9 | Mean Loss: 3.221351763463757 | Epoch Loss: 0.4650295687218507 | Loss: 0.13278919458389282\n",
      "\n",
      "Epoch: 5 | Batch: 10 | Mean Loss: 3.186388953899344 | Epoch Loss: 0.4259965021163225 | Loss: 0.07469890266656876\n",
      "\n",
      "Epoch: 5 | Batch: 11 | Mean Loss: 3.15635186077638 | Epoch Loss: 0.42845259098844096 | Loss: 0.45301347970962524\n",
      "\n",
      "Epoch: 5 | Batch: 12 | Mean Loss: 3.123903287210218 | Epoch Loss: 0.40700513279686373 | Loss: 0.17108309268951416\n",
      "\n",
      "Epoch: 5 | Batch: 13 | Mean Loss: 3.0917971866826215 | Epoch Loss: 0.3863151947466227 | Loss: 0.13803593814373016\n",
      "\n",
      "Epoch: 5 | Batch: 14 | Mean Loss: 3.0597082918232426 | Epoch Loss: 0.3641099001147917 | Loss: 0.07544106990098953\n",
      "\n",
      "Epoch: 5 | Batch: 15 | Mean Loss: 3.0310108033842162 | Epoch Loss: 0.36206569944818817 | Loss: 0.3334468901157379\n",
      "\n",
      "Epoch: 5 | Batch: 16 | Mean Loss: 3.0147230227400237 | Epoch Loss: 0.43114808457903564 | Loss: 1.467383861541748\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Prediction Step: 9.\n",
      "Prediction Step: 10.\n",
      "Prediction Step: 11.\n",
      "Prediction Step: 12.\n",
      "Prediction Step: 13.\n",
      "Prediction Step: 14.\n",
      "Prediction Step: 15.\n",
      "Prediction Step: 16.\n",
      "\n",
      "Epoch:6\n",
      "\n",
      "Epoch: 6 | Batch: 1 | Mean Loss: 2.992068043650733 | Epoch Loss: 0.8171900510787964 | Loss: 0.8171900510787964\n",
      "\n",
      "Epoch: 6 | Batch: 2 | Mean Loss: 2.9642162194893675 | Epoch Loss: 0.5398896634578705 | Loss: 0.2625892758369446\n",
      "\n",
      "Epoch: 6 | Batch: 3 | Mean Loss: 2.9436403169400163 | Epoch Loss: 0.6689937313397726 | Loss: 0.9272018671035767\n",
      "\n",
      "Epoch: 6 | Batch: 4 | Mean Loss: 2.9143952506035564 | Epoch Loss: 0.5065287193283439 | Loss: 0.019133683294057846\n",
      "\n",
      "Epoch: 6 | Batch: 5 | Mean Loss: 2.887181561403345 | Epoch Loss: 0.43838550373911855 | Loss: 0.1658126413822174\n",
      "\n",
      "Epoch: 6 | Batch: 6 | Mean Loss: 2.8601065603103124 | Epoch Loss: 0.3862431614349286 | Loss: 0.12553144991397858\n",
      "\n",
      "Epoch: 6 | Batch: 7 | Mean Loss: 2.836593407233363 | Epoch Loss: 0.3936729659991605 | Loss: 0.438251793384552\n",
      "\n",
      "Epoch: 6 | Batch: 8 | Mean Loss: 2.8117486285045743 | Epoch Loss: 0.3760558976791799 | Loss: 0.2527364194393158\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Prediction Step: 9.\n",
      "Prediction Step: 10.\n",
      "Prediction Step: 11.\n",
      "Prediction Step: 12.\n",
      "Prediction Step: 13.\n",
      "Prediction Step: 14.\n",
      "Prediction Step: 15.\n",
      "Prediction Step: 16.\n",
      "Epoch: 6 | Batch: 9 | Mean Loss: 2.789435669424988 | Epoch Loss: 0.38637056739793885 | Loss: 0.46888792514801025\n",
      "\n",
      "Epoch: 6 | Batch: 10 | Mean Loss: 2.7678837428256027 | Epoch Loss: 0.39822665564715864 | Loss: 0.5049314498901367\n",
      "\n",
      "Epoch: 6 | Batch: 11 | Mean Loss: 2.742443706735829 | Epoch Loss: 0.36618785797195 | Loss: 0.04579988121986389\n",
      "\n",
      "Epoch: 6 | Batch: 12 | Mean Loss: 2.7336812000721693 | Epoch Loss: 0.4853466187293331 | Loss: 1.7960929870605469\n",
      "\n",
      "Epoch: 6 | Batch: 13 | Mean Loss: 2.7088440011058927 | Epoch Loss: 0.45004507211538464 | Loss: 0.026426512748003006\n",
      "\n",
      "Epoch: 6 | Batch: 14 | Mean Loss: 2.6946045929396694 | Epoch Loss: 0.49950678859438213 | Loss: 1.14250910282135\n",
      "\n",
      "Epoch: 6 | Batch: 15 | Mean Loss: 2.6716152610475414 | Epoch Loss: 0.475725586215655 | Loss: 0.14278875291347504\n",
      "\n",
      "Epoch: 6 | Batch: 16 | Mean Loss: 2.648257569409907 | Epoch Loss: 0.44946484942920506 | Loss: 0.055553797632455826\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Prediction Step: 9.\n",
      "Prediction Step: 10.\n",
      "Prediction Step: 11.\n",
      "Prediction Step: 12.\n",
      "Prediction Step: 13.\n",
      "Prediction Step: 14.\n",
      "Prediction Step: 15.\n",
      "Prediction Step: 16.\n",
      "\n",
      "Epoch:7\n",
      "\n",
      "Epoch: 7 | Batch: 1 | Mean Loss: 2.6301583709706247 | Epoch Loss: 0.6030481457710266 | Loss: 0.6030481457710266\n",
      "\n",
      "Epoch: 7 | Batch: 2 | Mean Loss: 2.6074812385792794 | Epoch Loss: 0.324006712064147 | Loss: 0.04496527835726738\n",
      "\n",
      "Epoch: 7 | Batch: 3 | Mean Loss: 2.585147025209406 | Epoch Loss: 0.22902004172404608 | Loss: 0.03904670104384422\n",
      "\n",
      "Epoch: 7 | Batch: 4 | Mean Loss: 2.5636764531633975 | Epoch Loss: 0.19540519826114178 | Loss: 0.0945606678724289\n",
      "\n",
      "Epoch: 7 | Batch: 5 | Mean Loss: 2.5422915399838715 | Epoch Loss: 0.168652480840683 | Loss: 0.06164161115884781\n",
      "\n",
      "Epoch: 7 | Batch: 6 | Mean Loss: 2.5223875929743556 | Epoch Loss: 0.17281469951073328 | Loss: 0.1936257928609848\n",
      "\n",
      "Epoch: 7 | Batch: 7 | Mean Loss: 2.502304710635618 | Epoch Loss: 0.1670589702469962 | Loss: 0.13252459466457367\n",
      "\n",
      "Epoch: 7 | Batch: 8 | Mean Loss: 2.482136497522394 | Epoch Loss: 0.15644149109721184 | Loss: 0.08211913704872131\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Prediction Step: 9.\n",
      "Prediction Step: 10.\n",
      "Prediction Step: 11.\n",
      "Prediction Step: 12.\n",
      "Prediction Step: 13.\n",
      "Prediction Step: 14.\n",
      "Prediction Step: 15.\n",
      "Prediction Step: 16.\n",
      "Epoch: 7 | Batch: 9 | Mean Loss: 2.4623815562606843 | Epoch Loss: 0.1492578370703591 | Loss: 0.09178860485553741\n",
      "\n",
      "Epoch: 7 | Batch: 10 | Mean Loss: 2.444332290135446 | Epoch Loss: 0.16036916226148606 | Loss: 0.2603710889816284\n",
      "\n",
      "Epoch: 7 | Batch: 11 | Mean Loss: 2.4250281674832834 | Epoch Loss: 0.1521469842303883 | Loss: 0.0699252039194107\n",
      "\n",
      "Epoch: 7 | Batch: 12 | Mean Loss: 2.4057372460562374 | Epoch Loss: 0.14221422808865705 | Loss: 0.032953910529613495\n",
      "\n",
      "Epoch: 7 | Batch: 13 | Mean Loss: 2.406215524494648 | Epoch Loss: 0.3209302144554945 | Loss: 2.465522050857544\n",
      "\n",
      "Epoch: 7 | Batch: 14 | Mean Loss: 2.3874353548837086 | Epoch Loss: 0.30085763867412296 | Loss: 0.03991415351629257\n",
      "\n",
      "Epoch: 7 | Batch: 15 | Mean Loss: 2.3688119481075702 | Epoch Loss: 0.28228464238345624 | Loss: 0.0222626943141222\n",
      "\n",
      "Epoch: 7 | Batch: 16 | Mean Loss: 2.3507731099816738 | Epoch Loss: 0.2683818939840421 | Loss: 0.059840667992830276\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Prediction Step: 9.\n",
      "Prediction Step: 10.\n",
      "Prediction Step: 11.\n",
      "Prediction Step: 12.\n",
      "Prediction Step: 13.\n",
      "Prediction Step: 14.\n",
      "Prediction Step: 15.\n",
      "Prediction Step: 16.\n",
      "\n",
      "Epoch:8\n",
      "\n",
      "Epoch: 8 | Batch: 1 | Mean Loss: 2.3328589089936758 | Epoch Loss: 0.0398411825299263 | Loss: 0.0398411825299263\n",
      "\n",
      "Epoch: 8 | Batch: 2 | Mean Loss: 2.316461632099862 | Epoch Loss: 0.12052704766392708 | Loss: 0.20121291279792786\n",
      "\n",
      "Epoch: 8 | Batch: 3 | Mean Loss: 2.2989089308832664 | Epoch Loss: 0.0860372893512249 | Loss: 0.01705777272582054\n",
      "\n",
      "Epoch: 8 | Batch: 4 | Mean Loss: 2.281817602261788 | Epoch Loss: 0.07524135522544384 | Loss: 0.04285355284810066\n",
      "\n",
      "Epoch: 8 | Batch: 5 | Mean Loss: 2.2648582213551256 | Epoch Loss: 0.06543707251548767 | Loss: 0.026219941675662994\n",
      "\n",
      "Epoch: 8 | Batch: 6 | Mean Loss: 2.2487717905123508 | Epoch Loss: 0.07274364183346431 | Loss: 0.10927648842334747\n",
      "\n",
      "Epoch: 8 | Batch: 7 | Mean Loss: 2.2322621569037437 | Epoch Loss: 0.06520472919302327 | Loss: 0.019971253350377083\n",
      "\n",
      "Epoch: 8 | Batch: 8 | Mean Loss: 2.2160279244622765 | Epoch Loss: 0.06010495615191758 | Loss: 0.024406544864177704\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Prediction Step: 9.\n",
      "Prediction Step: 10.\n",
      "Prediction Step: 11.\n",
      "Prediction Step: 12.\n",
      "Prediction Step: 13.\n",
      "Prediction Step: 14.\n",
      "Prediction Step: 15.\n",
      "Prediction Step: 16.\n",
      "Epoch: 8 | Batch: 9 | Mean Loss: 2.201535701751709 | Epoch Loss: 0.07904811803665426 | Loss: 0.23059341311454773\n",
      "\n",
      "Epoch: 8 | Batch: 10 | Mean Loss: 2.1865235459113466 | Epoch Loss: 0.08412912581115961 | Loss: 0.12985819578170776\n",
      "\n",
      "Epoch: 8 | Batch: 11 | Mean Loss: 2.171265459425158 | Epoch Loss: 0.08244916204024445 | Loss: 0.06564952433109283\n",
      "\n",
      "Epoch: 8 | Batch: 12 | Mean Loss: 2.1562149624739373 | Epoch Loss: 0.08092805572474997 | Loss: 0.06419588625431061\n",
      "\n",
      "Epoch: 8 | Batch: 13 | Mean Loss: 2.1410572715499936 | Epoch Loss: 0.07616286237652485 | Loss: 0.018980542197823524\n",
      "\n",
      "Epoch: 8 | Batch: 14 | Mean Loss: 2.1261707698322936 | Epoch Loss: 0.07266365989510502 | Loss: 0.027174027636647224\n",
      "\n",
      "Epoch: 8 | Batch: 15 | Mean Loss: 2.112389381401814 | Epoch Loss: 0.07818156418701012 | Loss: 0.15543222427368164\n",
      "\n",
      "Epoch: 8 | Batch: 16 | Mean Loss: 2.0998322542291135 | Epoch Loss: 0.09230540820863098 | Loss: 0.3041630685329437\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Prediction Step: 9.\n",
      "Prediction Step: 10.\n",
      "Prediction Step: 11.\n",
      "Prediction Step: 12.\n",
      "Prediction Step: 13.\n",
      "Prediction Step: 14.\n",
      "Prediction Step: 15.\n",
      "Prediction Step: 16.\n",
      "\n",
      "Epoch:9\n",
      "\n",
      "Epoch: 9 | Batch: 1 | Mean Loss: 2.0914210887561584 | Epoch Loss: 0.8802132606506348 | Loss: 0.8802132606506348\n",
      "\n",
      "Epoch: 9 | Batch: 2 | Mean Loss: 2.07829463162957 | Epoch Loss: 0.5275858044624329 | Loss: 0.17495834827423096\n",
      "\n",
      "Epoch: 9 | Batch: 3 | Mean Loss: 2.065064755097336 | Epoch Loss: 0.3962247967720032 | Loss: 0.1335027813911438\n",
      "\n",
      "Epoch: 9 | Batch: 4 | Mean Loss: 2.051526131611821 | Epoch Loss: 0.31250571738928556 | Loss: 0.061348479241132736\n",
      "\n",
      "Epoch: 9 | Batch: 5 | Mean Loss: 2.0379503272323802 | Epoch Loss: 0.2557508297264576 | Loss: 0.02873127907514572\n",
      "\n",
      "Epoch: 9 | Batch: 6 | Mean Loss: 2.0245327605555454 | Epoch Loss: 0.21734491238991419 | Loss: 0.02531532570719719\n",
      "\n",
      "Epoch: 9 | Batch: 7 | Mean Loss: 2.011304999979225 | Epoch Loss: 0.19017291255295277 | Loss: 0.027140913531184196\n",
      "\n",
      "Epoch: 9 | Batch: 8 | Mean Loss: 1.998214161501413 | Epoch Loss: 0.16908849240280688 | Loss: 0.02149755135178566\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Prediction Step: 9.\n",
      "Prediction Step: 10.\n",
      "Prediction Step: 11.\n",
      "Prediction Step: 12.\n",
      "Prediction Step: 13.\n",
      "Prediction Step: 14.\n",
      "Prediction Step: 15.\n",
      "Prediction Step: 16.\n",
      "Epoch: 9 | Batch: 9 | Mean Loss: 1.9885075078558883 | Epoch Loss: 0.2073115658842855 | Loss: 0.5130961537361145\n",
      "\n",
      "Epoch: 9 | Batch: 10 | Mean Loss: 1.975855903712089 | Epoch Loss: 0.19059645626693963 | Loss: 0.040160469710826874\n",
      "\n",
      "Epoch: 9 | Batch: 11 | Mean Loss: 1.9633998127113428 | Epoch Loss: 0.1773751237514344 | Loss: 0.04516179859638214\n",
      "\n",
      "Epoch: 9 | Batch: 12 | Mean Loss: 1.9509525312564502 | Epoch Loss: 0.1643958555844923 | Loss: 0.02162390574812889\n",
      "\n",
      "Epoch: 9 | Batch: 13 | Mean Loss: 1.9386876895549192 | Epoch Loss: 0.15370174239461237 | Loss: 0.02537238411605358\n",
      "\n",
      "Epoch: 9 | Batch: 14 | Mean Loss: 1.9265966305886455 | Epoch Loss: 0.1447445017152599 | Loss: 0.028300372883677483\n",
      "\n",
      "Epoch: 9 | Batch: 15 | Mean Loss: 1.914746108705049 | Epoch Loss: 0.13791911167403062 | Loss: 0.04236365109682083\n",
      "\n",
      "Epoch: 9 | Batch: 16 | Mean Loss: 1.9033119475818239 | Epoch Loss: 0.13462918775621802 | Loss: 0.08528032898902893\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Prediction Step: 9.\n",
      "Prediction Step: 10.\n",
      "Prediction Step: 11.\n",
      "Prediction Step: 12.\n",
      "Prediction Step: 13.\n",
      "Prediction Step: 14.\n",
      "Prediction Step: 15.\n",
      "Prediction Step: 16.\n",
      "Training finished in 801.8349390029907 seconds!\n"
     ]
    }
   ],
   "source": [
    "train_loop(\n",
    "    model=model,\n",
    "    mode=\"train\",\n",
    "    model_path=model_path,\n",
    "    metric_to_save=\"qa_score\",\n",
    "    max_epochs=10,\n",
    "    training_steps=100000,  # not important\n",
    "    steps_per_checkpoint=8,\n",
    "    metric=qa_metric,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=dev_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Prediction Step: 9.\n",
      "Prediction Step: 10.\n",
      "Prediction Step: 11.\n",
      "Prediction Step: 12.\n",
      "Prediction Step: 13.\n",
      "Prediction Step: 14.\n",
      "Prediction Step: 15.\n",
      "Prediction Step: 16.\n",
      "Prediction Step: 17.\n",
      "Prediction Step: 18.\n",
      "Prediction Step: 19.\n",
      "Prediction Step: 20.\n",
      "Prediction Step: 21.\n",
      "Prediction Step: 22.\n",
      "Prediction Step: 23.\n",
      "Prediction Step: 24.\n",
      "Prediction Step: 25.\n",
      "Prediction Step: 26.\n",
      "Prediction Step: 27.\n",
      "Prediction Step: 28.\n",
      "Prediction Step: 29.\n",
      "Prediction Step: 30.\n",
      "Prediction Step: 31.\n",
      "Prediction Step: 32.\n",
      "Prediction Step: 33.\n",
      "Prediction Step: 34.\n",
      "Prediction Step: 35.\n",
      "Prediction Step: 36.\n",
      "Prediction Step: 37.\n",
      "Prediction Step: 38.\n",
      "Prediction Step: 39.\n",
      "Prediction Step: 40.\n",
      "Prediction Step: 41.\n",
      "Prediction Step: 42.\n",
      "Prediction Step: 43.\n",
      "Prediction Step: 44.\n",
      "Prediction Step: 45.\n",
      "Prediction Step: 46.\n",
      "Prediction Step: 47.\n",
      "Prediction Step: 48.\n",
      "Prediction Step: 49.\n",
      "Prediction Step: 50.\n",
      "Prediction Step: 51.\n",
      "Prediction Step: 52.\n",
      "Prediction Step: 53.\n",
      "Prediction Step: 54.\n",
      "Prediction Step: 55.\n",
      "Prediction Step: 56.\n",
      "Prediction Step: 57.\n",
      "Prediction Step: 58.\n",
      "Prediction Step: 59.\n",
      "Prediction Step: 60.\n",
      "Prediction Step: 61.\n",
      "Prediction Step: 62.\n",
      "Prediction Step: 63.\n",
      "Prediction Step: 64.\n",
      "Prediction Step: 65.\n",
      "Prediction Step: 66.\n",
      "Prediction Step: 67.\n",
      "Prediction Step: 68.\n",
      "Prediction Step: 69.\n",
      "Prediction Step: 70.\n",
      "Prediction Step: 71.\n",
      "Prediction Step: 72.\n",
      "Prediction Step: 73.\n",
      "Prediction Step: 74.\n",
      "Prediction Step: 75.\n",
      "Prediction Step: 76.\n",
      "Prediction Step: 77.\n",
      "Prediction Step: 78.\n",
      "Prediction Step: 79.\n",
      "Prediction Step: 80.\n",
      "Prediction Step: 81.\n",
      "Prediction Step: 82.\n",
      "Prediction Step: 83.\n",
      "Prediction Step: 84.\n",
      "Prediction Step: 85.\n",
      "Prediction Step: 86.\n",
      "Prediction Step: 87.\n",
      "Prediction Step: 88.\n",
      "Prediction Step: 89.\n",
      "Prediction Step: 90.\n",
      "Prediction Step: 91.\n",
      "Prediction Step: 92.\n",
      "Prediction Step: 93.\n",
      "Prediction Step: 94.\n",
      "Prediction Step: 95.\n",
      "Prediction Step: 96.\n",
      "Prediction Step: 97.\n",
      "Prediction Step: 98.\n",
      "Prediction Step: 99.\n",
      "Prediction Step: 100.\n",
      "Prediction Step: 101.\n",
      "Prediction Step: 102.\n",
      "Prediction Step: 103.\n",
      "Prediction Step: 104.\n",
      "Prediction Step: 105.\n",
      "Prediction Step: 106.\n",
      "Prediction Step: 107.\n",
      "Prediction Step: 108.\n",
      "Prediction Step: 109.\n",
      "Prediction Step: 110.\n",
      "Prediction Step: 111.\n",
      "Prediction Step: 112.\n",
      "Prediction Step: 113.\n",
      "Prediction Step: 114.\n",
      "Prediction Step: 115.\n",
      "Prediction Step: 116.\n",
      "Prediction Step: 117.\n",
      "Prediction Step: 118.\n",
      "Prediction Step: 119.\n",
      "Prediction Step: 120.\n",
      "Prediction Step: 121.\n",
      "Prediction Step: 122.\n",
      "Prediction Step: 123.\n",
      "Prediction Step: 124.\n",
      "Prediction Step: 125.\n",
      "Prediction Step: 126.\n",
      "Prediction Step: 127.\n",
      "Prediction Step: 128.\n",
      "Prediction Step: 129.\n",
      "Prediction Step: 130.\n",
      "Prediction Step: 131.\n",
      "Prediction Step: 132.\n",
      "Prediction Step: 133.\n",
      "Prediction Step: 134.\n",
      "Prediction Step: 135.\n",
      "Prediction Step: 136.\n",
      "Prediction Step: 137.\n",
      "Prediction Step: 138.\n",
      "Prediction Step: 139.\n",
      "Prediction Step: 140.\n",
      "Prediction Step: 141.\n",
      "Prediction Step: 142.\n",
      "Prediction Step: 143.\n",
      "Prediction Step: 144.\n",
      "Prediction Step: 145.\n",
      "Prediction Step: 146.\n",
      "Prediction Step: 147.\n",
      "Prediction Step: 148.\n",
      "Prediction Step: 149.\n",
      "Prediction Step: 150.\n",
      "Prediction Step: 151.\n",
      "Prediction Step: 152.\n",
      "Prediction Step: 153.\n",
      "Prediction Step: 154.\n",
      "Prediction Step: 155.\n",
      "Prediction Step: 156.\n",
      "Prediction Step: 157.\n",
      "Prediction Step: 158.\n",
      "Prediction Step: 159.\n",
      "Prediction Step: 160.\n",
      "Prediction Step: 161.\n",
      "Prediction Step: 162.\n",
      "Prediction Step: 163.\n",
      "Prediction Step: 164.\n",
      "Prediction Step: 165.\n",
      "Prediction Step: 166.\n",
      "Prediction Step: 167.\n",
      "Prediction Step: 168.\n",
      "Prediction Step: 169.\n",
      "Prediction Step: 170.\n",
      "Prediction Step: 171.\n",
      "Prediction Step: 172.\n",
      "Prediction Step: 173.\n",
      "Prediction Step: 174.\n",
      "Prediction Step: 175.\n",
      "Prediction Step: 176.\n",
      "Prediction Step: 177.\n",
      "Prediction Step: 178.\n",
      "Prediction Step: 179.\n",
      "Prediction Step: 180.\n",
      "Prediction Step: 181.\n",
      "Prediction Step: 182.\n",
      "Prediction Step: 183.\n",
      "Prediction Step: 184.\n",
      "Prediction Step: 185.\n",
      "Prediction Step: 186.\n",
      "Prediction Step: 187.\n",
      "Prediction Step: 188.\n",
      "Prediction Step: 189.\n",
      "Prediction Step: 190.\n",
      "Prediction Step: 191.\n",
      "Prediction Step: 192.\n",
      "Prediction Step: 193.\n",
      "Prediction Step: 194.\n",
      "Prediction Step: 195.\n",
      "Prediction Step: 196.\n",
      "Prediction Step: 197.\n",
      "Prediction Step: 198.\n",
      "Prediction Step: 199.\n",
      "Prediction Step: 200.\n",
      "Prediction Step: 201.\n",
      "Prediction Step: 202.\n",
      "Prediction Step: 203.\n",
      "Prediction Step: 204.\n",
      "Prediction Step: 205.\n",
      "Prediction Step: 206.\n",
      "Prediction Step: 207.\n",
      "Prediction Step: 208.\n",
      "Prediction Step: 209.\n",
      "Prediction Step: 210.\n",
      "Prediction Step: 211.\n",
      "Prediction Step: 212.\n",
      "Prediction Step: 213.\n",
      "Prediction Step: 214.\n",
      "Prediction Step: 215.\n",
      "Prediction Step: 216.\n",
      "Prediction Step: 217.\n",
      "Prediction Step: 218.\n",
      "Prediction Step: 219.\n",
      "Prediction Step: 220.\n",
      "Prediction Step: 221.\n",
      "Prediction Step: 222.\n",
      "Prediction Step: 223.\n",
      "Prediction Step: 224.\n",
      "Prediction Step: 225.\n",
      "Prediction Step: 226.\n",
      "Prediction Step: 227.\n",
      "Prediction Step: 228.\n",
      "Prediction Step: 229.\n",
      "Prediction Step: 230.\n",
      "Prediction Step: 231.\n",
      "Prediction Step: 232.\n",
      "Prediction Step: 233.\n",
      "Prediction Step: 234.\n",
      "Prediction Step: 235.\n",
      "Prediction Step: 236.\n",
      "Prediction Step: 237.\n",
      "Prediction Step: 238.\n",
      "Prediction Step: 239.\n",
      "Prediction Step: 240.\n",
      "Prediction Step: 241.\n",
      "Prediction Step: 242.\n",
      "Prediction Step: 243.\n",
      "Prediction Step: 244.\n",
      "Prediction Step: 245.\n",
      "Prediction Step: 246.\n",
      "Prediction Step: 247.\n",
      "Prediction Step: 248.\n",
      "Prediction Step: 249.\n",
      "Prediction Step: 250.\n",
      "Prediction Step: 251.\n",
      "Prediction Step: 252.\n",
      "Prediction Step: 253.\n",
      "Prediction Step: 254.\n",
      "Prediction Step: 255.\n",
      "Prediction Step: 256.\n",
      "Prediction Step: 257.\n",
      "Prediction Step: 258.\n",
      "Prediction Step: 259.\n",
      "Prediction Step: 260.\n",
      "Prediction Step: 261.\n",
      "Prediction Step: 262.\n",
      "Prediction Step: 263.\n",
      "Prediction Step: 264.\n",
      "Prediction Step: 265.\n",
      "Prediction Step: 266.\n",
      "Prediction Step: 267.\n",
      "Prediction Step: 268.\n",
      "Prediction Step: 269.\n",
      "Prediction Step: 270.\n",
      "Prediction Step: 271.\n",
      "Prediction Step: 272.\n",
      "Prediction Step: 273.\n",
      "Prediction Step: 274.\n",
      "Prediction Step: 275.\n",
      "Prediction Step: 276.\n",
      "Prediction Step: 277.\n",
      "Prediction Step: 278.\n",
      "Prediction Step: 279.\n",
      "Prediction Step: 280.\n",
      "Prediction Step: 281.\n",
      "Prediction Step: 282.\n",
      "Prediction Step: 283.\n",
      "Prediction Step: 284.\n",
      "Prediction Step: 285.\n",
      "Prediction Step: 286.\n",
      "Prediction Step: 287.\n",
      "Prediction Step: 288.\n",
      "Prediction Step: 289.\n",
      "Prediction Step: 290.\n",
      "Prediction Step: 291.\n",
      "Prediction Step: 292.\n",
      "Prediction Step: 293.\n",
      "Prediction Step: 294.\n",
      "Prediction Step: 295.\n",
      "Prediction Step: 296.\n",
      "Prediction Step: 297.\n",
      "Prediction Step: 298.\n",
      "Prediction Step: 299.\n",
      "Prediction Step: 300.\n",
      "Prediction Step: 301.\n",
      "Prediction Step: 302.\n",
      "Prediction Step: 303.\n",
      "Prediction Step: 304.\n",
      "Prediction Step: 305.\n",
      "Prediction Step: 306.\n",
      "Prediction Step: 307.\n",
      "Prediction Step: 308.\n",
      "Prediction Step: 309.\n",
      "Prediction Step: 310.\n",
      "Prediction Step: 311.\n",
      "Prediction Step: 312.\n",
      "Prediction Step: 313.\n",
      "Prediction Step: 314.\n",
      "Prediction Step: 315.\n",
      "Prediction Step: 316.\n",
      "Prediction Step: 317.\n",
      "Prediction Step: 318.\n",
      "Prediction Step: 319.\n",
      "Prediction Step: 320.\n",
      "Prediction Step: 321.\n",
      "Prediction Step: 322.\n",
      "Prediction Step: 323.\n",
      "Prediction Step: 324.\n",
      "Prediction Step: 325.\n",
      "Prediction Step: 326.\n",
      "Prediction Step: 327.\n",
      "Prediction Step: 328.\n",
      "Prediction Step: 329.\n",
      "Prediction Step: 330.\n",
      "Prediction Step: 331.\n",
      "Prediction Step: 332.\n",
      "Prediction Step: 333.\n",
      "Prediction Step: 334.\n",
      "Prediction Step: 335.\n",
      "Prediction Step: 336.\n",
      "Prediction Step: 337.\n",
      "Prediction Step: 338.\n",
      "Prediction Step: 339.\n",
      "Prediction Step: 340.\n",
      "Prediction Step: 341.\n",
      "Prediction Step: 342.\n",
      "Prediction Step: 343.\n",
      "Prediction Step: 344.\n",
      "Prediction Step: 345.\n",
      "Prediction Step: 346.\n",
      "Prediction Step: 347.\n",
      "Prediction Step: 348.\n",
      "Prediction Step: 349.\n",
      "Prediction Step: 350.\n",
      "Prediction Step: 351.\n",
      "Prediction Step: 352.\n",
      "Prediction Step: 353.\n",
      "Prediction Step: 354.\n",
      "Prediction Step: 355.\n",
      "Prediction Step: 356.\n",
      "Prediction Step: 357.\n",
      "Prediction Step: 358.\n",
      "Prediction Step: 359.\n",
      "Prediction Step: 360.\n",
      "Prediction Step: 361.\n",
      "Prediction Step: 362.\n",
      "Prediction Step: 363.\n",
      "Prediction Step: 364.\n",
      "Prediction Step: 365.\n",
      "Prediction Step: 366.\n",
      "Prediction Step: 367.\n",
      "Prediction Step: 368.\n",
      "Prediction Step: 369.\n",
      "Prediction Step: 370.\n",
      "Prediction Step: 371.\n",
      "Prediction Step: 372.\n",
      "Prediction Step: 373.\n",
      "Prediction Step: 374.\n",
      "Prediction Step: 375.\n",
      "Prediction Step: 376.\n",
      "Prediction Step: 377.\n",
      "Prediction Step: 378.\n",
      "Prediction Step: 379.\n",
      "Prediction Step: 380.\n",
      "Prediction Step: 381.\n",
      "Prediction Step: 382.\n",
      "Prediction Step: 383.\n",
      "Prediction Step: 384.\n",
      "Prediction Step: 385.\n",
      "Prediction Step: 386.\n",
      "Prediction Step: 387.\n",
      "Prediction Step: 388.\n",
      "Prediction Step: 389.\n",
      "Prediction Step: 390.\n",
      "Prediction Step: 391.\n",
      "Prediction Step: 392.\n",
      "Prediction Step: 393.\n",
      "Prediction Step: 394.\n",
      "Prediction Step: 395.\n",
      "Prediction Step: 396.\n",
      "Prediction Step: 397.\n",
      "Prediction Step: 398.\n",
      "Prediction Step: 399.\n",
      "Prediction Step: 400.\n",
      "Prediction Step: 401.\n",
      "Prediction Step: 402.\n",
      "Prediction Step: 403.\n",
      "Prediction Step: 404.\n",
      "Prediction Step: 405.\n",
      "Prediction Step: 406.\n",
      "Prediction Step: 407.\n",
      "Prediction Step: 408.\n",
      "Prediction Step: 409.\n",
      "Prediction Step: 410.\n",
      "Prediction Step: 411.\n",
      "Prediction Step: 412.\n",
      "Prediction Step: 413.\n",
      "Prediction Step: 414.\n",
      "Prediction Step: 415.\n",
      "Prediction Step: 416.\n",
      "Prediction Step: 417.\n",
      "Prediction Step: 418.\n",
      "Prediction Step: 419.\n",
      "Prediction Step: 420.\n",
      "Prediction Step: 421.\n",
      "Prediction Step: 422.\n",
      "Prediction Step: 423.\n",
      "Prediction Step: 424.\n",
      "Prediction Step: 425.\n",
      "Prediction Step: 426.\n",
      "Prediction Step: 427.\n",
      "Prediction Step: 428.\n",
      "Prediction Step: 429.\n",
      "Prediction Step: 430.\n",
      "Prediction Step: 431.\n",
      "Prediction Step: 432.\n",
      "Prediction Step: 433.\n",
      "Prediction Step: 434.\n",
      "Prediction Step: 435.\n",
      "Prediction Step: 436.\n",
      "Prediction Step: 437.\n",
      "Prediction Step: 438.\n",
      "Prediction Step: 439.\n",
      "Prediction Step: 440.\n",
      "Prediction Step: 441.\n",
      "Prediction Step: 442.\n",
      "Prediction Step: 443.\n",
      "Prediction Step: 444.\n",
      "Prediction Step: 445.\n",
      "Prediction Step: 446.\n",
      "Prediction Step: 447.\n",
      "Prediction Step: 448.\n",
      "Prediction Step: 449.\n",
      "Prediction Step: 450.\n",
      "Prediction Step: 451.\n",
      "Prediction Step: 452.\n",
      "Prediction Step: 453.\n",
      "Prediction Step: 454.\n",
      "Prediction Step: 455.\n",
      "Prediction Step: 456.\n",
      "Prediction Step: 457.\n",
      "Prediction Step: 458.\n",
      "Prediction Step: 459.\n",
      "Prediction Step: 460.\n",
      "Prediction Step: 461.\n",
      "Prediction Step: 462.\n",
      "Prediction Step: 463.\n",
      "Prediction Step: 464.\n",
      "Prediction Step: 465.\n",
      "Prediction Step: 466.\n",
      "Prediction Step: 467.\n",
      "Prediction Step: 468.\n",
      "Prediction Step: 469.\n",
      "Prediction Step: 470.\n",
      "Prediction Step: 471.\n",
      "Prediction Step: 472.\n",
      "Prediction Step: 473.\n",
      "Prediction Step: 474.\n",
      "Prediction Step: 475.\n",
      "Prediction Step: 476.\n",
      "Prediction Step: 477.\n",
      "Prediction Step: 478.\n",
      "Prediction Step: 479.\n",
      "Prediction Step: 480.\n",
      "Prediction Step: 481.\n",
      "Prediction Step: 482.\n",
      "Prediction Step: 483.\n",
      "Prediction Step: 484.\n",
      "Prediction Step: 485.\n",
      "Prediction Step: 486.\n",
      "Prediction Step: 487.\n",
      "Prediction Step: 488.\n",
      "Prediction Step: 489.\n",
      "Prediction Step: 490.\n",
      "Prediction Step: 491.\n",
      "Prediction Step: 492.\n",
      "Prediction Step: 493.\n",
      "Prediction Step: 494.\n",
      "Prediction Step: 495.\n",
      "Prediction Step: 496.\n",
      "Prediction Step: 497.\n",
      "Prediction Step: 498.\n",
      "Prediction Step: 499.\n",
      "Prediction Step: 500.\n",
      "Prediction Step: 501.\n",
      "Prediction Step: 502.\n",
      "Prediction Step: 503.\n",
      "Prediction Step: 504.\n",
      "Prediction Step: 505.\n",
      "Prediction Step: 506.\n",
      "Prediction Step: 507.\n",
      "Prediction Step: 508.\n",
      "Prediction Step: 509.\n",
      "Prediction Step: 510.\n",
      "Prediction Step: 511.\n",
      "Prediction Step: 512.\n",
      "Prediction Step: 513.\n",
      "Prediction Step: 514.\n",
      "Prediction Step: 515.\n",
      "Prediction Step: 516.\n",
      "Prediction Step: 517.\n",
      "Prediction Step: 518.\n",
      "Prediction Step: 519.\n",
      "Prediction Step: 520.\n",
      "Prediction Step: 521.\n",
      "Prediction Step: 522.\n",
      "Prediction Step: 523.\n",
      "Prediction Step: 524.\n",
      "Prediction Step: 525.\n",
      "Prediction Step: 526.\n",
      "Prediction Step: 527.\n",
      "Prediction Step: 528.\n",
      "Prediction Step: 529.\n",
      "Prediction Step: 530.\n",
      "Prediction Step: 531.\n",
      "Prediction Step: 532.\n",
      "Prediction Step: 533.\n",
      "Prediction Step: 534.\n",
      "Prediction Step: 535.\n",
      "Prediction Step: 536.\n",
      "Prediction Step: 537.\n",
      "Prediction Step: 538.\n",
      "Prediction Step: 539.\n",
      "Prediction Step: 540.\n",
      "Prediction Step: 541.\n",
      "Prediction Step: 542.\n",
      "Prediction Step: 543.\n",
      "Prediction Step: 544.\n",
      "Prediction Step: 545.\n",
      "Prediction Step: 546.\n",
      "Prediction Step: 547.\n",
      "Prediction Step: 548.\n",
      "Prediction Step: 549.\n",
      "Prediction Step: 550.\n",
      "Prediction Step: 551.\n",
      "Prediction Step: 552.\n",
      "Prediction Step: 553.\n",
      "Prediction Step: 554.\n",
      "Prediction Step: 555.\n",
      "Prediction Step: 556.\n",
      "Prediction Step: 557.\n",
      "Prediction Step: 558.\n",
      "Prediction Step: 559.\n",
      "Prediction Step: 560.\n",
      "Prediction Step: 561.\n",
      "Prediction Step: 562.\n",
      "Prediction Step: 563.\n",
      "Prediction Step: 564.\n",
      "Prediction Step: 565.\n",
      "Prediction Step: 566.\n",
      "Prediction Step: 567.\n",
      "Prediction Step: 568.\n",
      "Prediction Step: 569.\n",
      "Prediction Step: 570.\n",
      "Prediction Step: 571.\n",
      "Prediction Step: 572.\n",
      "Prediction Step: 573.\n",
      "Prediction Step: 574.\n",
      "Prediction Step: 575.\n",
      "Prediction Step: 576.\n",
      "Prediction Step: 577.\n",
      "Prediction Step: 578.\n",
      "Prediction Step: 579.\n",
      "Prediction Step: 580.\n",
      "Prediction Step: 581.\n",
      "Prediction Step: 582.\n",
      "Prediction Step: 583.\n",
      "Prediction Step: 584.\n",
      "Prediction Step: 585.\n",
      "Prediction Step: 586.\n",
      "Prediction Step: 587.\n",
      "Prediction Step: 588.\n",
      "Prediction Step: 589.\n",
      "Prediction Step: 590.\n",
      "Prediction Step: 591.\n",
      "Prediction Step: 592.\n",
      "Prediction Step: 593.\n",
      "Prediction Step: 594.\n",
      "Prediction Step: 595.\n",
      "Prediction Step: 596.\n",
      "Prediction Step: 597.\n",
      "Prediction Step: 598.\n",
      "Prediction Step: 599.\n",
      "Prediction Step: 600.\n",
      "Prediction Step: 601.\n",
      "Prediction Step: 602.\n",
      "Prediction Step: 603.\n",
      "Prediction Step: 604.\n",
      "Prediction Step: 605.\n",
      "Prediction Step: 606.\n",
      "Prediction Step: 607.\n",
      "Prediction Step: 608.\n",
      "Prediction Step: 609.\n",
      "Prediction Step: 610.\n",
      "Prediction Step: 611.\n",
      "Prediction Step: 612.\n",
      "Prediction Step: 613.\n",
      "Prediction Step: 614.\n",
      "Prediction Step: 615.\n",
      "Prediction Step: 616.\n",
      "Prediction Step: 617.\n",
      "Prediction Step: 618.\n",
      "Prediction Step: 619.\n",
      "Prediction Step: 620.\n",
      "Prediction Step: 621.\n",
      "Prediction Step: 622.\n",
      "Prediction Step: 623.\n",
      "Prediction Step: 624.\n",
      "Prediction Step: 625.\n",
      "Prediction Step: 626.\n",
      "Prediction Step: 627.\n",
      "Prediction Step: 628.\n",
      "Prediction Step: 629.\n",
      "Prediction Step: 630.\n",
      "Prediction Step: 631.\n",
      "Prediction Step: 632.\n",
      "Prediction Step: 633.\n",
      "Prediction Step: 634.\n",
      "Prediction Step: 635.\n",
      "Prediction Step: 636.\n",
      "Prediction Step: 637.\n",
      "Prediction Step: 638.\n",
      "Prediction Step: 639.\n",
      "Prediction Step: 640.\n",
      "Prediction Step: 641.\n",
      "Prediction Step: 642.\n",
      "Prediction Step: 643.\n",
      "Prediction Step: 644.\n",
      "Prediction Step: 645.\n",
      "Prediction Step: 646.\n",
      "Prediction Step: 647.\n",
      "Prediction Step: 648.\n",
      "Prediction Step: 649.\n",
      "Prediction Step: 650.\n",
      "Prediction Step: 651.\n",
      "Prediction Step: 652.\n",
      "Prediction Step: 653.\n",
      "Prediction Step: 654.\n",
      "Prediction Step: 655.\n",
      "Prediction Step: 656.\n",
      "Prediction Step: 657.\n",
      "Prediction Step: 658.\n",
      "Prediction Step: 659.\n",
      "Prediction Step: 660.\n",
      "Prediction Step: 661.\n",
      "Prediction Step: 662.\n",
      "Prediction Step: 663.\n",
      "Prediction Step: 664.\n",
      "Prediction Step: 665.\n",
      "Prediction Step: 666.\n",
      "Prediction Step: 667.\n",
      "Prediction Step: 668.\n",
      "Prediction Step: 669.\n",
      "Prediction Step: 670.\n",
      "Prediction Step: 671.\n",
      "Prediction Step: 672.\n",
      "Prediction Step: 673.\n",
      "Prediction Step: 674.\n",
      "Prediction Step: 675.\n",
      "Prediction Step: 676.\n",
      "Prediction Step: 677.\n",
      "Prediction Step: 678.\n",
      "Prediction Step: 679.\n",
      "Prediction Step: 680.\n",
      "Prediction Step: 681.\n",
      "Prediction Step: 682.\n",
      "Prediction Step: 683.\n",
      "Prediction Step: 684.\n",
      "Prediction Step: 685.\n",
      "Prediction Step: 686.\n",
      "Prediction Step: 687.\n",
      "Prediction Step: 688.\n",
      "Prediction Step: 689.\n",
      "Prediction Step: 690.\n",
      "Prediction Step: 691.\n",
      "Prediction Step: 692.\n",
      "Prediction Step: 693.\n",
      "Prediction Step: 694.\n",
      "Prediction Step: 695.\n",
      "Prediction Step: 696.\n",
      "Prediction Step: 697.\n",
      "Prediction Step: 698.\n",
      "Prediction Step: 699.\n",
      "Prediction Step: 700.\n",
      "Prediction Step: 701.\n",
      "Prediction Step: 702.\n",
      "Prediction Step: 703.\n",
      "Prediction Step: 704.\n",
      "Prediction Step: 705.\n",
      "Prediction Step: 706.\n",
      "Prediction Step: 707.\n",
      "Prediction Step: 708.\n",
      "Prediction Step: 709.\n",
      "Prediction Step: 710.\n",
      "Prediction Step: 711.\n",
      "Prediction Step: 712.\n",
      "Prediction Step: 713.\n",
      "Prediction Step: 714.\n",
      "Prediction Step: 715.\n",
      "Prediction Step: 716.\n",
      "Prediction Step: 717.\n",
      "Prediction Step: 718.\n",
      "Prediction Step: 719.\n",
      "Prediction Step: 720.\n",
      "Prediction Step: 721.\n",
      "Prediction Step: 722.\n",
      "Prediction Step: 723.\n",
      "Prediction Step: 724.\n",
      "Prediction Step: 725.\n",
      "Prediction Step: 726.\n",
      "Prediction Step: 727.\n",
      "Prediction Step: 728.\n",
      "Prediction Step: 729.\n",
      "Prediction Step: 730.\n",
      "Prediction Step: 731.\n",
      "Prediction Step: 732.\n",
      "Prediction Step: 733.\n",
      "Prediction Step: 734.\n",
      "Prediction Step: 735.\n",
      "Prediction Step: 736.\n",
      "Prediction Step: 737.\n",
      "Prediction Step: 738.\n",
      "Prediction Step: 739.\n",
      "Prediction Step: 740.\n",
      "Prediction Step: 741.\n",
      "Prediction Step: 742.\n",
      "Prediction Step: 743.\n",
      "Prediction Step: 744.\n",
      "Prediction Step: 745.\n",
      "Prediction Step: 746.\n",
      "Prediction Step: 747.\n",
      "Prediction Step: 748.\n",
      "Prediction Step: 749.\n",
      "Prediction Step: 750.\n",
      "Prediction Step: 751.\n",
      "Prediction Step: 752.\n",
      "Prediction Step: 753.\n",
      "Prediction Step: 754.\n",
      "Prediction Step: 755.\n",
      "Prediction Step: 756.\n",
      "Prediction Step: 757.\n",
      "Prediction Step: 758.\n",
      "Prediction Step: 759.\n",
      "Prediction Step: 760.\n",
      "Prediction Step: 761.\n",
      "Prediction Step: 762.\n",
      "Prediction Step: 763.\n",
      "Prediction Step: 764.\n",
      "Prediction Step: 765.\n",
      "Prediction Step: 766.\n",
      "Prediction Step: 767.\n",
      "Prediction Step: 768.\n",
      "Prediction Step: 769.\n",
      "Prediction Step: 770.\n",
      "Prediction Step: 771.\n",
      "Prediction Step: 772.\n",
      "Prediction Step: 773.\n",
      "Prediction Step: 774.\n",
      "Prediction Step: 775.\n",
      "Prediction Step: 776.\n",
      "Prediction Step: 777.\n",
      "Prediction Step: 778.\n",
      "Prediction Step: 779.\n",
      "Prediction Step: 780.\n",
      "Prediction Step: 781.\n",
      "Prediction Step: 782.\n",
      "Prediction Step: 783.\n",
      "Prediction Step: 784.\n",
      "Prediction Step: 785.\n",
      "Prediction Step: 786.\n",
      "Prediction Step: 787.\n",
      "Prediction Step: 788.\n",
      "Prediction Step: 789.\n",
      "Prediction Step: 790.\n",
      "Prediction Step: 791.\n",
      "Prediction Step: 792.\n",
      "Prediction Step: 793.\n",
      "Prediction Step: 794.\n",
      "Prediction Step: 795.\n",
      "Prediction Step: 796.\n",
      "Prediction Step: 797.\n",
      "Prediction Step: 798.\n",
      "Prediction Step: 799.\n",
      "Prediction Step: 800.\n",
      "Prediction Step: 801.\n",
      "Prediction Step: 802.\n",
      "Prediction Step: 803.\n",
      "Prediction Step: 804.\n",
      "Prediction Step: 805.\n",
      "Prediction Step: 806.\n",
      "Prediction Step: 807.\n",
      "Prediction Step: 808.\n",
      "Prediction Step: 809.\n",
      "Prediction Step: 810.\n",
      "Prediction Step: 811.\n",
      "Prediction Step: 812.\n",
      "Prediction Step: 813.\n",
      "Prediction Step: 814.\n",
      "Prediction Step: 815.\n",
      "Prediction Step: 816.\n",
      "Prediction Step: 817.\n",
      "Prediction Step: 818.\n",
      "Prediction Step: 819.\n",
      "Prediction Step: 820.\n",
      "Prediction Step: 821.\n",
      "Prediction Step: 822.\n",
      "Prediction Step: 823.\n",
      "Prediction Step: 824.\n",
      "Prediction Step: 825.\n",
      "Prediction Step: 826.\n",
      "Prediction Step: 827.\n",
      "Prediction Step: 828.\n",
      "Prediction Step: 829.\n",
      "Prediction Step: 830.\n",
      "Prediction Step: 831.\n",
      "Prediction Step: 832.\n",
      "Prediction Step: 833.\n",
      "Prediction Step: 834.\n",
      "Prediction Step: 835.\n",
      "Prediction Step: 836.\n",
      "Prediction Step: 837.\n",
      "Prediction Step: 838.\n",
      "Prediction Step: 839.\n",
      "Prediction Step: 840.\n",
      "Prediction Step: 841.\n",
      "Prediction Step: 842.\n",
      "Prediction Step: 843.\n",
      "Prediction Step: 844.\n",
      "Prediction Step: 845.\n",
      "Prediction Step: 846.\n",
      "Prediction Step: 847.\n",
      "Prediction Step: 848.\n",
      "Prediction Step: 849.\n",
      "Prediction Step: 850.\n",
      "Prediction Step: 851.\n",
      "Prediction Step: 852.\n",
      "Prediction Step: 853.\n",
      "Prediction Step: 854.\n",
      "Prediction Step: 855.\n",
      "Prediction Step: 856.\n",
      "Prediction Step: 857.\n",
      "Prediction Step: 858.\n",
      "Prediction Step: 859.\n",
      "Prediction Step: 860.\n",
      "Prediction Step: 861.\n",
      "Prediction Step: 862.\n",
      "Prediction Step: 863.\n",
      "Prediction Step: 864.\n",
      "Prediction Step: 865.\n",
      "Prediction Step: 866.\n",
      "Prediction Step: 867.\n",
      "Prediction Step: 868.\n",
      "Prediction Step: 869.\n",
      "Prediction Step: 870.\n",
      "Prediction Step: 871.\n",
      "Prediction Step: 872.\n",
      "Prediction Step: 873.\n",
      "Prediction Step: 874.\n",
      "Prediction Step: 875.\n",
      "Prediction Step: 876.\n",
      "Prediction Step: 877.\n",
      "Prediction Step: 878.\n",
      "Prediction Step: 879.\n",
      "Prediction Step: 880.\n",
      "Prediction Step: 881.\n",
      "Prediction Step: 882.\n",
      "Prediction Step: 883.\n",
      "Prediction Step: 884.\n",
      "Prediction Step: 885.\n",
      "Prediction Step: 886.\n",
      "Prediction Step: 887.\n",
      "Prediction Step: 888.\n",
      "Prediction Step: 889.\n",
      "Prediction Step: 890.\n",
      "Prediction Step: 891.\n",
      "Prediction Step: 892.\n",
      "Prediction Step: 893.\n",
      "Prediction Step: 894.\n",
      "Prediction Step: 895.\n",
      "Prediction Step: 896.\n",
      "Prediction Step: 897.\n",
      "Prediction Step: 898.\n",
      "Prediction Step: 899.\n",
      "Prediction Step: 900.\n",
      "Prediction Step: 901.\n",
      "Prediction Step: 902.\n",
      "Prediction Step: 903.\n",
      "Prediction Step: 904.\n",
      "Prediction Step: 905.\n",
      "Prediction Step: 906.\n",
      "Prediction Step: 907.\n",
      "Prediction Step: 908.\n",
      "Prediction Step: 909.\n",
      "Prediction Step: 910.\n",
      "Prediction Step: 911.\n",
      "Prediction Step: 912.\n",
      "Prediction Step: 913.\n",
      "Prediction Step: 914.\n",
      "Prediction Step: 915.\n",
      "Prediction Step: 916.\n",
      "Prediction Step: 917.\n",
      "Prediction Step: 918.\n",
      "Prediction Step: 919.\n",
      "Prediction Step: 920.\n",
      "Prediction Step: 921.\n",
      "Prediction Step: 922.\n",
      "Prediction Step: 923.\n",
      "Prediction Step: 924.\n",
      "Prediction Step: 925.\n",
      "Prediction Step: 926.\n",
      "Prediction Step: 927.\n",
      "Prediction Step: 928.\n",
      "Prediction Step: 929.\n",
      "Prediction Step: 930.\n",
      "Prediction Step: 931.\n",
      "Prediction Step: 932.\n",
      "Prediction Step: 933.\n",
      "Prediction Step: 934.\n",
      "Prediction Step: 935.\n",
      "Prediction Step: 936.\n",
      "Prediction Step: 937.\n",
      "Prediction Step: 938.\n",
      "Prediction Step: 939.\n",
      "Prediction Step: 940.\n",
      "Prediction Step: 941.\n",
      "Prediction Step: 942.\n",
      "Prediction Step: 943.\n",
      "Prediction Step: 944.\n",
      "Prediction Step: 945.\n",
      "Prediction Step: 946.\n",
      "Prediction Step: 947.\n",
      "Prediction Step: 948.\n",
      "Prediction Step: 949.\n",
      "Prediction Step: 950.\n",
      "Prediction Step: 951.\n",
      "Prediction Step: 952.\n",
      "Prediction Step: 953.\n",
      "Prediction Step: 954.\n",
      "Prediction Step: 955.\n",
      "Prediction Step: 956.\n",
      "Prediction Step: 957.\n",
      "Prediction Step: 958.\n",
      "Prediction Step: 959.\n",
      "Prediction Step: 960.\n",
      "Prediction Step: 961.\n",
      "Prediction Step: 962.\n",
      "Prediction Step: 963.\n",
      "Prediction Step: 964.\n",
      "Prediction Step: 965.\n",
      "Prediction Step: 966.\n",
      "Prediction Step: 967.\n",
      "Prediction Step: 968.\n",
      "Prediction Step: 969.\n",
      "Prediction Step: 970.\n",
      "Prediction Step: 971.\n",
      "Prediction Step: 972.\n",
      "Prediction Step: 973.\n",
      "Prediction Step: 974.\n",
      "Prediction Step: 975.\n",
      "Prediction Step: 976.\n",
      "Prediction Step: 977.\n",
      "Prediction Step: 978.\n",
      "Prediction Step: 979.\n",
      "Prediction Step: 980.\n",
      "Prediction Step: 981.\n",
      "Prediction Step: 982.\n",
      "Prediction Step: 983.\n",
      "Prediction Step: 984.\n",
      "Prediction Step: 985.\n",
      "Prediction Step: 986.\n",
      "Prediction Step: 987.\n",
      "Prediction Step: 988.\n",
      "Prediction Step: 989.\n",
      "Prediction Step: 990.\n",
      "Prediction Step: 991.\n",
      "Prediction Step: 992.\n",
      "Prediction Step: 993.\n",
      "Prediction Step: 994.\n",
      "Prediction Step: 995.\n",
      "Prediction Step: 996.\n",
      "Prediction Step: 997.\n",
      "Prediction Step: 998.\n",
      "Prediction Step: 999.\n",
      "Prediction Step: 1000.\n",
      "Prediction Step: 1001.\n",
      "Prediction Step: 1002.\n",
      "Prediction Step: 1003.\n",
      "Prediction Step: 1004.\n",
      "Prediction Step: 1005.\n",
      "Prediction Step: 1006.\n",
      "Prediction Step: 1007.\n",
      "Prediction Step: 1008.\n",
      "Prediction Step: 1009.\n",
      "Prediction Step: 1010.\n",
      "Prediction Step: 1011.\n",
      "Prediction Step: 1012.\n",
      "Prediction Step: 1013.\n",
      "Prediction Step: 1014.\n",
      "Prediction Step: 1015.\n",
      "Prediction Step: 1016.\n",
      "Prediction Step: 1017.\n",
      "Prediction Step: 1018.\n",
      "Prediction Step: 1019.\n",
      "Prediction Step: 1020.\n",
      "Prediction Step: 1021.\n",
      "Prediction Step: 1022.\n",
      "Prediction Step: 1023.\n",
      "Prediction Step: 1024.\n",
      "Prediction Step: 1025.\n",
      "Prediction Step: 1026.\n",
      "Prediction Step: 1027.\n",
      "Prediction Step: 1028.\n",
      "Prediction Step: 1029.\n",
      "Prediction Step: 1030.\n",
      "Prediction Step: 1031.\n",
      "Prediction Step: 1032.\n",
      "Prediction Step: 1033.\n",
      "Prediction Step: 1034.\n",
      "Prediction Step: 1035.\n",
      "Prediction Step: 1036.\n",
      "Prediction Step: 1037.\n",
      "Prediction Step: 1038.\n",
      "Prediction Step: 1039.\n",
      "Prediction Step: 1040.\n",
      "Prediction Step: 1041.\n",
      "Prediction Step: 1042.\n",
      "Prediction Step: 1043.\n",
      "Prediction Step: 1044.\n",
      "Prediction Step: 1045.\n",
      "Prediction Step: 1046.\n",
      "Prediction Step: 1047.\n",
      "Prediction Step: 1048.\n",
      "Prediction Step: 1049.\n",
      "Prediction Step: 1050.\n",
      "Prediction Step: 1051.\n",
      "Prediction Step: 1052.\n",
      "Prediction Step: 1053.\n",
      "Prediction Step: 1054.\n",
      "Prediction Step: 1055.\n",
      "Prediction Step: 1056.\n",
      "Prediction Step: 1057.\n",
      "Prediction Step: 1058.\n",
      "Prediction Step: 1059.\n",
      "Prediction Step: 1060.\n",
      "Prediction Step: 1061.\n",
      "Prediction Step: 1062.\n",
      "Prediction Step: 1063.\n",
      "Prediction Step: 1064.\n",
      "Prediction Step: 1065.\n",
      "Prediction Step: 1066.\n",
      "Prediction Step: 1067.\n",
      "Prediction Step: 1068.\n",
      "Prediction Step: 1069.\n",
      "Prediction Step: 1070.\n",
      "Prediction Step: 1071.\n",
      "Prediction Step: 1072.\n",
      "Prediction Step: 1073.\n",
      "Prediction Step: 1074.\n",
      "Prediction Step: 1075.\n",
      "Prediction Step: 1076.\n",
      "Prediction Step: 1077.\n",
      "Prediction Step: 1078.\n",
      "Prediction Step: 1079.\n",
      "Prediction Step: 1080.\n",
      "Prediction Step: 1081.\n",
      "Prediction Step: 1082.\n",
      "Prediction Step: 1083.\n",
      "Prediction Step: 1084.\n",
      "Prediction Step: 1085.\n",
      "Prediction Step: 1086.\n",
      "Prediction Step: 1087.\n",
      "Prediction Step: 1088.\n",
      "Prediction Step: 1089.\n",
      "Prediction Step: 1090.\n",
      "Prediction Step: 1091.\n",
      "Prediction Step: 1092.\n",
      "Prediction Step: 1093.\n",
      "Prediction Step: 1094.\n",
      "Prediction Step: 1095.\n",
      "Prediction Step: 1096.\n",
      "Prediction Step: 1097.\n",
      "Prediction Step: 1098.\n",
      "Prediction Step: 1099.\n",
      "Prediction Step: 1100.\n",
      "Prediction Step: 1101.\n",
      "Prediction Step: 1102.\n",
      "Prediction Step: 1103.\n",
      "Prediction Step: 1104.\n",
      "Prediction Step: 1105.\n",
      "Prediction Step: 1106.\n",
      "Prediction Step: 1107.\n",
      "Prediction Step: 1108.\n",
      "Prediction Step: 1109.\n",
      "Prediction Step: 1110.\n",
      "Prediction Step: 1111.\n",
      "Prediction Step: 1112.\n",
      "Prediction Step: 1113.\n",
      "Prediction Step: 1114.\n",
      "Prediction Step: 1115.\n",
      "Prediction Step: 1116.\n",
      "Prediction Step: 1117.\n",
      "Prediction Step: 1118.\n",
      "Prediction Step: 1119.\n",
      "Prediction Step: 1120.\n",
      "Prediction Step: 1121.\n",
      "Prediction Step: 1122.\n",
      "Prediction Step: 1123.\n",
      "Prediction Step: 1124.\n",
      "Prediction Step: 1125.\n",
      "Prediction Step: 1126.\n",
      "Prediction Step: 1127.\n",
      "Prediction Step: 1128.\n",
      "Prediction Step: 1129.\n",
      "Prediction Step: 1130.\n",
      "Prediction Step: 1131.\n",
      "Prediction Step: 1132.\n",
      "Prediction Step: 1133.\n",
      "Prediction Step: 1134.\n",
      "Prediction Step: 1135.\n",
      "Prediction Step: 1136.\n",
      "Prediction Step: 1137.\n",
      "Prediction Step: 1138.\n",
      "Prediction Step: 1139.\n",
      "Prediction Step: 1140.\n",
      "Prediction Step: 1141.\n",
      "Prediction Step: 1142.\n",
      "Prediction Step: 1143.\n",
      "Prediction Step: 1144.\n",
      "Prediction Step: 1145.\n",
      "Prediction Step: 1146.\n",
      "Prediction Step: 1147.\n",
      "Prediction Step: 1148.\n",
      "Prediction Step: 1149.\n",
      "Prediction Step: 1150.\n",
      "Prediction Step: 1151.\n",
      "Prediction Step: 1152.\n",
      "Prediction Step: 1153.\n",
      "Prediction Step: 1154.\n",
      "Prediction Step: 1155.\n",
      "Prediction Step: 1156.\n",
      "Prediction Step: 1157.\n",
      "Prediction Step: 1158.\n",
      "Prediction Step: 1159.\n",
      "Prediction Step: 1160.\n",
      "Prediction Step: 1161.\n",
      "Prediction Step: 1162.\n",
      "Prediction Step: 1163.\n",
      "Prediction Step: 1164.\n",
      "Prediction Step: 1165.\n",
      "Prediction Step: 1166.\n",
      "Prediction Step: 1167.\n",
      "Prediction Step: 1168.\n",
      "Prediction Step: 1169.\n",
      "Prediction Step: 1170.\n",
      "Prediction Step: 1171.\n",
      "Prediction Step: 1172.\n",
      "Prediction Step: 1173.\n",
      "Prediction Step: 1174.\n",
      "Prediction Step: 1175.\n",
      "Prediction Step: 1176.\n",
      "Prediction Step: 1177.\n",
      "Prediction Step: 1178.\n",
      "Prediction Step: 1179.\n",
      "Prediction Step: 1180.\n",
      "Prediction Step: 1181.\n",
      "Prediction Step: 1182.\n",
      "Prediction Step: 1183.\n",
      "Prediction Step: 1184.\n",
      "Prediction Step: 1185.\n",
      "Prediction Step: 1186.\n",
      "Prediction Step: 1187.\n",
      "Prediction Step: 1188.\n",
      "Prediction Step: 1189.\n",
      "Prediction Step: 1190.\n",
      "Prediction Step: 1191.\n",
      "Prediction Step: 1192.\n",
      "Prediction Step: 1193.\n",
      "Prediction Step: 1194.\n",
      "Prediction Step: 1195.\n",
      "Prediction Step: 1196.\n",
      "Prediction Step: 1197.\n",
      "Prediction Step: 1198.\n",
      "Prediction Step: 1199.\n",
      "Prediction Step: 1200.\n",
      "Prediction Step: 1201.\n",
      "Prediction Step: 1202.\n",
      "Prediction Step: 1203.\n",
      "Prediction Step: 1204.\n",
      "Prediction Step: 1205.\n",
      "Prediction Step: 1206.\n",
      "Prediction Step: 1207.\n",
      "Prediction Step: 1208.\n",
      "Prediction Step: 1209.\n",
      "Prediction Step: 1210.\n",
      "Prediction Step: 1211.\n",
      "Prediction Step: 1212.\n",
      "Prediction Step: 1213.\n",
      "Prediction Step: 1214.\n",
      "Prediction Step: 1215.\n",
      "Prediction Step: 1216.\n",
      "Prediction Step: 1217.\n",
      "Prediction Step: 1218.\n",
      "Prediction Step: 1219.\n",
      "Prediction Step: 1220.\n",
      "Prediction Step: 1221.\n",
      "Prediction Step: 1222.\n",
      "Prediction Step: 1223.\n",
      "Prediction Step: 1224.\n",
      "Prediction Step: 1225.\n",
      "Prediction Step: 1226.\n",
      "Prediction Step: 1227.\n",
      "Prediction Step: 1228.\n",
      "Prediction Step: 1229.\n",
      "Prediction Step: 1230.\n",
      "Prediction Step: 1231.\n",
      "Prediction Step: 1232.\n",
      "Prediction Step: 1233.\n",
      "Prediction Step: 1234.\n",
      "Prediction Step: 1235.\n",
      "Prediction Step: 1236.\n",
      "Prediction Step: 1237.\n",
      "Prediction Step: 1238.\n",
      "Prediction Step: 1239.\n",
      "Prediction Step: 1240.\n",
      "Prediction Step: 1241.\n",
      "Prediction Step: 1242.\n",
      "Prediction Step: 1243.\n",
      "Prediction Step: 1244.\n",
      "Prediction Step: 1245.\n",
      "Prediction Step: 1246.\n",
      "Prediction Step: 1247.\n",
      "Prediction Step: 1248.\n",
      "Prediction Step: 1249.\n",
      "Prediction Step: 1250.\n",
      "Prediction Step: 1251.\n",
      "Prediction Step: 1252.\n",
      "Prediction Step: 1253.\n",
      "Prediction Step: 1254.\n",
      "Prediction Step: 1255.\n",
      "Prediction Step: 1256.\n",
      "Prediction Step: 1257.\n",
      "Prediction Step: 1258.\n",
      "Prediction Step: 1259.\n",
      "Prediction Step: 1260.\n",
      "Prediction Step: 1261.\n",
      "Prediction Step: 1262.\n",
      "Prediction Step: 1263.\n",
      "Prediction Step: 1264.\n",
      "Prediction Step: 1265.\n",
      "Prediction Step: 1266.\n",
      "Prediction Step: 1267.\n",
      "Prediction Step: 1268.\n",
      "Prediction Step: 1269.\n",
      "Prediction Step: 1270.\n",
      "Prediction Step: 1271.\n",
      "Prediction Step: 1272.\n",
      "Prediction Step: 1273.\n",
      "Prediction Step: 1274.\n",
      "Prediction Step: 1275.\n",
      "Prediction Step: 1276.\n",
      "Prediction Step: 1277.\n",
      "Prediction Step: 1278.\n",
      "Prediction Step: 1279.\n",
      "Prediction Step: 1280.\n",
      "Prediction Step: 1281.\n",
      "Prediction Step: 1282.\n",
      "Prediction Step: 1283.\n",
      "Prediction Step: 1284.\n",
      "Prediction Step: 1285.\n",
      "Prediction Step: 1286.\n",
      "Prediction Step: 1287.\n",
      "Prediction Step: 1288.\n",
      "Prediction Step: 1289.\n",
      "Prediction Step: 1290.\n",
      "Prediction Step: 1291.\n",
      "Prediction Step: 1292.\n",
      "Prediction Step: 1293.\n",
      "Prediction Step: 1294.\n",
      "Prediction Step: 1295.\n",
      "Prediction Step: 1296.\n",
      "Prediction Step: 1297.\n",
      "Prediction Step: 1298.\n",
      "Prediction Step: 1299.\n",
      "Prediction Step: 1300.\n",
      "Prediction Step: 1301.\n",
      "Prediction Step: 1302.\n",
      "Prediction Step: 1303.\n",
      "Prediction Step: 1304.\n",
      "Prediction Step: 1305.\n",
      "Prediction Step: 1306.\n",
      "Prediction Step: 1307.\n",
      "Prediction Step: 1308.\n",
      "Prediction Step: 1309.\n",
      "Prediction Step: 1310.\n",
      "Prediction Step: 1311.\n",
      "Prediction Step: 1312.\n",
      "Prediction Step: 1313.\n",
      "Prediction Step: 1314.\n",
      "Prediction Step: 1315.\n",
      "Prediction Step: 1316.\n",
      "Prediction Step: 1317.\n",
      "Prediction Step: 1318.\n",
      "Prediction Step: 1319.\n",
      "Prediction Step: 1320.\n",
      "Prediction Step: 1321.\n",
      "Prediction Step: 1322.\n",
      "Prediction Step: 1323.\n",
      "Prediction Step: 1324.\n",
      "Prediction Step: 1325.\n",
      "Prediction Step: 1326.\n",
      "Prediction Step: 1327.\n",
      "Prediction Step: 1328.\n",
      "Prediction Step: 1329.\n",
      "Prediction Step: 1330.\n",
      "Prediction Step: 1331.\n",
      "Prediction Step: 1332.\n",
      "Prediction Step: 1333.\n",
      "Prediction Step: 1334.\n",
      "Prediction Step: 1335.\n",
      "Prediction Step: 1336.\n",
      "Prediction Step: 1337.\n",
      "Prediction Step: 1338.\n",
      "Prediction Step: 1339.\n",
      "Prediction Step: 1340.\n",
      "Prediction Step: 1341.\n",
      "Prediction Step: 1342.\n",
      "Prediction Step: 1343.\n",
      "Prediction Step: 1344.\n",
      "Prediction Step: 1345.\n",
      "Prediction Step: 1346.\n",
      "Prediction Step: 1347.\n",
      "Prediction Step: 1348.\n",
      "Prediction Step: 1349.\n",
      "Prediction Step: 1350.\n",
      "Prediction Step: 1351.\n",
      "Prediction Step: 1352.\n",
      "Prediction Step: 1353.\n",
      "Prediction Step: 1354.\n",
      "Prediction Step: 1355.\n",
      "Prediction Step: 1356.\n",
      "Prediction Step: 1357.\n",
      "Prediction Step: 1358.\n",
      "Prediction Step: 1359.\n",
      "Prediction Step: 1360.\n",
      "Prediction Step: 1361.\n",
      "Prediction Step: 1362.\n",
      "Prediction Step: 1363.\n",
      "Prediction Step: 1364.\n",
      "Prediction Step: 1365.\n",
      "Prediction Step: 1366.\n",
      "Prediction Step: 1367.\n",
      "Prediction Step: 1368.\n",
      "Prediction Step: 1369.\n",
      "Prediction Step: 1370.\n",
      "Prediction Step: 1371.\n",
      "Prediction Step: 1372.\n",
      "Prediction Step: 1373.\n",
      "Prediction Step: 1374.\n",
      "Prediction Step: 1375.\n",
      "Prediction Step: 1376.\n",
      "Prediction Step: 1377.\n",
      "Prediction Step: 1378.\n",
      "Prediction Step: 1379.\n",
      "Prediction Step: 1380.\n",
      "Prediction Step: 1381.\n",
      "Prediction Step: 1382.\n",
      "Prediction Step: 1383.\n",
      "Prediction Step: 1384.\n",
      "Prediction Step: 1385.\n",
      "Prediction Step: 1386.\n",
      "Prediction Step: 1387.\n",
      "Prediction Step: 1388.\n",
      "Prediction Step: 1389.\n",
      "Prediction Step: 1390.\n",
      "Prediction Step: 1391.\n",
      "Prediction Step: 1392.\n",
      "Prediction Step: 1393.\n",
      "Prediction Step: 1394.\n",
      "Prediction Step: 1395.\n",
      "Prediction Step: 1396.\n",
      "Prediction Step: 1397.\n",
      "Prediction Step: 1398.\n",
      "Prediction Step: 1399.\n",
      "Prediction Step: 1400.\n",
      "Prediction Step: 1401.\n",
      "Prediction Step: 1402.\n",
      "Prediction Step: 1403.\n",
      "Prediction Step: 1404.\n",
      "Prediction Step: 1405.\n",
      "Prediction Step: 1406.\n",
      "Prediction Step: 1407.\n",
      "Prediction Step: 1408.\n",
      "Prediction Step: 1409.\n",
      "Prediction Step: 1410.\n",
      "Prediction Step: 1411.\n",
      "Prediction Step: 1412.\n",
      "Prediction Step: 1413.\n",
      "Prediction Step: 1414.\n",
      "Prediction Step: 1415.\n",
      "Prediction Step: 1416.\n",
      "Prediction Step: 1417.\n",
      "Prediction Step: 1418.\n",
      "Prediction Step: 1419.\n",
      "Prediction Step: 1420.\n",
      "Prediction Step: 1421.\n",
      "Prediction Step: 1422.\n",
      "Prediction Step: 1423.\n",
      "Prediction Step: 1424.\n",
      "Prediction Step: 1425.\n",
      "Prediction Step: 1426.\n",
      "Prediction Step: 1427.\n",
      "Prediction Step: 1428.\n",
      "Prediction Step: 1429.\n",
      "Prediction Step: 1430.\n",
      "Prediction Step: 1431.\n",
      "Prediction Step: 1432.\n",
      "Prediction Step: 1433.\n",
      "Prediction Step: 1434.\n",
      "Prediction Step: 1435.\n",
      "Prediction Step: 1436.\n",
      "Prediction Step: 1437.\n",
      "Prediction Step: 1438.\n",
      "Prediction Step: 1439.\n",
      "Prediction Step: 1440.\n",
      "Prediction Step: 1441.\n",
      "Prediction Step: 1442.\n",
      "Prediction Step: 1443.\n",
      "Prediction Step: 1444.\n",
      "Prediction Step: 1445.\n",
      "Prediction Step: 1446.\n",
      "Prediction Step: 1447.\n",
      "Prediction Step: 1448.\n",
      "Prediction Step: 1449.\n",
      "Prediction Step: 1450.\n",
      "Prediction Step: 1451.\n",
      "Prediction Step: 1452.\n",
      "Prediction Step: 1453.\n",
      "Prediction Step: 1454.\n",
      "Prediction Step: 1455.\n",
      "Prediction Step: 1456.\n",
      "Prediction Step: 1457.\n",
      "Prediction Step: 1458.\n",
      "Prediction Step: 1459.\n",
      "Prediction Step: 1460.\n",
      "Prediction Step: 1461.\n",
      "Prediction Step: 1462.\n",
      "Prediction Step: 1463.\n",
      "Prediction Step: 1464.\n",
      "Prediction Step: 1465.\n",
      "Prediction Step: 1466.\n",
      "Prediction Step: 1467.\n",
      "Prediction Step: 1468.\n",
      "Prediction Step: 1469.\n",
      "Prediction Step: 1470.\n",
      "Prediction Step: 1471.\n",
      "Prediction Step: 1472.\n",
      "Prediction Step: 1473.\n",
      "Prediction Step: 1474.\n",
      "Prediction Step: 1475.\n",
      "Prediction Step: 1476.\n",
      "Prediction Step: 1477.\n",
      "Prediction Step: 1478.\n",
      "Prediction Step: 1479.\n",
      "Prediction Step: 1480.\n",
      "Prediction Step: 1481.\n",
      "Prediction Step: 1482.\n",
      "Prediction Step: 1483.\n",
      "Prediction Step: 1484.\n",
      "Prediction Step: 1485.\n"
     ]
    }
   ],
   "source": [
    "# Run on the Test Data.\n",
    "model.load_from_checkpoint(model_path, \"best_step\")\n",
    "model.to_device()\n",
    "test_dataloader = create_dataloader(model, test_file_name=test_file_name)\n",
    "test_loop(\n",
    "    model=model,\n",
    "    mode=\"test\",\n",
    "    model_path=model_path,\n",
    "    prediction_file_name=\"test.predicted.tsv\",\n",
    "    test_dataloader=test_dataloader,\n",
    "    metric=qa_metric,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
