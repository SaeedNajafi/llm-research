{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-04 12:13:36.699086: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-04 12:13:36.763703: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-04 12:13:38.236428: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from typing import Any, Dict, Iterator, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "from src.base_lm import BaseLM\n",
    "from src.general_utils import DictDataset, train_loop\n",
    "from src.model_utils import clear_cache, encoder_decoder_log_of_labels, mlm_log_of_labels, set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 4\n",
    "eval_batch_size = 16\n",
    "lm_input_max_length = 1024\n",
    "lm_output_max_length = 128\n",
    "lm_top_p = 0.9\n",
    "temperature = 0.6\n",
    "metric_device = \"cuda:1\"\n",
    "metric_batch_size = 16\n",
    "learning_rate = 0.001\n",
    "train_file_name = \"1024-shot-datasets/squad/1024-42-train.tsv\"\n",
    "dev_file_name = \"1024-shot-datasets/squad/1024-42-dev.tsv\"\n",
    "test_file_name = \"1024-shot-datasets/squad/test.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"QA Model based on T5 base and without any optimizations for large-scale training.\"\"\"\n",
    "\n",
    "\n",
    "class T5BaseQA(BaseLM):\n",
    "    \"\"\"Class to implement T5-base for QA task.\"\"\"\n",
    "\n",
    "    def __init__(self, device: str, seed: int) -> None:\n",
    "        super().__init__(device=device, model_name=\"t5_base\", seed=seed)\n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"google/t5-base-lm-adapt\")\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(\"google/t5-base-lm-adapt\")\n",
    "        # to train the main lm, we update all of its parameters.\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "        self.scheduler = CosineAnnealingWarmRestarts(self.optimizer, T_0=10, eta_min=learning_rate / 10.0)\n",
    "\n",
    "    def prepare_text(self, texts: List[str], output_texts: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Convert texts to ids and return the dataset required for training\n",
    "        and inference.\"\"\"\n",
    "        instruction = \"In this task, you are given a context and question. \\\n",
    "            Provide a short phrase as the answer for the given question using only the information from the context. \\\n",
    "            If you do not know the answer from the context, generate 'no_answer' in the output. \\\n",
    "            Do not repeat the question in the output.\"\n",
    "        inputs = [f\"{instruction} {text}\" for text in texts]\n",
    "        # sample of the answers if possible.\n",
    "        sampled_answers = [random.choice(text.split(\"[<@>]\")) for text in output_texts]\n",
    "        answers = [f\"Answer: {answer}\" for answer in sampled_answers]\n",
    "        input_encodings = self.tokenizer(\n",
    "            inputs,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=lm_input_max_length,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        answer_encodings = self.tokenizer(\n",
    "            answers,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=lm_output_max_length,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        data = {\n",
    "            \"input_ids\": input_encodings.input_ids,\n",
    "            \"attention_mask\": input_encodings.attention_mask,\n",
    "            \"labels\": answer_encodings.input_ids,\n",
    "            \"target_attention_mask\": answer_encodings.attention_mask,\n",
    "            \"input_texts\": texts,\n",
    "            \"output_texts\": output_texts,\n",
    "        }\n",
    "        return data\n",
    "\n",
    "    def train(self, batch: torch.utils.data.Dataset) -> torch.Tensor:\n",
    "        \"\"\"Using the T5-base, run a forward computation over the batch, compute\n",
    "        the log probability over the batch.\n",
    "\n",
    "        This will be used for training.\n",
    "        \"\"\"\n",
    "        self.train_mode_on()\n",
    "        loaded_batch = self.data_to_device(batch, keys=[\"input_ids\", \"attention_mask\", \"target_attention_mask\", \"labels\"])\n",
    "        orig_labels = loaded_batch[\"labels\"]\n",
    "        labels = orig_labels.masked_fill(orig_labels == self.tokenizer.pad_token_id, -100)\n",
    "        with torch.set_grad_enabled(True):\n",
    "            class_log_p = encoder_decoder_log_of_labels(\n",
    "                model=self.model,\n",
    "                input_ids=loaded_batch[\"input_ids\"],\n",
    "                input_mask=loaded_batch[\"attention_mask\"],\n",
    "                decoder_mask=loaded_batch[\"target_attention_mask\"],\n",
    "                labels=labels,\n",
    "                loss_func=self.loss_func,\n",
    "            )\n",
    "        return class_log_p\n",
    "\n",
    "    def generation_pass(self, batch: torch.utils.data.Dataset) -> Tuple[List[str], torch.Tensor]:\n",
    "        \"\"\"This will be used for inference.\"\"\"\n",
    "        self.predict_mode_on()\n",
    "        loaded_batch = self.data_to_device(batch, keys=[\"input_ids\", \"attention_mask\"])\n",
    "        input_ids = loaded_batch[\"input_ids\"]\n",
    "        attention_mask = loaded_batch[\"attention_mask\"]\n",
    "        with torch.no_grad():\n",
    "            # more look here:\n",
    "            # https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L130\n",
    "            predictions_output = self.model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                do_sample=True,\n",
    "                top_p=lm_top_p,\n",
    "                temperature=temperature,\n",
    "                max_length=lm_output_max_length + lm_input_max_length,\n",
    "                num_return_sequences=1,\n",
    "                output_logits=True,\n",
    "                return_dict_in_generate=True,\n",
    "                use_cache=True,\n",
    "                renormalize_logits=True,\n",
    "            )\n",
    "\n",
    "        selected_samples = predictions_output.sequences\n",
    "        # all special tokens will be removed.\n",
    "        predictions_str = self.tokenizer.batch_decode(selected_samples, skip_special_tokens=True)\n",
    "        predictions_str = [pred.lstrip('\"').lstrip(\"'\").rstrip(\"'\").rstrip('\"').strip() for pred in predictions_str]\n",
    "\n",
    "        logits_list = list(predictions_output.logits)\n",
    "        logits = torch.stack(logits_list, dim=1)\n",
    "        ignore_first_token_samples = selected_samples[:, 1:]\n",
    "        labels_to_consider = ignore_first_token_samples.masked_fill(\n",
    "            ignore_first_token_samples == self.tokenizer.pad_token_id, -100\n",
    "        )\n",
    "        final_log_ps = mlm_log_of_labels(logits=logits, labels=labels_to_consider, loss_func=self.loss_func)\n",
    "        actual_lens = torch.sum(torch.where(labels_to_consider > 0, 1, 0), dim=1)\n",
    "        # Average log probs per token (length normalization).\n",
    "        return predictions_str, final_log_ps / actual_lens\n",
    "\n",
    "    def predict(self, batch: torch.utils.data.Dataset) -> Iterator[Dict[str, str]]:\n",
    "        \"\"\"The main prediction loop.\"\"\"\n",
    "        answers, log_ps = self.generation_pass(batch)\n",
    "        log_ps = log_ps.cpu().detach().numpy()\n",
    "        for idx, answer in enumerate(answers):\n",
    "            output_row = {\n",
    "                \"potential_answer\": answer,\n",
    "                \"prediction_score\": log_ps[idx],\n",
    "                \"gold_answer\": batch[\"output_texts\"][idx],\n",
    "            }\n",
    "            yield output_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gen_fewshot_file(file_path: str) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Load the fewshot files for QA task.\"\"\"\n",
    "    df = pd.read_csv(file_path, sep=\"\\t\")\n",
    "    input_texts = df.article.tolist()\n",
    "    output_texts = df.answer.tolist()\n",
    "    return input_texts, output_texts\n",
    "\n",
    "\n",
    "def create_dataloader(\n",
    "    model: T5BaseQA,\n",
    "    train_file_name: Optional[str] = None,\n",
    "    dev_file_name: Optional[str] = None,\n",
    "    test_file_name: Optional[str] = None,\n",
    ") -> DataLoader:\n",
    "    \"\"\"Function to create the required dataloader to train the LM models.\"\"\"\n",
    "    if train_file_name is not None:\n",
    "        input_texts, output_texts = read_gen_fewshot_file(train_file_name)\n",
    "        shuffle = True\n",
    "        batch_size = train_batch_size\n",
    "\n",
    "    if dev_file_name is not None:\n",
    "        input_texts, output_texts = read_gen_fewshot_file(dev_file_name)\n",
    "        shuffle = False\n",
    "        batch_size = eval_batch_size\n",
    "\n",
    "    if test_file_name is not None:\n",
    "        input_texts, output_texts = read_gen_fewshot_file(test_file_name)\n",
    "        shuffle = False\n",
    "        batch_size = eval_batch_size\n",
    "\n",
    "    data = model.prepare_text(input_texts, output_texts)\n",
    "    dataset = DictDataset(data)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAMetricModel:\n",
    "    \"\"\"Load and cache a model used for evaluating generative text\n",
    "    generation.\"\"\"\n",
    "\n",
    "    model_id = \"sentence-transformers/sentence-t5-xxl\"\n",
    "\n",
    "    def __init__(self, device: str = \"cuda:0\", batch_size: int = 16) -> None:\n",
    "        \"\"\"Save the gpu device and construct the model and cache it.\"\"\"\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.metric_model = SentenceTransformer(self.model_id, device=self.device).eval()\n",
    "\n",
    "    def compute_metric(self, predictions: List[str], references: List[List[str]]) -> float:\n",
    "        \"\"\"Compute the metric for the given predictions and multiple\n",
    "        references.\"\"\"\n",
    "        average_score = torch.tensor(0.0, device=self.device)\n",
    "        num_chunks = max(len(predictions) // self.batch_size, 1)\n",
    "        for chunk_i in range(num_chunks):\n",
    "            clear_cache()\n",
    "\n",
    "            if (chunk_i + 1) * self.batch_size <= len(predictions):\n",
    "                predictions_sub_arr = predictions[chunk_i * self.batch_size : (chunk_i + 1) * self.batch_size]\n",
    "                references_sub_arr = references[chunk_i * self.batch_size : (chunk_i + 1) * self.batch_size]\n",
    "            else:\n",
    "                predictions_sub_arr = predictions[chunk_i * self.batch_size :]\n",
    "                references_sub_arr = references[chunk_i * self.batch_size :]\n",
    "\n",
    "            # need to track multiple references.\n",
    "            ref_sub_arr_len = [len(ref_sub_arr) for ref_sub_arr in references_sub_arr]\n",
    "            references_sub_arr_flattened = []\n",
    "            for ref_sub_arr in references_sub_arr:\n",
    "                references_sub_arr_flattened.extend(ref_sub_arr)\n",
    "\n",
    "            prediction_embeddings = self.metric_model.encode(\n",
    "                predictions_sub_arr,\n",
    "                show_progress_bar=False,\n",
    "                batch_size=self.batch_size,\n",
    "                device=self.device,\n",
    "                normalize_embeddings=True,\n",
    "                convert_to_tensor=True,\n",
    "            )\n",
    "\n",
    "            references_embeddings = self.metric_model.encode(\n",
    "                references_sub_arr_flattened,\n",
    "                show_progress_bar=False,\n",
    "                batch_size=self.batch_size,\n",
    "                device=self.device,\n",
    "                normalize_embeddings=True,\n",
    "                convert_to_tensor=True,\n",
    "            )\n",
    "            dot_products = torch.matmul(prediction_embeddings, references_embeddings.t())\n",
    "            score_collector = torch.zeros_like(dot_products)\n",
    "            i = 0\n",
    "            j = 0\n",
    "            while i < len(predictions_sub_arr):\n",
    "                j_len = ref_sub_arr_len[i]\n",
    "                score_collector[i][j : j + j_len] = 1.0 / j_len\n",
    "                i += 1\n",
    "                j += j_len\n",
    "\n",
    "            average_score += torch.sum(dot_products * score_collector)\n",
    "        return (average_score / len(predictions)).item()\n",
    "\n",
    "\n",
    "qa_metric_model = None\n",
    "\n",
    "\n",
    "def postprocess_qa(label: str) -> str:\n",
    "    label = str(label)\n",
    "    label = label.lower()\n",
    "    label = label.replace(\"\\n\", \" \")\n",
    "    label = label.removesuffix(\"</s>\")\n",
    "    label = label.removeprefix(\"<s>\")\n",
    "    label = label.removeprefix(\"\\n\")\n",
    "    label = label.removesuffix(\"\\n\")\n",
    "    label = label.removeprefix(\".\")\n",
    "    label = label.removesuffix(\".\")\n",
    "    label = label.removeprefix(\"answer:\")\n",
    "    label = label.removeprefix(\",\")\n",
    "    label = label.strip()\n",
    "    if \"no answer\" in label or \"no_answer\" in label:\n",
    "        label = \"no answer\"\n",
    "    return label\n",
    "\n",
    "\n",
    "def qa_metric(prediction_file: str) -> Dict[str, float]:\n",
    "    \"\"\"Compute the metric for the qa task.\"\"\"\n",
    "    global qa_metric_model\n",
    "    if qa_metric_model is None:\n",
    "        qa_metric_model = QAMetricModel(device=metric_device, batch_size=metric_batch_size)\n",
    "\n",
    "    df = pd.read_csv(prediction_file, delimiter=\",\")\n",
    "\n",
    "    gold_answers = [postprocess_qa(label) for label in df[\"gold_answer\"].tolist()]\n",
    "\n",
    "    multiple_gold_answers = []\n",
    "    for answer in gold_answers:\n",
    "        multiple_gold_answers.append(answer.split(\"[<@>]\"))\n",
    "\n",
    "    return_metrics: Dict[str, float] = {}\n",
    "    metrics = {\n",
    "        \"potential_answer\": \"qa_score\",\n",
    "    }\n",
    "\n",
    "    for metric_column, metric in metrics.items():\n",
    "        if metric_column in df.columns:\n",
    "            predictions = [postprocess_qa(pred) for pred in df[metric_column].tolist()]\n",
    "            score = qa_metric_model.compute_metric(predictions, multiple_gold_answers)\n",
    "            return_metrics[metric] = score\n",
    "\n",
    "    return return_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and start training.\n",
    "set_random_seed(42)\n",
    "\n",
    "model = T5BaseQA(device=\"cuda:0\", seed=42)\n",
    "model.to_device()\n",
    "train_dataloader = create_dataloader(model, train_file_name=train_file_name)\n",
    "dev_dataloader = create_dataloader(model, dev_file_name=dev_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:0\n",
      "\n",
      "Prediction Step: 1.\n",
      "Epoch: 0 | Batch: 1 | Mean Loss: 45.267826080322266 | Epoch Loss: 45.267826080322266 | Loss: 45.267826080322266\n",
      "\n",
      "Prediction Step: 2.\n",
      "Epoch: 0 | Batch: 2 | Mean Loss: 32.99549865722656 | Epoch Loss: 32.99549865722656 | Loss: 20.72317123413086\n",
      "\n",
      "Prediction Step: 3.\n",
      "Epoch: 0 | Batch: 3 | Mean Loss: 23.761794408162434 | Epoch Loss: 23.761794408162434 | Loss: 5.29438591003418\n",
      "\n",
      "Prediction Step: 4.\n",
      "Epoch: 0 | Batch: 4 | Mean Loss: 21.109869956970215 | Epoch Loss: 21.109869956970215 | Loss: 13.154096603393555\n",
      "\n",
      "Prediction Step: 5.\n",
      "Epoch: 0 | Batch: 5 | Mean Loss: 20.640832901000977 | Epoch Loss: 20.640832901000977 | Loss: 18.764684677124023\n",
      "\n",
      "Prediction Step: 6.\n",
      "Epoch: 0 | Batch: 6 | Mean Loss: 19.64315112431844 | Epoch Loss: 19.64315112431844 | Loss: 14.654742240905762\n",
      "\n",
      "Prediction Step: 7.\n",
      "Epoch: 0 | Batch: 7 | Mean Loss: 18.264043399265834 | Epoch Loss: 18.264043399265834 | Loss: 9.989397048950195\n",
      "\n",
      "Prediction Step: 8.\n",
      "Epoch: 0 | Batch: 8 | Mean Loss: 16.87773835659027 | Epoch Loss: 16.87773835659027 | Loss: 7.173603057861328\n",
      "\n",
      "Prediction Step: 9.\n",
      "Epoch: 0 | Batch: 9 | Mean Loss: 15.770938237508139 | Epoch Loss: 15.770938237508139 | Loss: 6.916537284851074\n",
      "\n",
      "Prediction Step: 10.\n",
      "Epoch: 0 | Batch: 10 | Mean Loss: 14.91219882965088 | Epoch Loss: 14.91219882965088 | Loss: 7.183544158935547\n",
      "\n",
      "Prediction Step: 11.\n",
      "Epoch: 0 | Batch: 11 | Mean Loss: 14.486775311556729 | Epoch Loss: 14.486775311556729 | Loss: 10.232540130615234\n",
      "\n",
      "Prediction Step: 12.\n",
      "Epoch: 0 | Batch: 12 | Mean Loss: 13.76372524102529 | Epoch Loss: 13.76372524102529 | Loss: 5.810174465179443\n",
      "\n",
      "Prediction Step: 13.\n",
      "Epoch: 0 | Batch: 13 | Mean Loss: 13.40738821029663 | Epoch Loss: 13.40738821029663 | Loss: 9.131343841552734\n",
      "\n",
      "Prediction Step: 14.\n",
      "Epoch: 0 | Batch: 14 | Mean Loss: 12.721693583897181 | Epoch Loss: 12.721693583897181 | Loss: 3.8076634407043457\n",
      "\n",
      "Prediction Step: 15.\n",
      "Epoch: 0 | Batch: 15 | Mean Loss: 12.744876098632812 | Epoch Loss: 12.744876098632812 | Loss: 13.06943130493164\n",
      "\n",
      "Prediction Step: 16.\n",
      "Epoch: 0 | Batch: 16 | Mean Loss: 12.405760884284973 | Epoch Loss: 12.405760884284973 | Loss: 7.319032669067383\n",
      "\n",
      "Prediction Step: 17.\n",
      "Epoch: 0 | Batch: 17 | Mean Loss: 12.144302480361041 | Epoch Loss: 12.144302480361041 | Loss: 7.960968017578125\n",
      "\n",
      "Prediction Step: 18.\n",
      "Epoch: 0 | Batch: 18 | Mean Loss: 12.05430179172092 | Epoch Loss: 12.05430179172092 | Loss: 10.524290084838867\n",
      "\n",
      "Prediction Step: 19.\n",
      "Epoch: 0 | Batch: 19 | Mean Loss: 11.677322638662238 | Epoch Loss: 11.677322638662238 | Loss: 4.891697883605957\n",
      "\n",
      "Prediction Step: 20.\n",
      "Epoch: 0 | Batch: 20 | Mean Loss: 11.547342014312743 | Epoch Loss: 11.547342014312743 | Loss: 9.077710151672363\n",
      "\n",
      "Prediction Step: 21.\n",
      "Epoch: 0 | Batch: 21 | Mean Loss: 11.160175368899392 | Epoch Loss: 11.160175368899392 | Loss: 3.416842460632324\n",
      "\n",
      "Prediction Step: 22.\n",
      "Epoch: 0 | Batch: 22 | Mean Loss: 10.802730256860906 | Epoch Loss: 10.802730256860906 | Loss: 3.2963829040527344\n",
      "\n",
      "Prediction Step: 23.\n",
      "Epoch: 0 | Batch: 23 | Mean Loss: 10.990826357965885 | Epoch Loss: 10.990826357965885 | Loss: 15.12894058227539\n",
      "\n",
      "Prediction Step: 24.\n",
      "Epoch: 0 | Batch: 24 | Mean Loss: 10.704322894414267 | Epoch Loss: 10.704322894414267 | Loss: 4.114743232727051\n",
      "\n",
      "Prediction Step: 25.\n",
      "Epoch: 0 | Batch: 25 | Mean Loss: 10.620552673339844 | Epoch Loss: 10.620552673339844 | Loss: 8.610067367553711\n",
      "\n",
      "Prediction Step: 26.\n",
      "Epoch: 0 | Batch: 26 | Mean Loss: 10.548222395089956 | Epoch Loss: 10.548222395089956 | Loss: 8.739965438842773\n",
      "\n",
      "Prediction Step: 27.\n",
      "Epoch: 0 | Batch: 27 | Mean Loss: 10.394792945296675 | Epoch Loss: 10.394792945296675 | Loss: 6.405627250671387\n",
      "\n",
      "Prediction Step: 28.\n",
      "Epoch: 0 | Batch: 28 | Mean Loss: 10.399573871067592 | Epoch Loss: 10.399573871067592 | Loss: 10.528658866882324\n",
      "\n",
      "Prediction Step: 29.\n",
      "Epoch: 0 | Batch: 29 | Mean Loss: 10.287212273170208 | Epoch Loss: 10.287212273170208 | Loss: 7.141087532043457\n",
      "\n",
      "Prediction Step: 30.\n",
      "Epoch: 0 | Batch: 30 | Mean Loss: 10.327102947235108 | Epoch Loss: 10.327102947235108 | Loss: 11.483932495117188\n",
      "\n",
      "Prediction Step: 31.\n",
      "Epoch: 0 | Batch: 31 | Mean Loss: 10.336145708637853 | Epoch Loss: 10.336145708637853 | Loss: 10.607428550720215\n",
      "\n",
      "Prediction Step: 32.\n",
      "Epoch: 0 | Batch: 32 | Mean Loss: 10.148622632026672 | Epoch Loss: 10.148622632026672 | Loss: 4.335407257080078\n",
      "\n",
      "Prediction Step: 33.\n",
      "Epoch: 0 | Batch: 33 | Mean Loss: 10.075213836901115 | Epoch Loss: 10.075213836901115 | Loss: 7.726132392883301\n",
      "\n",
      "Prediction Step: 34.\n",
      "Epoch: 0 | Batch: 34 | Mean Loss: 10.044345154481775 | Epoch Loss: 10.044345154481775 | Loss: 9.025678634643555\n",
      "\n",
      "Prediction Step: 35.\n",
      "Epoch: 0 | Batch: 35 | Mean Loss: 9.954333550589425 | Epoch Loss: 9.954333550589425 | Loss: 6.893939018249512\n",
      "\n",
      "Prediction Step: 36.\n",
      "Epoch: 0 | Batch: 36 | Mean Loss: 9.800235721800062 | Epoch Loss: 9.800235721800062 | Loss: 4.406811714172363\n",
      "\n",
      "Prediction Step: 37.\n",
      "Epoch: 0 | Batch: 37 | Mean Loss: 9.727278322786898 | Epoch Loss: 9.727278322786898 | Loss: 7.100811958312988\n",
      "\n",
      "Prediction Step: 38.\n",
      "Epoch: 0 | Batch: 38 | Mean Loss: 9.628881303887619 | Epoch Loss: 9.628881303887619 | Loss: 5.988191604614258\n",
      "\n",
      "Prediction Step: 39.\n",
      "Epoch: 0 | Batch: 39 | Mean Loss: 9.739484689174555 | Epoch Loss: 9.739484689174555 | Loss: 13.942413330078125\n",
      "\n",
      "Prediction Step: 40.\n",
      "Epoch: 0 | Batch: 40 | Mean Loss: 9.613386106491088 | Epoch Loss: 9.613386106491088 | Loss: 4.6955413818359375\n",
      "\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/snajafi/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_32711/4204586278.py\", line 1, in <module>\n",
      "    train_loop(\n",
      "  File \"/remote/cirrus-home/snajafi/llm-research/src/general_utils.py\", line 102, in train_loop\n",
      "  File \"/tmp/ipykernel_32711/3103261845.py\", line 108, in qa_metric\n",
      "    score = qa_metric_model.compute_metric(predictions, multiple_gold_answers)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_32711/3103261845.py\", line 19, in compute_metric\n",
      "    clear_cache()\n",
      "  File \"/remote/cirrus-home/snajafi/llm-research/src/model_utils.py\", line 31, in clear_cache\n",
      "    torch.cuda.empty_cache()\n",
      "  File \"/home/snajafi/anaconda3/lib/python3.11/site-packages/torch/cuda/memory.py\", line 162, in empty_cache\n",
      "    torch._C._cuda_emptyCache()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/snajafi/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2144, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/snajafi/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/snajafi/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/snajafi/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/snajafi/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/snajafi/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/snajafi/anaconda3/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/home/snajafi/anaconda3/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/home/snajafi/anaconda3/lib/python3.11/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/snajafi/anaconda3/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/home/snajafi/anaconda3/lib/python3.11/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/snajafi/anaconda3/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/home/snajafi/anaconda3/lib/python3.11/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"/home/snajafi/anaconda3/lib/python3.11/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "train_loop(\n",
    "    model=model,\n",
    "    mode=\"train\",\n",
    "    model_path=\"/tmp\",\n",
    "    metric_to_save=\"qa_score\",\n",
    "    max_epochs=10,\n",
    "    training_steps=100000,  # not important\n",
    "    steps_per_checkpoint=8,\n",
    "    metric=qa_metric,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=dev_dataloader,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env-conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
