{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Any, Dict, Iterator, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "from src.base_lm import BaseLM\n",
    "from src.general_utils import DictDataset, test_loop, train_loop\n",
    "from src.model_utils import clear_cache, encoder_decoder_log_of_labels, mlm_log_of_labels, set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 8\n",
    "eval_batch_size = 16\n",
    "lm_input_max_length = 1024\n",
    "lm_output_max_length = 128\n",
    "lm_top_p = 0.9\n",
    "temperature = 0.6\n",
    "metric_device = \"cuda:1\"\n",
    "metric_batch_size = 16\n",
    "learning_rate = 0.001\n",
    "train_file_name = \"128-shot-datasets/squad/128-42-train.tsv\"\n",
    "dev_file_name = \"128-shot-datasets/squad/128-42-dev.tsv\"\n",
    "test_file_name = \"128-shot-datasets/squad/test.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"QA Model based on T5 base and without any optimizations for large-scale training.\"\"\"\n",
    "\n",
    "\n",
    "class T5BaseQA(BaseLM):\n",
    "    \"\"\"Class to implement T5-base for QA task.\"\"\"\n",
    "\n",
    "    def __init__(self, device: str, seed: int) -> None:\n",
    "        super().__init__(device=device, model_name=\"t5_base\", seed=seed)\n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"google/t5-base-lm-adapt\")\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(\"google/t5-base-lm-adapt\")\n",
    "        # to train the main lm, we update all of its parameters.\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "        self.scheduler = CosineAnnealingWarmRestarts(self.optimizer, T_0=10, eta_min=learning_rate / 10.0)\n",
    "\n",
    "    def prepare_text(self, texts: List[str], output_texts: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Convert texts to ids and return the dataset required for training\n",
    "        and inference.\"\"\"\n",
    "        instruction = \"In this task, you are given a context and question. \\\n",
    "            Provide a short phrase as the answer for the given question using only the information from the context. \\\n",
    "            If you do not know the answer from the context, generate 'no_answer' in the output. \\\n",
    "            Do not repeat the question in the output.\"\n",
    "        inputs = [f\"{instruction} {text}\" for text in texts]\n",
    "        # sample of the answers if possible.\n",
    "        sampled_answers = [random.choice(text.split(\"[<@>]\")) for text in output_texts]\n",
    "        answers = [f\"Answer: {answer}\" for answer in sampled_answers]\n",
    "        input_encodings = self.tokenizer(\n",
    "            inputs,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=lm_input_max_length,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        answer_encodings = self.tokenizer(\n",
    "            answers,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=lm_output_max_length,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        data = {\n",
    "            \"input_ids\": input_encodings.input_ids,\n",
    "            \"attention_mask\": input_encodings.attention_mask,\n",
    "            \"labels\": answer_encodings.input_ids,\n",
    "            \"target_attention_mask\": answer_encodings.attention_mask,\n",
    "            \"input_texts\": texts,\n",
    "            \"output_texts\": output_texts,\n",
    "        }\n",
    "        return data\n",
    "\n",
    "    def train(self, batch: torch.utils.data.Dataset) -> torch.Tensor:\n",
    "        \"\"\"Using the T5-base, run a forward computation over the batch, compute\n",
    "        the log probability over the batch.\n",
    "\n",
    "        This will be used for training.\n",
    "        \"\"\"\n",
    "        self.train_mode_on()\n",
    "        loaded_batch = self.data_to_device(batch, keys=[\"input_ids\", \"attention_mask\", \"target_attention_mask\", \"labels\"])\n",
    "        orig_labels = loaded_batch[\"labels\"]\n",
    "        labels = orig_labels.masked_fill(orig_labels == self.tokenizer.pad_token_id, -100)\n",
    "        with torch.set_grad_enabled(True):\n",
    "            class_log_p = encoder_decoder_log_of_labels(\n",
    "                model=self.model,\n",
    "                input_ids=loaded_batch[\"input_ids\"],\n",
    "                input_mask=loaded_batch[\"attention_mask\"],\n",
    "                decoder_mask=loaded_batch[\"target_attention_mask\"],\n",
    "                labels=labels,\n",
    "                loss_func=self.loss_func,\n",
    "            )\n",
    "        return class_log_p\n",
    "\n",
    "    def generation_pass(self, batch: torch.utils.data.Dataset) -> Tuple[List[str], torch.Tensor]:\n",
    "        \"\"\"This will be used for inference.\"\"\"\n",
    "        self.predict_mode_on()\n",
    "        loaded_batch = self.data_to_device(batch, keys=[\"input_ids\", \"attention_mask\"])\n",
    "        input_ids = loaded_batch[\"input_ids\"]\n",
    "        attention_mask = loaded_batch[\"attention_mask\"]\n",
    "        with torch.no_grad():\n",
    "            # more look here:\n",
    "            # https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L130\n",
    "            predictions_output = self.model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                do_sample=True,\n",
    "                top_p=lm_top_p,\n",
    "                temperature=temperature,\n",
    "                max_length=lm_output_max_length + lm_input_max_length,\n",
    "                num_return_sequences=1,\n",
    "                output_logits=True,\n",
    "                return_dict_in_generate=True,\n",
    "                use_cache=True,\n",
    "                renormalize_logits=True,\n",
    "            )\n",
    "\n",
    "        selected_samples = predictions_output.sequences\n",
    "        # all special tokens will be removed.\n",
    "        predictions_str = self.tokenizer.batch_decode(selected_samples, skip_special_tokens=True)\n",
    "        predictions_str = [pred.lstrip('\"').lstrip(\"'\").rstrip(\"'\").rstrip('\"').strip() for pred in predictions_str]\n",
    "\n",
    "        logits_list = list(predictions_output.logits)\n",
    "        logits = torch.stack(logits_list, dim=1)\n",
    "        ignore_first_token_samples = selected_samples[:, 1:]\n",
    "        labels_to_consider = ignore_first_token_samples.masked_fill(\n",
    "            ignore_first_token_samples == self.tokenizer.pad_token_id, -100\n",
    "        )\n",
    "        final_log_ps = mlm_log_of_labels(logits=logits, labels=labels_to_consider, loss_func=self.loss_func)\n",
    "        actual_lens = torch.sum(torch.where(labels_to_consider > 0, 1, 0), dim=1)\n",
    "        # Average log probs per token (length normalization).\n",
    "        return predictions_str, final_log_ps / actual_lens\n",
    "\n",
    "    def predict(self, batch: torch.utils.data.Dataset) -> Iterator[Dict[str, str]]:\n",
    "        \"\"\"The main prediction loop.\"\"\"\n",
    "        answers, log_ps = self.generation_pass(batch)\n",
    "        log_ps = log_ps.cpu().detach().numpy()\n",
    "        for idx, answer in enumerate(answers):\n",
    "            output_row = {\n",
    "                \"potential_answer\": answer,\n",
    "                \"prediction_score\": log_ps[idx],\n",
    "                \"gold_answer\": batch[\"output_texts\"][idx],\n",
    "            }\n",
    "            yield output_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gen_fewshot_file(file_path: str) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Load the fewshot files for QA task.\"\"\"\n",
    "    df = pd.read_csv(file_path, sep=\"\\t\")\n",
    "    input_texts = df.article.tolist()\n",
    "    output_texts = df.answer.tolist()\n",
    "    return input_texts, output_texts\n",
    "\n",
    "\n",
    "def create_dataloader(\n",
    "    model: T5BaseQA,\n",
    "    train_file_name: Optional[str] = None,\n",
    "    dev_file_name: Optional[str] = None,\n",
    "    test_file_name: Optional[str] = None,\n",
    ") -> DataLoader:\n",
    "    \"\"\"Function to create the required dataloader to train the LM models.\"\"\"\n",
    "    if train_file_name is not None:\n",
    "        input_texts, output_texts = read_gen_fewshot_file(train_file_name)\n",
    "        shuffle = True\n",
    "        batch_size = train_batch_size\n",
    "\n",
    "    if dev_file_name is not None:\n",
    "        input_texts, output_texts = read_gen_fewshot_file(dev_file_name)\n",
    "        shuffle = False\n",
    "        batch_size = eval_batch_size\n",
    "\n",
    "    if test_file_name is not None:\n",
    "        input_texts, output_texts = read_gen_fewshot_file(test_file_name)\n",
    "        shuffle = False\n",
    "        batch_size = eval_batch_size\n",
    "\n",
    "    data = model.prepare_text(input_texts, output_texts)\n",
    "    dataset = DictDataset(data)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAMetricModel:\n",
    "    \"\"\"Load and cache a model used for evaluating generative text\n",
    "    generation.\"\"\"\n",
    "\n",
    "    model_id = \"sentence-transformers/sentence-t5-xxl\"\n",
    "\n",
    "    def __init__(self, device: str = \"cuda:0\", batch_size: int = 16) -> None:\n",
    "        \"\"\"Save the gpu device and construct the model and cache it.\"\"\"\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.metric_model = SentenceTransformer(self.model_id, device=self.device).eval()\n",
    "\n",
    "    def compute_metric(self, predictions: List[str], references: List[List[str]]) -> float:\n",
    "        \"\"\"Compute the metric for the given predictions and multiple\n",
    "        references.\"\"\"\n",
    "        average_score = torch.tensor(0.0, device=self.device)\n",
    "        num_chunks = max(len(predictions) // self.batch_size, 1)\n",
    "        for chunk_i in range(num_chunks):\n",
    "            clear_cache()\n",
    "\n",
    "            if (chunk_i + 1) * self.batch_size <= len(predictions):\n",
    "                predictions_sub_arr = predictions[chunk_i * self.batch_size : (chunk_i + 1) * self.batch_size]\n",
    "                references_sub_arr = references[chunk_i * self.batch_size : (chunk_i + 1) * self.batch_size]\n",
    "            else:\n",
    "                predictions_sub_arr = predictions[chunk_i * self.batch_size :]\n",
    "                references_sub_arr = references[chunk_i * self.batch_size :]\n",
    "\n",
    "            # need to track multiple references.\n",
    "            ref_sub_arr_len = [len(ref_sub_arr) for ref_sub_arr in references_sub_arr]\n",
    "            references_sub_arr_flattened = []\n",
    "            for ref_sub_arr in references_sub_arr:\n",
    "                references_sub_arr_flattened.extend(ref_sub_arr)\n",
    "\n",
    "            prediction_embeddings = self.metric_model.encode(\n",
    "                predictions_sub_arr,\n",
    "                show_progress_bar=False,\n",
    "                batch_size=self.batch_size,\n",
    "                device=self.device,\n",
    "                normalize_embeddings=True,\n",
    "                convert_to_tensor=True,\n",
    "            )\n",
    "\n",
    "            references_embeddings = self.metric_model.encode(\n",
    "                references_sub_arr_flattened,\n",
    "                show_progress_bar=False,\n",
    "                batch_size=self.batch_size,\n",
    "                device=self.device,\n",
    "                normalize_embeddings=True,\n",
    "                convert_to_tensor=True,\n",
    "            )\n",
    "            dot_products = torch.matmul(prediction_embeddings, references_embeddings.t())\n",
    "            score_collector = torch.zeros_like(dot_products)\n",
    "            i = 0\n",
    "            j = 0\n",
    "            while i < len(predictions_sub_arr):\n",
    "                j_len = ref_sub_arr_len[i]\n",
    "                score_collector[i][j : j + j_len] = 1.0 / j_len\n",
    "                i += 1\n",
    "                j += j_len\n",
    "\n",
    "            average_score += torch.sum(dot_products * score_collector)\n",
    "        return (average_score / len(predictions)).item()\n",
    "\n",
    "\n",
    "qa_metric_model = None\n",
    "\n",
    "\n",
    "def postprocess_qa(label: str) -> str:\n",
    "    label = str(label)\n",
    "    label = label.lower()\n",
    "    label = label.replace(\"\\n\", \" \")\n",
    "    label = label.removesuffix(\"</s>\")\n",
    "    label = label.removeprefix(\"<s>\")\n",
    "    label = label.removeprefix(\"\\n\")\n",
    "    label = label.removesuffix(\"\\n\")\n",
    "    label = label.removeprefix(\".\")\n",
    "    label = label.removesuffix(\".\")\n",
    "    label = label.removeprefix(\"answer:\")\n",
    "    label = label.removeprefix(\",\")\n",
    "    label = label.strip()\n",
    "    if \"no answer\" in label or \"no_answer\" in label:\n",
    "        label = \"no answer\"\n",
    "    return label\n",
    "\n",
    "\n",
    "def qa_metric(prediction_file: str) -> Dict[str, float]:\n",
    "    \"\"\"Compute the metric for the qa task.\"\"\"\n",
    "    global qa_metric_model\n",
    "    if qa_metric_model is None:\n",
    "        qa_metric_model = QAMetricModel(device=metric_device, batch_size=metric_batch_size)\n",
    "\n",
    "    df = pd.read_csv(prediction_file, delimiter=\",\")\n",
    "\n",
    "    gold_answers = [postprocess_qa(label) for label in df[\"gold_answer\"].tolist()]\n",
    "\n",
    "    multiple_gold_answers = []\n",
    "    for answer in gold_answers:\n",
    "        multiple_gold_answers.append(answer.split(\"[<@>]\"))\n",
    "\n",
    "    return_metrics: Dict[str, float] = {}\n",
    "    metrics = {\n",
    "        \"potential_answer\": \"qa_score\",\n",
    "    }\n",
    "\n",
    "    for metric_column, metric in metrics.items():\n",
    "        if metric_column in df.columns:\n",
    "            predictions = [postprocess_qa(pred) for pred in df[metric_column].tolist()]\n",
    "            score = qa_metric_model.compute_metric(predictions, multiple_gold_answers)\n",
    "            return_metrics[metric] = score\n",
    "\n",
    "    return return_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and start training.\n",
    "set_random_seed(42)\n",
    "\n",
    "model = T5BaseQA(device=\"cuda:0\", seed=42)\n",
    "model.to_device()\n",
    "train_dataloader = create_dataloader(model, train_file_name=train_file_name)\n",
    "dev_dataloader = create_dataloader(model, dev_file_name=dev_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "\n",
      "Epoch:0\n",
      "\n",
      "Epoch: 0 | Batch: 1 | Mean Loss: 44.15757751464844 | Epoch Loss: 44.15757751464844 | Loss: 44.15757751464844\n",
      "\n",
      "Epoch: 0 | Batch: 2 | Mean Loss: 32.253082275390625 | Epoch Loss: 32.253082275390625 | Loss: 20.348587036132812\n",
      "\n",
      "Epoch: 0 | Batch: 3 | Mean Loss: 25.979056040445965 | Epoch Loss: 25.979056040445965 | Loss: 13.43100357055664\n",
      "\n",
      "Epoch: 0 | Batch: 4 | Mean Loss: 21.853553533554077 | Epoch Loss: 21.853553533554077 | Loss: 9.477046012878418\n",
      "\n",
      "Epoch: 0 | Batch: 5 | Mean Loss: 19.072867107391357 | Epoch Loss: 19.072867107391357 | Loss: 7.9501214027404785\n",
      "\n",
      "Epoch: 0 | Batch: 6 | Mean Loss: 17.21241593360901 | Epoch Loss: 17.21241593360901 | Loss: 7.910160064697266\n",
      "\n",
      "Epoch: 0 | Batch: 7 | Mean Loss: 15.54835571561541 | Epoch Loss: 15.54835571561541 | Loss: 5.563994407653809\n",
      "\n",
      "Epoch: 0 | Batch: 8 | Mean Loss: 14.393797934055328 | Epoch Loss: 14.393797934055328 | Loss: 6.311893463134766\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Epoch: 0 | Batch: 9 | Mean Loss: 13.498482333289253 | Epoch Loss: 13.498482333289253 | Loss: 6.3359575271606445\n",
      "\n",
      "Epoch: 0 | Batch: 10 | Mean Loss: 12.66730456352234 | Epoch Loss: 12.66730456352234 | Loss: 5.186704635620117\n",
      "\n",
      "Epoch: 0 | Batch: 11 | Mean Loss: 12.123628703030674 | Epoch Loss: 12.123628703030674 | Loss: 6.686870098114014\n",
      "\n",
      "Epoch: 0 | Batch: 12 | Mean Loss: 11.582106351852417 | Epoch Loss: 11.582106351852417 | Loss: 5.625360488891602\n",
      "\n",
      "Epoch: 0 | Batch: 13 | Mean Loss: 11.240076872018667 | Epoch Loss: 11.240076872018667 | Loss: 7.135723114013672\n",
      "\n",
      "Epoch: 0 | Batch: 14 | Mean Loss: 10.71891096660069 | Epoch Loss: 10.71891096660069 | Loss: 3.943754196166992\n",
      "\n",
      "Epoch: 0 | Batch: 15 | Mean Loss: 10.487592601776123 | Epoch Loss: 10.487592601776123 | Loss: 7.249135494232178\n",
      "\n",
      "Epoch: 0 | Batch: 16 | Mean Loss: 10.26942589879036 | Epoch Loss: 10.26942589879036 | Loss: 6.996925354003906\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "\n",
      "Epoch:1\n",
      "\n",
      "Epoch: 1 | Batch: 1 | Mean Loss: 9.893583998960608 | Epoch Loss: 3.8801136016845703 | Loss: 3.8801136016845703\n",
      "\n",
      "Epoch: 1 | Batch: 2 | Mean Loss: 9.439748406410217 | Epoch Loss: 2.8023284673690796 | Loss: 1.7245433330535889\n",
      "\n",
      "Epoch: 1 | Batch: 3 | Mean Loss: 9.023334691399022 | Epoch Loss: 2.377514918645223 | Loss: 1.5278878211975098\n",
      "\n",
      "Epoch: 1 | Batch: 4 | Mean Loss: 8.702714335918426 | Epoch Loss: 2.4358680844306946 | Loss: 2.6109275817871094\n",
      "\n",
      "Epoch: 1 | Batch: 5 | Mean Loss: 8.44236870039077 | Epoch Loss: 2.595785665512085 | Loss: 3.2354559898376465\n",
      "\n",
      "Epoch: 1 | Batch: 6 | Mean Loss: 8.22176923535087 | Epoch Loss: 2.761351466178894 | Loss: 3.5891804695129395\n",
      "\n",
      "Epoch: 1 | Batch: 7 | Mean Loss: 8.022527290427167 | Epoch Loss: 2.886759042739868 | Loss: 3.639204502105713\n",
      "\n",
      "Epoch: 1 | Batch: 8 | Mean Loss: 7.871530185143153 | Epoch Loss: 3.0757387578487396 | Loss: 4.39859676361084\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Epoch: 1 | Batch: 9 | Mean Loss: 7.710813770294189 | Epoch Loss: 3.162169986300998 | Loss: 3.8536198139190674\n",
      "\n",
      "Epoch: 1 | Batch: 10 | Mean Loss: 7.531188506346482 | Epoch Loss: 3.1500086784362793 | Loss: 3.0405569076538086\n",
      "\n",
      "Epoch: 1 | Batch: 11 | Mean Loss: 7.3631336512389005 | Epoch Loss: 3.1357994729822334 | Loss: 2.9937074184417725\n",
      "\n",
      "Epoch: 1 | Batch: 12 | Mean Loss: 7.1836307644844055 | Epoch Loss: 3.069237252076467 | Loss: 2.337052822113037\n",
      "\n",
      "Epoch: 1 | Batch: 13 | Mean Loss: 7.079849810435854 | Epoch Loss: 3.154217701691848 | Loss: 4.173983097076416\n",
      "\n",
      "Epoch: 1 | Batch: 14 | Mean Loss: 7.005330920219421 | Epoch Loss: 3.2749366589954922 | Loss: 4.844283103942871\n",
      "\n",
      "Epoch: 1 | Batch: 15 | Mean Loss: 6.898220223765219 | Epoch Loss: 3.3022675037384035 | Loss: 3.68489933013916\n",
      "\n",
      "Epoch: 1 | Batch: 16 | Mean Loss: 6.79593925178051 | Epoch Loss: 3.3224526047706604 | Loss: 3.6252291202545166\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "\n",
      "Epoch:2\n",
      "\n",
      "Epoch: 2 | Batch: 1 | Mean Loss: 6.640153841538862 | Epoch Loss: 1.6550207138061523 | Loss: 1.6550207138061523\n",
      "\n",
      "Epoch: 2 | Batch: 2 | Mean Loss: 6.499282170744503 | Epoch Loss: 1.752768874168396 | Loss: 1.8505170345306396\n",
      "\n",
      "Epoch: 2 | Batch: 3 | Mean Loss: 6.481885262898037 | Epoch Loss: 3.131976048151652 | Loss: 5.890390396118164\n",
      "\n",
      "Epoch: 2 | Batch: 4 | Mean Loss: 6.319147924582164 | Epoch Loss: 2.504817306995392 | Loss: 0.6233410835266113\n",
      "\n",
      "Epoch: 2 | Batch: 5 | Mean Loss: 6.1838672644383195 | Epoch Loss: 2.2666065454483033 | Loss: 1.3137634992599487\n",
      "\n",
      "Epoch: 2 | Batch: 6 | Mean Loss: 6.1404160606233695 | Epoch Loss: 2.6442923744519553 | Loss: 4.532721519470215\n",
      "\n",
      "Epoch: 2 | Batch: 7 | Mean Loss: 6.041259487469991 | Epoch Loss: 2.5912948506219045 | Loss: 2.2733097076416016\n",
      "\n",
      "Epoch: 2 | Batch: 8 | Mean Loss: 5.900385195761919 | Epoch Loss: 2.3181689716875553 | Loss: 0.40628781914711\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Epoch: 2 | Batch: 9 | Mean Loss: 5.7860053348832015 | Epoch Loss: 2.1951291859149933 | Loss: 1.210810899734497\n",
      "\n",
      "Epoch: 2 | Batch: 10 | Mean Loss: 5.6675881040947775 | Epoch Loss: 2.056864431500435 | Loss: 0.8124816417694092\n",
      "\n",
      "Epoch: 2 | Batch: 11 | Mean Loss: 5.60806572783825 | Epoch Loss: 2.1524336581880394 | Loss: 3.108125925064087\n",
      "\n",
      "Epoch: 2 | Batch: 12 | Mean Loss: 5.52744404903867 | Epoch Loss: 2.144790175060431 | Loss: 2.0607118606567383\n",
      "\n",
      "Epoch: 2 | Batch: 13 | Mean Loss: 5.442690669827991 | Epoch Loss: 2.111617237329483 | Loss: 1.7135419845581055\n",
      "\n",
      "Epoch: 2 | Batch: 14 | Mean Loss: 5.345434236137764 | Epoch Loss: 2.0299942003829137 | Loss: 0.9688947200775146\n",
      "\n",
      "Epoch: 2 | Batch: 15 | Mean Loss: 5.314615772759661 | Epoch Loss: 2.1544590175151823 | Loss: 3.8969664573669434\n",
      "\n",
      "Epoch: 2 | Batch: 16 | Mean Loss: 5.215474688137571 | Epoch Loss: 2.054545560851693 | Loss: 0.555843710899353\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "\n",
      "Epoch:3\n",
      "\n",
      "Epoch: 3 | Batch: 1 | Mean Loss: 5.123790806045338 | Epoch Loss: 0.7229644656181335 | Loss: 0.7229644656181335\n",
      "\n",
      "Epoch: 3 | Batch: 2 | Mean Loss: 5.027274407744407 | Epoch Loss: 0.5104676783084869 | Loss: 0.29797089099884033\n",
      "\n",
      "Epoch: 3 | Batch: 3 | Mean Loss: 4.9394409253316764 | Epoch Loss: 0.5229007204373678 | Loss: 0.5477668046951294\n",
      "\n",
      "Epoch: 3 | Batch: 4 | Mean Loss: 4.85587509148396 | Epoch Loss: 0.540679931640625 | Loss: 0.5940175652503967\n",
      "\n",
      "Epoch: 3 | Batch: 5 | Mean Loss: 4.790644612514748 | Epoch Loss: 0.7122758865356446 | Loss: 1.3986597061157227\n",
      "\n",
      "Epoch: 3 | Batch: 6 | Mean Loss: 4.715320062306192 | Epoch Loss: 0.7140830556551615 | Loss: 0.7231189012527466\n",
      "\n",
      "Epoch: 3 | Batch: 7 | Mean Loss: 4.703559672290629 | Epoch Loss: 1.193285277911595 | Loss: 4.068498611450195\n",
      "\n",
      "Epoch: 3 | Batch: 8 | Mean Loss: 4.653631459921598 | Epoch Loss: 1.282572090625763 | Loss: 1.907579779624939\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Epoch: 3 | Batch: 9 | Mean Loss: 4.575980008171316 | Epoch Loss: 1.1653417150179546 | Loss: 0.22749871015548706\n",
      "\n",
      "Epoch: 3 | Batch: 10 | Mean Loss: 4.503802895032126 | Epoch Loss: 1.0877782881259919 | Loss: 0.38970744609832764\n",
      "\n",
      "Epoch: 3 | Batch: 11 | Mean Loss: 4.431939792835106 | Epoch Loss: 1.0128784315152601 | Loss: 0.2638798654079437\n",
      "\n",
      "Epoch: 3 | Batch: 12 | Mean Loss: 4.35985412026445 | Epoch Loss: 0.9373718487719694 | Loss: 0.10679943859577179\n",
      "\n",
      "Epoch: 3 | Batch: 13 | Mean Loss: 4.306830625309319 | Epoch Loss: 0.9518371625588491 | Loss: 1.1254209280014038\n",
      "\n",
      "Epoch: 3 | Batch: 14 | Mean Loss: 4.246266172057198 | Epoch Loss: 0.9232655454959188 | Loss: 0.5518345236778259\n",
      "\n",
      "Epoch: 3 | Batch: 15 | Mean Loss: 4.186945848285206 | Epoch Loss: 0.895653560757637 | Loss: 0.5090857744216919\n",
      "\n",
      "Epoch: 3 | Batch: 16 | Mean Loss: 4.148252145620063 | Epoch Loss: 0.9465845180675387 | Loss: 1.7105488777160645\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "\n",
      "Epoch:4\n",
      "\n",
      "Epoch: 4 | Batch: 1 | Mean Loss: 4.08468176733989 | Epoch Loss: 0.016177557408809662 | Loss: 0.016177557408809662\n",
      "\n",
      "Epoch: 4 | Batch: 2 | Mean Loss: 4.048362534718983 | Epoch Loss: 0.851894985884428 | Loss: 1.6876124143600464\n",
      "\n",
      "Epoch: 4 | Batch: 3 | Mean Loss: 3.996122925695199 | Epoch Loss: 0.7506995672980944 | Loss: 0.5483087301254272\n",
      "\n",
      "Epoch: 4 | Batch: 4 | Mean Loss: 3.954909837969086 | Epoch Loss: 0.8614329155534506 | Loss: 1.193632960319519\n",
      "\n",
      "Epoch: 4 | Batch: 5 | Mean Loss: 3.9314212225701497 | Epoch Loss: 1.1559854075312614 | Loss: 2.334195375442505\n",
      "\n",
      "Epoch: 4 | Batch: 6 | Mean Loss: 3.8758394609604565 | Epoch Loss: 0.9701041579246521 | Loss: 0.04069790989160538\n",
      "\n",
      "Epoch: 4 | Batch: 7 | Mean Loss: 3.8486174825631396 | Epoch Loss: 1.1091005631855555 | Loss: 1.9430789947509766\n",
      "\n",
      "Epoch: 4 | Batch: 8 | Mean Loss: 3.8003506927440562 | Epoch Loss: 1.0171390697360039 | Loss: 0.37340861558914185\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Epoch: 4 | Batch: 9 | Mean Loss: 3.755813889511644 | Epoch Loss: 0.9651418460739983 | Loss: 0.5491640567779541\n",
      "\n",
      "Epoch: 4 | Batch: 10 | Mean Loss: 3.735730164961235 | Epoch Loss: 1.0955894887447357 | Loss: 2.269618272781372\n",
      "\n",
      "Epoch: 4 | Batch: 11 | Mean Loss: 3.7159371759494144 | Epoch Loss: 1.200650079683824 | Loss: 2.251255989074707\n",
      "\n",
      "Epoch: 4 | Batch: 12 | Mean Loss: 3.674361279332324 | Epoch Loss: 1.1469433257977169 | Loss: 0.5561690330505371\n",
      "\n",
      "Epoch: 4 | Batch: 13 | Mean Loss: 3.6288027188607623 | Epoch Loss: 1.0715132332765138 | Loss: 0.16635212302207947\n",
      "\n",
      "Epoch: 4 | Batch: 14 | Mean Loss: 3.5831611078137007 | Epoch Loss: 0.999887792127473 | Loss: 0.0687570571899414\n",
      "\n",
      "Epoch: 4 | Batch: 15 | Mean Loss: 3.546513160384154 | Epoch Loss: 0.9790934900442759 | Loss: 0.6879732608795166\n",
      "\n",
      "Epoch: 4 | Batch: 16 | Mean Loss: 3.506889776326716 | Epoch Loss: 0.9414402991533279 | Loss: 0.3766424357891083\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "\n",
      "Epoch:5\n",
      "\n",
      "Epoch: 5 | Batch: 1 | Mean Loss: 3.4804350088040032 | Epoch Loss: 1.3640536069869995 | Loss: 1.3640536069869995\n",
      "\n",
      "Epoch: 5 | Batch: 2 | Mean Loss: 3.4392707515053633 | Epoch Loss: 0.7345097586512566 | Loss: 0.10496591031551361\n",
      "\n",
      "Epoch: 5 | Batch: 3 | Mean Loss: 3.411868065954691 | Epoch Loss: 0.8779557893673579 | Loss: 1.1648478507995605\n",
      "\n",
      "Epoch: 5 | Batch: 4 | Mean Loss: 3.3749101821865355 | Epoch Loss: 0.735318299382925 | Loss: 0.30740582942962646\n",
      "\n",
      "Epoch: 5 | Batch: 5 | Mean Loss: 3.336041213046102 | Epoch Loss: 0.6024642005562783 | Loss: 0.07104780524969101\n",
      "\n",
      "Epoch: 5 | Batch: 6 | Mean Loss: 3.2996463507933673 | Epoch Loss: 0.5364006770153841 | Loss: 0.20608305931091309\n",
      "\n",
      "Epoch: 5 | Batch: 7 | Mean Loss: 3.2734762425909096 | Epoch Loss: 0.6058929998959813 | Loss: 1.0228469371795654\n",
      "\n",
      "Epoch: 5 | Batch: 8 | Mean Loss: 3.2381016154011544 | Epoch Loss: 0.5502200061455369 | Loss: 0.16050904989242554\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Epoch: 5 | Batch: 9 | Mean Loss: 3.206783703120237 | Epoch Loss: 0.5391741635070907 | Loss: 0.4508074223995209\n",
      "\n",
      "Epoch: 5 | Batch: 10 | Mean Loss: 3.1725238205658064 | Epoch Loss: 0.4975961744785309 | Loss: 0.12339427322149277\n",
      "\n",
      "Epoch: 5 | Batch: 11 | Mean Loss: 3.1460005891519587 | Epoch Loss: 0.5213519551537253 | Loss: 0.7589097619056702\n",
      "\n",
      "Epoch: 5 | Batch: 12 | Mean Loss: 3.116675257196893 | Epoch Loss: 0.515245129664739 | Loss: 0.44807004928588867\n",
      "\n",
      "Epoch: 5 | Batch: 13 | Mean Loss: 3.08599346379439 | Epoch Loss: 0.49586230974930984 | Loss: 0.26326847076416016\n",
      "\n",
      "Epoch: 5 | Batch: 14 | Mean Loss: 3.101649968706547 | Epoch Loss: 0.785993925162724 | Loss: 4.557704925537109\n",
      "\n",
      "Epoch: 5 | Batch: 15 | Mean Loss: 3.0770870128744527 | Epoch Loss: 0.7848056077957153 | Loss: 0.7681691646575928\n",
      "\n",
      "Epoch: 5 | Batch: 16 | Mean Loss: 3.050375592739632 | Epoch Loss: 0.7678046748042107 | Loss: 0.5127906799316406\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "\n",
      "Epoch:6\n",
      "\n",
      "Epoch: 6 | Batch: 1 | Mean Loss: 3.022072902198919 | Epoch Loss: 0.30501461029052734 | Loss: 0.30501461029052734\n",
      "\n",
      "Epoch: 6 | Batch: 2 | Mean Loss: 2.992249523042416 | Epoch Loss: 0.20219817757606506 | Loss: 0.09938174486160278\n",
      "\n",
      "Epoch: 6 | Batch: 3 | Mean Loss: 2.9639442525427753 | Epoch Loss: 0.19814136624336243 | Loss: 0.19002774357795715\n",
      "\n",
      "Epoch: 6 | Batch: 4 | Mean Loss: 2.9364767330884933 | Epoch Loss: 0.2029041014611721 | Loss: 0.21719230711460114\n",
      "\n",
      "Epoch: 6 | Batch: 5 | Mean Loss: 2.919635814605373 | Epoch Loss: 0.40943207442760465 | Loss: 1.235543966293335\n",
      "\n",
      "Epoch: 6 | Batch: 6 | Mean Loss: 2.896317889293035 | Epoch Loss: 0.4313946341474851 | Loss: 0.5412074327468872\n",
      "\n",
      "Epoch: 6 | Batch: 7 | Mean Loss: 2.8707036841841576 | Epoch Loss: 0.40663179542337147 | Loss: 0.2580547630786896\n",
      "\n",
      "Epoch: 6 | Batch: 8 | Mean Loss: 2.8493937377173166 | Epoch Loss: 0.4376114774495363 | Loss: 0.6544692516326904\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Epoch: 6 | Batch: 9 | Mean Loss: 2.823513214503016 | Epoch Loss: 0.4036478466457791 | Loss: 0.13193880021572113\n",
      "\n",
      "Epoch: 6 | Batch: 10 | Mean Loss: 2.7984637793786122 | Epoch Loss: 0.3801103711128235 | Loss: 0.16827309131622314\n",
      "\n",
      "Epoch: 6 | Batch: 11 | Mean Loss: 2.7729472437194573 | Epoch Loss: 0.3517543795433911 | Loss: 0.06819446384906769\n",
      "\n",
      "Epoch: 6 | Batch: 12 | Mean Loss: 2.7488618382701167 | Epoch Loss: 0.3367518025139968 | Loss: 0.17172345519065857\n",
      "\n",
      "Epoch: 6 | Batch: 13 | Mean Loss: 2.7258433110396796 | Epoch Loss: 0.32929723079387957 | Loss: 0.23984237015247345\n",
      "\n",
      "Epoch: 6 | Batch: 14 | Mean Loss: 2.7015907249328763 | Epoch Loss: 0.3099230599722692 | Loss: 0.05805883929133415\n",
      "\n",
      "Epoch: 6 | Batch: 15 | Mean Loss: 2.6788760036699943 | Epoch Loss: 0.3012786336243153 | Loss: 0.1802566647529602\n",
      "\n",
      "Epoch: 6 | Batch: 16 | Mean Loss: 2.656758598018704 | Epoch Loss: 0.2950566296931356 | Loss: 0.20172657072544098\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "\n",
      "Epoch:7\n",
      "\n",
      "Epoch: 7 | Batch: 1 | Mean Loss: 2.633929195756142 | Epoch Loss: 0.07703614234924316 | Loss: 0.07703614234924316\n",
      "\n",
      "Epoch: 7 | Batch: 2 | Mean Loss: 2.615469732403494 | Epoch Loss: 0.30329325795173645 | Loss: 0.5295503735542297\n",
      "\n",
      "Epoch: 7 | Batch: 3 | Mean Loss: 2.593165638880885 | Epoch Loss: 0.21902849773565927 | Loss: 0.050498977303504944\n",
      "\n",
      "Epoch: 7 | Batch: 4 | Mean Loss: 2.5762213116254786 | Epoch Loss: 0.32117729261517525 | Loss: 0.6276236772537231\n",
      "\n",
      "Epoch: 7 | Batch: 5 | Mean Loss: 2.554568734083675 | Epoch Loss: 0.2655157819390297 | Loss: 0.04286973923444748\n",
      "\n",
      "Epoch: 7 | Batch: 6 | Mean Loss: 2.533135058737155 | Epoch Loss: 0.22549565881490707 | Loss: 0.025395043194293976\n",
      "\n",
      "Epoch: 7 | Batch: 7 | Mean Loss: 2.5228261869682482 | Epoch Loss: 0.37990761016096386 | Loss: 1.3063793182373047\n",
      "\n",
      "Epoch: 7 | Batch: 8 | Mean Loss: 2.503108608815819 | Epoch Loss: 0.35200875997543335 | Loss: 0.15671680867671967\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Epoch: 7 | Batch: 9 | Mean Loss: 2.4826607804222047 | Epoch Loss: 0.31611016144355136 | Loss: 0.028921373188495636\n",
      "\n",
      "Epoch: 7 | Batch: 10 | Mean Loss: 2.4642577639429786 | Epoch Loss: 0.3082484222948551 | Loss: 0.23749276995658875\n",
      "\n",
      "Epoch: 7 | Batch: 11 | Mean Loss: 2.4447236837349773 | Epoch Loss: 0.2858227383006703 | Loss: 0.06156589835882187\n",
      "\n",
      "Epoch: 7 | Batch: 12 | Mean Loss: 2.4252994622915023 | Epoch Loss: 0.2650141955042879 | Loss: 0.0361202247440815\n",
      "\n",
      "Epoch: 7 | Batch: 13 | Mean Loss: 2.4066659721732138 | Epoch Loss: 0.25202181104284066 | Loss: 0.09611319750547409\n",
      "\n",
      "Epoch: 7 | Batch: 14 | Mean Loss: 2.39082113564724 | Epoch Loss: 0.26332143667553154 | Loss: 0.4102165699005127\n",
      "\n",
      "Epoch: 7 | Batch: 15 | Mean Loss: 2.375537608024173 | Epoch Loss: 0.27575421606500944 | Loss: 0.4498131275177002\n",
      "\n",
      "Epoch: 7 | Batch: 16 | Mean Loss: 2.3579033141140826 | Epoch Loss: 0.2659163267817348 | Loss: 0.11834798753261566\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "\n",
      "Epoch:8\n",
      "\n",
      "Epoch: 8 | Batch: 1 | Mean Loss: 2.3398315089415442 | Epoch Loss: 0.026640446856617928 | Loss: 0.026640446856617928\n",
      "\n",
      "Epoch: 8 | Batch: 2 | Mean Loss: 2.3226265790514073 | Epoch Loss: 0.06491553504019976 | Loss: 0.10319062322378159\n",
      "\n",
      "Epoch: 8 | Batch: 3 | Mean Loss: 2.311945580585881 | Epoch Loss: 0.35108228338261444 | Loss: 0.9234157800674438\n",
      "\n",
      "Epoch: 8 | Batch: 4 | Mean Loss: 2.2946930315035083 | Epoch Loss: 0.27196398796513677 | Loss: 0.034609101712703705\n",
      "\n",
      "Epoch: 8 | Batch: 5 | Mean Loss: 2.2779199357347606 | Epoch Loss: 0.23034544922411443 | Loss: 0.06387129426002502\n",
      "\n",
      "Epoch: 8 | Batch: 6 | Mean Loss: 2.2610977730616484 | Epoch Loss: 0.19591289727638164 | Loss: 0.02375013753771782\n",
      "\n",
      "Epoch: 8 | Batch: 7 | Mean Loss: 2.2490101838967314 | Epoch Loss: 0.2578215170651674 | Loss: 0.6292732357978821\n",
      "\n",
      "Epoch: 8 | Batch: 8 | Mean Loss: 2.2369479701772113 | Epoch Loss: 0.3016624671872705 | Loss: 0.6085491180419922\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Epoch: 8 | Batch: 9 | Mean Loss: 2.22108413049285 | Epoch Loss: 0.2752112967686521 | Loss: 0.06360193341970444\n",
      "\n",
      "Epoch: 8 | Batch: 10 | Mean Loss: 2.2053190367566287 | Epoch Loss: 0.2522402865812182 | Loss: 0.04550119489431381\n",
      "\n",
      "Epoch: 8 | Batch: 11 | Mean Loss: 2.1899330034825226 | Epoch Loss: 0.23536938886073502 | Loss: 0.06666041165590286\n",
      "\n",
      "Epoch: 8 | Batch: 12 | Mean Loss: 2.174465081415006 | Epoch Loss: 0.2177905992915233 | Loss: 0.024423914030194283\n",
      "\n",
      "Epoch: 8 | Batch: 13 | Mean Loss: 2.159390242281535 | Epoch Loss: 0.2047999965456816 | Loss: 0.048912763595581055\n",
      "\n",
      "Epoch: 8 | Batch: 14 | Mean Loss: 2.144516963564174 | Epoch Loss: 0.1935560442507267 | Loss: 0.04738466441631317\n",
      "\n",
      "Epoch: 8 | Batch: 15 | Mean Loss: 2.1306612386361703 | Epoch Loss: 0.19152886122465135 | Loss: 0.16314829885959625\n",
      "\n",
      "Epoch: 8 | Batch: 16 | Mean Loss: 2.1188851041305394 | Epoch Loss: 0.20673942426219583 | Loss: 0.43489786982536316\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "\n",
      "Epoch:9\n",
      "\n",
      "Epoch: 9 | Batch: 1 | Mean Loss: 2.104342090299931 | Epoch Loss: 0.010148098692297935 | Loss: 0.010148098692297935\n",
      "\n",
      "Epoch: 9 | Batch: 2 | Mean Loss: 2.091566233742625 | Epoch Loss: 0.12460756581276655 | Loss: 0.23906703293323517\n",
      "\n",
      "Epoch: 9 | Batch: 3 | Mean Loss: 2.0777669101826794 | Epoch Loss: 0.10409360068539779 | Loss: 0.06306567043066025\n",
      "\n",
      "Epoch: 9 | Batch: 4 | Mean Loss: 2.0643014311161196 | Epoch Loss: 0.09928920259699225 | Loss: 0.08487600833177567\n",
      "\n",
      "Epoch: 9 | Batch: 5 | Mean Loss: 2.0505820721162844 | Epoch Loss: 0.08345475010573863 | Loss: 0.020116940140724182\n",
      "\n",
      "Epoch: 9 | Batch: 6 | Mean Loss: 2.0439215461785594 | Epoch Loss: 0.24479615533103546 | Loss: 1.0515031814575195\n",
      "\n",
      "Epoch: 9 | Batch: 7 | Mean Loss: 2.030504079239554 | Epoch Loss: 0.2123801386249917 | Loss: 0.017884038388729095\n",
      "\n",
      "Epoch: 9 | Batch: 8 | Mean Loss: 2.0172270166038193 | Epoch Loss: 0.18738144112285227 | Loss: 0.0123905586078763\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Epoch: 9 | Batch: 9 | Mean Loss: 2.0044948544073242 | Epoch Loss: 0.174250858835876 | Loss: 0.06920620054006577\n",
      "\n",
      "Epoch: 9 | Batch: 10 | Mean Loss: 1.9917547840854855 | Epoch Loss: 0.16107817543670536 | Loss: 0.04252402484416962\n",
      "\n",
      "Epoch: 9 | Batch: 11 | Mean Loss: 1.980868234239038 | Epoch Loss: 0.17410193747756156 | Loss: 0.30433955788612366\n",
      "\n",
      "Epoch: 9 | Batch: 12 | Mean Loss: 1.9682218184073765 | Epoch Loss: 0.16026238972942033 | Loss: 0.008027364499866962\n",
      "\n",
      "Epoch: 9 | Batch: 13 | Mean Loss: 1.9559893852728567 | Epoch Loss: 0.15160603792621538 | Loss: 0.047729816287755966\n",
      "\n",
      "Epoch: 9 | Batch: 14 | Mean Loss: 1.9446864271229958 | Epoch Loss: 0.15292860647397383 | Loss: 0.17012199759483337\n",
      "\n",
      "Epoch: 9 | Batch: 15 | Mean Loss: 1.9327766957850951 | Epoch Loss: 0.1461359756688277 | Loss: 0.05103914439678192\n",
      "\n",
      "Epoch: 9 | Batch: 16 | Mean Loss: 1.9223599157063291 | Epoch Loss: 0.15363321988843381 | Loss: 0.26609188318252563\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Training finished in 532.6636378765106 seconds!\n"
     ]
    }
   ],
   "source": [
    "train_loop(\n",
    "    model=model,\n",
    "    mode=\"train\",\n",
    "    model_path=\"/home/snajafi/checkpoints/t5-base\",\n",
    "    metric_to_save=\"qa_score\",\n",
    "    max_epochs=10,\n",
    "    training_steps=100000,  # not important\n",
    "    steps_per_checkpoint=8,\n",
    "    metric=qa_metric,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=dev_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Prediction Step: 9.\n",
      "Prediction Step: 10.\n",
      "Prediction Step: 11.\n",
      "Prediction Step: 12.\n",
      "Prediction Step: 13.\n",
      "Prediction Step: 14.\n",
      "Prediction Step: 15.\n",
      "Prediction Step: 16.\n",
      "Prediction Step: 17.\n",
      "Prediction Step: 18.\n",
      "Prediction Step: 19.\n",
      "Prediction Step: 20.\n",
      "Prediction Step: 21.\n",
      "Prediction Step: 22.\n",
      "Prediction Step: 23.\n",
      "Prediction Step: 24.\n",
      "Prediction Step: 25.\n",
      "Prediction Step: 26.\n",
      "Prediction Step: 27.\n",
      "Prediction Step: 28.\n",
      "Prediction Step: 29.\n",
      "Prediction Step: 30.\n",
      "Prediction Step: 31.\n",
      "Prediction Step: 32.\n",
      "Prediction Step: 33.\n",
      "Prediction Step: 34.\n",
      "Prediction Step: 35.\n",
      "Prediction Step: 36.\n",
      "Prediction Step: 37.\n",
      "Prediction Step: 38.\n",
      "Prediction Step: 39.\n",
      "Prediction Step: 40.\n",
      "Prediction Step: 41.\n",
      "Prediction Step: 42.\n",
      "Prediction Step: 43.\n",
      "Prediction Step: 44.\n",
      "Prediction Step: 45.\n",
      "Prediction Step: 46.\n",
      "Prediction Step: 47.\n",
      "Prediction Step: 48.\n",
      "Prediction Step: 49.\n",
      "Prediction Step: 50.\n",
      "Prediction Step: 51.\n",
      "Prediction Step: 52.\n",
      "Prediction Step: 53.\n",
      "Prediction Step: 54.\n",
      "Prediction Step: 55.\n",
      "Prediction Step: 56.\n",
      "Prediction Step: 57.\n",
      "Prediction Step: 58.\n",
      "Prediction Step: 59.\n",
      "Prediction Step: 60.\n",
      "Prediction Step: 61.\n",
      "Prediction Step: 62.\n",
      "Prediction Step: 63.\n",
      "Prediction Step: 64.\n",
      "Prediction Step: 65.\n",
      "Prediction Step: 66.\n",
      "Prediction Step: 67.\n",
      "Prediction Step: 68.\n",
      "Prediction Step: 69.\n",
      "Prediction Step: 70.\n",
      "Prediction Step: 71.\n",
      "Prediction Step: 72.\n",
      "Prediction Step: 73.\n",
      "Prediction Step: 74.\n",
      "Prediction Step: 75.\n",
      "Prediction Step: 76.\n",
      "Prediction Step: 77.\n",
      "Prediction Step: 78.\n",
      "Prediction Step: 79.\n",
      "Prediction Step: 80.\n",
      "Prediction Step: 81.\n",
      "Prediction Step: 82.\n",
      "Prediction Step: 83.\n",
      "Prediction Step: 84.\n",
      "Prediction Step: 85.\n",
      "Prediction Step: 86.\n",
      "Prediction Step: 87.\n",
      "Prediction Step: 88.\n",
      "Prediction Step: 89.\n",
      "Prediction Step: 90.\n",
      "Prediction Step: 91.\n",
      "Prediction Step: 92.\n",
      "Prediction Step: 93.\n",
      "Prediction Step: 94.\n",
      "Prediction Step: 95.\n",
      "Prediction Step: 96.\n",
      "Prediction Step: 97.\n",
      "Prediction Step: 98.\n",
      "Prediction Step: 99.\n",
      "Prediction Step: 100.\n",
      "Prediction Step: 101.\n",
      "Prediction Step: 102.\n",
      "Prediction Step: 103.\n",
      "Prediction Step: 104.\n",
      "Prediction Step: 105.\n",
      "Prediction Step: 106.\n",
      "Prediction Step: 107.\n",
      "Prediction Step: 108.\n",
      "Prediction Step: 109.\n",
      "Prediction Step: 110.\n",
      "Prediction Step: 111.\n",
      "Prediction Step: 112.\n",
      "Prediction Step: 113.\n",
      "Prediction Step: 114.\n",
      "Prediction Step: 115.\n",
      "Prediction Step: 116.\n",
      "Prediction Step: 117.\n",
      "Prediction Step: 118.\n",
      "Prediction Step: 119.\n",
      "Prediction Step: 120.\n",
      "Prediction Step: 121.\n",
      "Prediction Step: 122.\n",
      "Prediction Step: 123.\n",
      "Prediction Step: 124.\n",
      "Prediction Step: 125.\n",
      "Prediction Step: 126.\n",
      "Prediction Step: 127.\n",
      "Prediction Step: 128.\n",
      "Prediction Step: 129.\n",
      "Prediction Step: 130.\n",
      "Prediction Step: 131.\n",
      "Prediction Step: 132.\n",
      "Prediction Step: 133.\n",
      "Prediction Step: 134.\n",
      "Prediction Step: 135.\n",
      "Prediction Step: 136.\n",
      "Prediction Step: 137.\n",
      "Prediction Step: 138.\n",
      "Prediction Step: 139.\n",
      "Prediction Step: 140.\n",
      "Prediction Step: 141.\n",
      "Prediction Step: 142.\n",
      "Prediction Step: 143.\n",
      "Prediction Step: 144.\n",
      "Prediction Step: 145.\n",
      "Prediction Step: 146.\n",
      "Prediction Step: 147.\n",
      "Prediction Step: 148.\n",
      "Prediction Step: 149.\n",
      "Prediction Step: 150.\n",
      "Prediction Step: 151.\n",
      "Prediction Step: 152.\n",
      "Prediction Step: 153.\n",
      "Prediction Step: 154.\n",
      "Prediction Step: 155.\n",
      "Prediction Step: 156.\n",
      "Prediction Step: 157.\n",
      "Prediction Step: 158.\n",
      "Prediction Step: 159.\n",
      "Prediction Step: 160.\n",
      "Prediction Step: 161.\n",
      "Prediction Step: 162.\n",
      "Prediction Step: 163.\n",
      "Prediction Step: 164.\n",
      "Prediction Step: 165.\n",
      "Prediction Step: 166.\n",
      "Prediction Step: 167.\n",
      "Prediction Step: 168.\n",
      "Prediction Step: 169.\n",
      "Prediction Step: 170.\n",
      "Prediction Step: 171.\n",
      "Prediction Step: 172.\n",
      "Prediction Step: 173.\n",
      "Prediction Step: 174.\n",
      "Prediction Step: 175.\n",
      "Prediction Step: 176.\n",
      "Prediction Step: 177.\n",
      "Prediction Step: 178.\n",
      "Prediction Step: 179.\n",
      "Prediction Step: 180.\n",
      "Prediction Step: 181.\n",
      "Prediction Step: 182.\n",
      "Prediction Step: 183.\n",
      "Prediction Step: 184.\n",
      "Prediction Step: 185.\n",
      "Prediction Step: 186.\n",
      "Prediction Step: 187.\n",
      "Prediction Step: 188.\n",
      "Prediction Step: 189.\n",
      "Prediction Step: 190.\n",
      "Prediction Step: 191.\n",
      "Prediction Step: 192.\n",
      "Prediction Step: 193.\n",
      "Prediction Step: 194.\n",
      "Prediction Step: 195.\n",
      "Prediction Step: 196.\n",
      "Prediction Step: 197.\n",
      "Prediction Step: 198.\n",
      "Prediction Step: 199.\n",
      "Prediction Step: 200.\n",
      "Prediction Step: 201.\n",
      "Prediction Step: 202.\n",
      "Prediction Step: 203.\n",
      "Prediction Step: 204.\n",
      "Prediction Step: 205.\n",
      "Prediction Step: 206.\n",
      "Prediction Step: 207.\n",
      "Prediction Step: 208.\n",
      "Prediction Step: 209.\n",
      "Prediction Step: 210.\n",
      "Prediction Step: 211.\n",
      "Prediction Step: 212.\n",
      "Prediction Step: 213.\n",
      "Prediction Step: 214.\n",
      "Prediction Step: 215.\n",
      "Prediction Step: 216.\n",
      "Prediction Step: 217.\n",
      "Prediction Step: 218.\n",
      "Prediction Step: 219.\n",
      "Prediction Step: 220.\n",
      "Prediction Step: 221.\n",
      "Prediction Step: 222.\n",
      "Prediction Step: 223.\n",
      "Prediction Step: 224.\n",
      "Prediction Step: 225.\n",
      "Prediction Step: 226.\n",
      "Prediction Step: 227.\n",
      "Prediction Step: 228.\n",
      "Prediction Step: 229.\n",
      "Prediction Step: 230.\n",
      "Prediction Step: 231.\n",
      "Prediction Step: 232.\n",
      "Prediction Step: 233.\n",
      "Prediction Step: 234.\n",
      "Prediction Step: 235.\n",
      "Prediction Step: 236.\n",
      "Prediction Step: 237.\n",
      "Prediction Step: 238.\n",
      "Prediction Step: 239.\n",
      "Prediction Step: 240.\n",
      "Prediction Step: 241.\n",
      "Prediction Step: 242.\n",
      "Prediction Step: 243.\n",
      "Prediction Step: 244.\n",
      "Prediction Step: 245.\n",
      "Prediction Step: 246.\n",
      "Prediction Step: 247.\n",
      "Prediction Step: 248.\n",
      "Prediction Step: 249.\n",
      "Prediction Step: 250.\n",
      "Prediction Step: 251.\n",
      "Prediction Step: 252.\n",
      "Prediction Step: 253.\n",
      "Prediction Step: 254.\n",
      "Prediction Step: 255.\n",
      "Prediction Step: 256.\n",
      "Prediction Step: 257.\n",
      "Prediction Step: 258.\n",
      "Prediction Step: 259.\n",
      "Prediction Step: 260.\n",
      "Prediction Step: 261.\n",
      "Prediction Step: 262.\n",
      "Prediction Step: 263.\n",
      "Prediction Step: 264.\n",
      "Prediction Step: 265.\n",
      "Prediction Step: 266.\n",
      "Prediction Step: 267.\n",
      "Prediction Step: 268.\n",
      "Prediction Step: 269.\n",
      "Prediction Step: 270.\n",
      "Prediction Step: 271.\n",
      "Prediction Step: 272.\n",
      "Prediction Step: 273.\n",
      "Prediction Step: 274.\n",
      "Prediction Step: 275.\n",
      "Prediction Step: 276.\n",
      "Prediction Step: 277.\n",
      "Prediction Step: 278.\n",
      "Prediction Step: 279.\n",
      "Prediction Step: 280.\n",
      "Prediction Step: 281.\n",
      "Prediction Step: 282.\n",
      "Prediction Step: 283.\n",
      "Prediction Step: 284.\n",
      "Prediction Step: 285.\n",
      "Prediction Step: 286.\n",
      "Prediction Step: 287.\n",
      "Prediction Step: 288.\n",
      "Prediction Step: 289.\n",
      "Prediction Step: 290.\n",
      "Prediction Step: 291.\n",
      "Prediction Step: 292.\n",
      "Prediction Step: 293.\n",
      "Prediction Step: 294.\n",
      "Prediction Step: 295.\n",
      "Prediction Step: 296.\n",
      "Prediction Step: 297.\n",
      "Prediction Step: 298.\n",
      "Prediction Step: 299.\n",
      "Prediction Step: 300.\n",
      "Prediction Step: 301.\n",
      "Prediction Step: 302.\n",
      "Prediction Step: 303.\n",
      "Prediction Step: 304.\n",
      "Prediction Step: 305.\n",
      "Prediction Step: 306.\n",
      "Prediction Step: 307.\n",
      "Prediction Step: 308.\n",
      "Prediction Step: 309.\n",
      "Prediction Step: 310.\n",
      "Prediction Step: 311.\n",
      "Prediction Step: 312.\n",
      "Prediction Step: 313.\n",
      "Prediction Step: 314.\n",
      "Prediction Step: 315.\n",
      "Prediction Step: 316.\n",
      "Prediction Step: 317.\n",
      "Prediction Step: 318.\n",
      "Prediction Step: 319.\n",
      "Prediction Step: 320.\n",
      "Prediction Step: 321.\n",
      "Prediction Step: 322.\n",
      "Prediction Step: 323.\n",
      "Prediction Step: 324.\n",
      "Prediction Step: 325.\n",
      "Prediction Step: 326.\n",
      "Prediction Step: 327.\n",
      "Prediction Step: 328.\n",
      "Prediction Step: 329.\n",
      "Prediction Step: 330.\n",
      "Prediction Step: 331.\n",
      "Prediction Step: 332.\n",
      "Prediction Step: 333.\n",
      "Prediction Step: 334.\n",
      "Prediction Step: 335.\n",
      "Prediction Step: 336.\n",
      "Prediction Step: 337.\n",
      "Prediction Step: 338.\n",
      "Prediction Step: 339.\n",
      "Prediction Step: 340.\n",
      "Prediction Step: 341.\n",
      "Prediction Step: 342.\n",
      "Prediction Step: 343.\n",
      "Prediction Step: 344.\n",
      "Prediction Step: 345.\n",
      "Prediction Step: 346.\n",
      "Prediction Step: 347.\n",
      "Prediction Step: 348.\n",
      "Prediction Step: 349.\n",
      "Prediction Step: 350.\n",
      "Prediction Step: 351.\n",
      "Prediction Step: 352.\n",
      "Prediction Step: 353.\n",
      "Prediction Step: 354.\n",
      "Prediction Step: 355.\n",
      "Prediction Step: 356.\n",
      "Prediction Step: 357.\n",
      "Prediction Step: 358.\n",
      "Prediction Step: 359.\n",
      "Prediction Step: 360.\n",
      "Prediction Step: 361.\n",
      "Prediction Step: 362.\n",
      "Prediction Step: 363.\n",
      "Prediction Step: 364.\n",
      "Prediction Step: 365.\n",
      "Prediction Step: 366.\n",
      "Prediction Step: 367.\n",
      "Prediction Step: 368.\n",
      "Prediction Step: 369.\n",
      "Prediction Step: 370.\n",
      "Prediction Step: 371.\n",
      "Prediction Step: 372.\n",
      "Prediction Step: 373.\n",
      "Prediction Step: 374.\n",
      "Prediction Step: 375.\n",
      "Prediction Step: 376.\n",
      "Prediction Step: 377.\n",
      "Prediction Step: 378.\n",
      "Prediction Step: 379.\n",
      "Prediction Step: 380.\n",
      "Prediction Step: 381.\n",
      "Prediction Step: 382.\n",
      "Prediction Step: 383.\n",
      "Prediction Step: 384.\n",
      "Prediction Step: 385.\n",
      "Prediction Step: 386.\n",
      "Prediction Step: 387.\n",
      "Prediction Step: 388.\n",
      "Prediction Step: 389.\n",
      "Prediction Step: 390.\n",
      "Prediction Step: 391.\n",
      "Prediction Step: 392.\n",
      "Prediction Step: 393.\n",
      "Prediction Step: 394.\n",
      "Prediction Step: 395.\n",
      "Prediction Step: 396.\n",
      "Prediction Step: 397.\n",
      "Prediction Step: 398.\n",
      "Prediction Step: 399.\n",
      "Prediction Step: 400.\n",
      "Prediction Step: 401.\n",
      "Prediction Step: 402.\n",
      "Prediction Step: 403.\n",
      "Prediction Step: 404.\n",
      "Prediction Step: 405.\n",
      "Prediction Step: 406.\n",
      "Prediction Step: 407.\n",
      "Prediction Step: 408.\n",
      "Prediction Step: 409.\n",
      "Prediction Step: 410.\n",
      "Prediction Step: 411.\n",
      "Prediction Step: 412.\n",
      "Prediction Step: 413.\n",
      "Prediction Step: 414.\n",
      "Prediction Step: 415.\n",
      "Prediction Step: 416.\n",
      "Prediction Step: 417.\n",
      "Prediction Step: 418.\n",
      "Prediction Step: 419.\n",
      "Prediction Step: 420.\n",
      "Prediction Step: 421.\n",
      "Prediction Step: 422.\n",
      "Prediction Step: 423.\n",
      "Prediction Step: 424.\n",
      "Prediction Step: 425.\n",
      "Prediction Step: 426.\n",
      "Prediction Step: 427.\n",
      "Prediction Step: 428.\n",
      "Prediction Step: 429.\n",
      "Prediction Step: 430.\n",
      "Prediction Step: 431.\n",
      "Prediction Step: 432.\n",
      "Prediction Step: 433.\n",
      "Prediction Step: 434.\n",
      "Prediction Step: 435.\n",
      "Prediction Step: 436.\n",
      "Prediction Step: 437.\n",
      "Prediction Step: 438.\n",
      "Prediction Step: 439.\n",
      "Prediction Step: 440.\n",
      "Prediction Step: 441.\n",
      "Prediction Step: 442.\n",
      "Prediction Step: 443.\n",
      "Prediction Step: 444.\n",
      "Prediction Step: 445.\n",
      "Prediction Step: 446.\n",
      "Prediction Step: 447.\n",
      "Prediction Step: 448.\n",
      "Prediction Step: 449.\n",
      "Prediction Step: 450.\n",
      "Prediction Step: 451.\n",
      "Prediction Step: 452.\n",
      "Prediction Step: 453.\n",
      "Prediction Step: 454.\n",
      "Prediction Step: 455.\n",
      "Prediction Step: 456.\n",
      "Prediction Step: 457.\n",
      "Prediction Step: 458.\n",
      "Prediction Step: 459.\n",
      "Prediction Step: 460.\n",
      "Prediction Step: 461.\n",
      "Prediction Step: 462.\n",
      "Prediction Step: 463.\n",
      "Prediction Step: 464.\n",
      "Prediction Step: 465.\n",
      "Prediction Step: 466.\n",
      "Prediction Step: 467.\n",
      "Prediction Step: 468.\n",
      "Prediction Step: 469.\n",
      "Prediction Step: 470.\n",
      "Prediction Step: 471.\n",
      "Prediction Step: 472.\n",
      "Prediction Step: 473.\n",
      "Prediction Step: 474.\n",
      "Prediction Step: 475.\n",
      "Prediction Step: 476.\n",
      "Prediction Step: 477.\n",
      "Prediction Step: 478.\n",
      "Prediction Step: 479.\n",
      "Prediction Step: 480.\n",
      "Prediction Step: 481.\n",
      "Prediction Step: 482.\n",
      "Prediction Step: 483.\n",
      "Prediction Step: 484.\n",
      "Prediction Step: 485.\n",
      "Prediction Step: 486.\n",
      "Prediction Step: 487.\n",
      "Prediction Step: 488.\n",
      "Prediction Step: 489.\n",
      "Prediction Step: 490.\n",
      "Prediction Step: 491.\n",
      "Prediction Step: 492.\n",
      "Prediction Step: 493.\n",
      "Prediction Step: 494.\n",
      "Prediction Step: 495.\n",
      "Prediction Step: 496.\n",
      "Prediction Step: 497.\n",
      "Prediction Step: 498.\n",
      "Prediction Step: 499.\n",
      "Prediction Step: 500.\n",
      "Prediction Step: 501.\n",
      "Prediction Step: 502.\n",
      "Prediction Step: 503.\n",
      "Prediction Step: 504.\n",
      "Prediction Step: 505.\n",
      "Prediction Step: 506.\n",
      "Prediction Step: 507.\n",
      "Prediction Step: 508.\n",
      "Prediction Step: 509.\n",
      "Prediction Step: 510.\n",
      "Prediction Step: 511.\n",
      "Prediction Step: 512.\n",
      "Prediction Step: 513.\n",
      "Prediction Step: 514.\n",
      "Prediction Step: 515.\n",
      "Prediction Step: 516.\n",
      "Prediction Step: 517.\n",
      "Prediction Step: 518.\n",
      "Prediction Step: 519.\n",
      "Prediction Step: 520.\n",
      "Prediction Step: 521.\n",
      "Prediction Step: 522.\n",
      "Prediction Step: 523.\n",
      "Prediction Step: 524.\n",
      "Prediction Step: 525.\n",
      "Prediction Step: 526.\n",
      "Prediction Step: 527.\n",
      "Prediction Step: 528.\n",
      "Prediction Step: 529.\n",
      "Prediction Step: 530.\n",
      "Prediction Step: 531.\n",
      "Prediction Step: 532.\n",
      "Prediction Step: 533.\n",
      "Prediction Step: 534.\n",
      "Prediction Step: 535.\n",
      "Prediction Step: 536.\n",
      "Prediction Step: 537.\n",
      "Prediction Step: 538.\n",
      "Prediction Step: 539.\n",
      "Prediction Step: 540.\n",
      "Prediction Step: 541.\n",
      "Prediction Step: 542.\n",
      "Prediction Step: 543.\n",
      "Prediction Step: 544.\n",
      "Prediction Step: 545.\n",
      "Prediction Step: 546.\n",
      "Prediction Step: 547.\n",
      "Prediction Step: 548.\n",
      "Prediction Step: 549.\n",
      "Prediction Step: 550.\n",
      "Prediction Step: 551.\n",
      "Prediction Step: 552.\n",
      "Prediction Step: 553.\n",
      "Prediction Step: 554.\n",
      "Prediction Step: 555.\n",
      "Prediction Step: 556.\n",
      "Prediction Step: 557.\n",
      "Prediction Step: 558.\n",
      "Prediction Step: 559.\n",
      "Prediction Step: 560.\n",
      "Prediction Step: 561.\n",
      "Prediction Step: 562.\n",
      "Prediction Step: 563.\n",
      "Prediction Step: 564.\n",
      "Prediction Step: 565.\n",
      "Prediction Step: 566.\n",
      "Prediction Step: 567.\n",
      "Prediction Step: 568.\n",
      "Prediction Step: 569.\n",
      "Prediction Step: 570.\n",
      "Prediction Step: 571.\n",
      "Prediction Step: 572.\n",
      "Prediction Step: 573.\n",
      "Prediction Step: 574.\n",
      "Prediction Step: 575.\n",
      "Prediction Step: 576.\n",
      "Prediction Step: 577.\n",
      "Prediction Step: 578.\n",
      "Prediction Step: 579.\n",
      "Prediction Step: 580.\n",
      "Prediction Step: 581.\n",
      "Prediction Step: 582.\n",
      "Prediction Step: 583.\n",
      "Prediction Step: 584.\n",
      "Prediction Step: 585.\n",
      "Prediction Step: 586.\n",
      "Prediction Step: 587.\n",
      "Prediction Step: 588.\n",
      "Prediction Step: 589.\n",
      "Prediction Step: 590.\n",
      "Prediction Step: 591.\n",
      "Prediction Step: 592.\n",
      "Prediction Step: 593.\n",
      "Prediction Step: 594.\n",
      "Prediction Step: 595.\n",
      "Prediction Step: 596.\n",
      "Prediction Step: 597.\n",
      "Prediction Step: 598.\n",
      "Prediction Step: 599.\n",
      "Prediction Step: 600.\n",
      "Prediction Step: 601.\n",
      "Prediction Step: 602.\n",
      "Prediction Step: 603.\n",
      "Prediction Step: 604.\n",
      "Prediction Step: 605.\n",
      "Prediction Step: 606.\n",
      "Prediction Step: 607.\n",
      "Prediction Step: 608.\n",
      "Prediction Step: 609.\n",
      "Prediction Step: 610.\n",
      "Prediction Step: 611.\n",
      "Prediction Step: 612.\n",
      "Prediction Step: 613.\n",
      "Prediction Step: 614.\n",
      "Prediction Step: 615.\n",
      "Prediction Step: 616.\n",
      "Prediction Step: 617.\n",
      "Prediction Step: 618.\n",
      "Prediction Step: 619.\n",
      "Prediction Step: 620.\n",
      "Prediction Step: 621.\n",
      "Prediction Step: 622.\n",
      "Prediction Step: 623.\n",
      "Prediction Step: 624.\n",
      "Prediction Step: 625.\n",
      "Prediction Step: 626.\n",
      "Prediction Step: 627.\n",
      "Prediction Step: 628.\n",
      "Prediction Step: 629.\n",
      "Prediction Step: 630.\n",
      "Prediction Step: 631.\n",
      "Prediction Step: 632.\n",
      "Prediction Step: 633.\n",
      "Prediction Step: 634.\n",
      "Prediction Step: 635.\n",
      "Prediction Step: 636.\n",
      "Prediction Step: 637.\n",
      "Prediction Step: 638.\n",
      "Prediction Step: 639.\n",
      "Prediction Step: 640.\n",
      "Prediction Step: 641.\n",
      "Prediction Step: 642.\n",
      "Prediction Step: 643.\n",
      "Prediction Step: 644.\n",
      "Prediction Step: 645.\n",
      "Prediction Step: 646.\n",
      "Prediction Step: 647.\n",
      "Prediction Step: 648.\n",
      "Prediction Step: 649.\n",
      "Prediction Step: 650.\n",
      "Prediction Step: 651.\n",
      "Prediction Step: 652.\n",
      "Prediction Step: 653.\n",
      "Prediction Step: 654.\n",
      "Prediction Step: 655.\n",
      "Prediction Step: 656.\n",
      "Prediction Step: 657.\n",
      "Prediction Step: 658.\n",
      "Prediction Step: 659.\n",
      "Prediction Step: 660.\n",
      "Prediction Step: 661.\n",
      "Prediction Step: 662.\n",
      "Prediction Step: 663.\n",
      "Prediction Step: 664.\n",
      "Prediction Step: 665.\n",
      "Prediction Step: 666.\n",
      "Prediction Step: 667.\n",
      "Prediction Step: 668.\n",
      "Prediction Step: 669.\n",
      "Prediction Step: 670.\n",
      "Prediction Step: 671.\n",
      "Prediction Step: 672.\n",
      "Prediction Step: 673.\n",
      "Prediction Step: 674.\n",
      "Prediction Step: 675.\n",
      "Prediction Step: 676.\n",
      "Prediction Step: 677.\n",
      "Prediction Step: 678.\n",
      "Prediction Step: 679.\n",
      "Prediction Step: 680.\n",
      "Prediction Step: 681.\n",
      "Prediction Step: 682.\n",
      "Prediction Step: 683.\n",
      "Prediction Step: 684.\n",
      "Prediction Step: 685.\n",
      "Prediction Step: 686.\n",
      "Prediction Step: 687.\n",
      "Prediction Step: 688.\n",
      "Prediction Step: 689.\n",
      "Prediction Step: 690.\n",
      "Prediction Step: 691.\n",
      "Prediction Step: 692.\n",
      "Prediction Step: 693.\n",
      "Prediction Step: 694.\n",
      "Prediction Step: 695.\n",
      "Prediction Step: 696.\n",
      "Prediction Step: 697.\n",
      "Prediction Step: 698.\n",
      "Prediction Step: 699.\n",
      "Prediction Step: 700.\n",
      "Prediction Step: 701.\n",
      "Prediction Step: 702.\n",
      "Prediction Step: 703.\n",
      "Prediction Step: 704.\n",
      "Prediction Step: 705.\n",
      "Prediction Step: 706.\n",
      "Prediction Step: 707.\n",
      "Prediction Step: 708.\n",
      "Prediction Step: 709.\n",
      "Prediction Step: 710.\n",
      "Prediction Step: 711.\n",
      "Prediction Step: 712.\n",
      "Prediction Step: 713.\n",
      "Prediction Step: 714.\n",
      "Prediction Step: 715.\n",
      "Prediction Step: 716.\n",
      "Prediction Step: 717.\n",
      "Prediction Step: 718.\n",
      "Prediction Step: 719.\n",
      "Prediction Step: 720.\n",
      "Prediction Step: 721.\n",
      "Prediction Step: 722.\n",
      "Prediction Step: 723.\n",
      "Prediction Step: 724.\n",
      "Prediction Step: 725.\n",
      "Prediction Step: 726.\n",
      "Prediction Step: 727.\n",
      "Prediction Step: 728.\n",
      "Prediction Step: 729.\n",
      "Prediction Step: 730.\n",
      "Prediction Step: 731.\n",
      "Prediction Step: 732.\n",
      "Prediction Step: 733.\n",
      "Prediction Step: 734.\n",
      "Prediction Step: 735.\n",
      "Prediction Step: 736.\n",
      "Prediction Step: 737.\n",
      "Prediction Step: 738.\n",
      "Prediction Step: 739.\n",
      "Prediction Step: 740.\n",
      "Prediction Step: 741.\n",
      "Prediction Step: 742.\n",
      "Prediction Step: 743.\n"
     ]
    }
   ],
   "source": [
    "# Run on the Test Data.\n",
    "model.load_from_checkpoint(\"/home/snajafi/checkpoints/t5-base\", \"best_step\")\n",
    "model.to_device()\n",
    "test_dataloader = create_dataloader(model, test_file_name=test_file_name)\n",
    "test_loop(\n",
    "    model=model,\n",
    "    mode=\"test\",\n",
    "    model_path=\"/home/snajafi/checkpoints/t5-base\",\n",
    "    prediction_file_name=\"test.predicted.tsv\",\n",
    "    test_dataloader=test_dataloader,\n",
    "    metric=qa_metric,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
