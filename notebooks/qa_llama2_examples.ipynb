{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-03-07 00:07:10.338879: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-07 00:07:10.421292: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-07 00:07:10.421353: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-07 00:07:10.429177: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-07 00:07:10.445411: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-07 00:07:12.385004: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from typing import Any, Dict, Iterator, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from bitsandbytes.optim.adamw import PagedAdamW8bit\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    ")\n",
    "\n",
    "from src.base_lm import BaseLM\n",
    "from src.general_utils import DictDataset, test_loop, train_loop\n",
    "from src.model_utils import clear_cache, llama2_log_of_labels, lm_logits, mlm_log_of_labels, set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar  7 00:07:40 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A40          On   | 00000000:86:00.0 Off |                    0 |\n",
      "|  0%   38C    P8    34W / 300W |      2MiB / 46068MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A40          On   | 00000000:D8:00.0 Off |                    0 |\n",
      "|  0%   34C    P8    29W / 300W |      2MiB / 46068MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (0.21.3)\n",
      "Requirement already satisfied: filelock in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from huggingface_hub) (2024.2.0)\n",
      "Requirement already satisfied: requests in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from huggingface_hub) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from huggingface_hub) (4.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from requests->huggingface_hub) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from requests->huggingface_hub) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /h/snajafi/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token=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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 4\n",
    "eval_batch_size = 4\n",
    "lm_input_max_length = 1024\n",
    "lm_output_max_length = 128\n",
    "lm_top_p = 0.9\n",
    "temperature = 0.6\n",
    "metric_device = \"cuda:1\"\n",
    "metric_batch_size = 8\n",
    "learning_rate = 0.001\n",
    "train_file_name = \"128-shot-datasets/squad/128-42-train.tsv\"\n",
    "dev_file_name = \"128-shot-datasets/squad/128-42-dev.tsv\"\n",
    "test_file_name = \"128-shot-datasets/squad/test.tsv\"\n",
    "\n",
    "# folder to store models and predictions.\n",
    "model_path = \"/scratch/ssd004/scratch/snajafi/checkpoints/llama2\"\n",
    "\n",
    "# related to lora\n",
    "r = 16\n",
    "lora_alpha = 8\n",
    "lora_dropout = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load LM efficiently.\"\"\"\n",
    "\n",
    "# Make sure we have some tokens defined for the LM, if not defined in the model.\n",
    "_EXTRA_TOKENS = {\n",
    "    \"pad_token\": \"<pad>\",\n",
    "    \"mask_token\": \"<mask>\",\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"</s>\",\n",
    "    \"unk_token\": \"<unk>\",\n",
    "    \"cls_token\": \"<CLS>\",\n",
    "}\n",
    "\n",
    "target_modules = [\"q_proj\", \"v_proj\", \"o_proj\", \"k_proj\"]\n",
    "\n",
    "\n",
    "def load_peft_model(\n",
    "    model: PreTrainedModel,\n",
    "    adapter_name: str = \"lora\",\n",
    "    is_trainable: bool = False,\n",
    "    model_type: str = \"causal_lm\",\n",
    "    lora_target_modules: List[str] = target_modules,\n",
    ") -> torch.nn.Module:\n",
    "    \"\"\"Load a trained PEFT adapter to the base model and return the PeftModel.\n",
    "\n",
    "    Args:\n",
    "    ----\n",
    "        model: the main model.\n",
    "        num_quantized_bits: number of bits in the loaded model.\n",
    "        adapter_name: e.g. lora.\n",
    "        is_trainable: train or inference mode.\n",
    "        model_type: causal lm or seq-to-seq.\n",
    "        lora_target_modules: which modules to train with lora.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "        The PEFT model and tokenizer.\n",
    "    \"\"\"\n",
    "    if model_type == \"causal_lm\":\n",
    "        task_type = TaskType.CAUSAL_LM\n",
    "    elif model_type == \"seq_to_seq_lm\":\n",
    "        task_type = TaskType.SEQ_2_SEQ_LM\n",
    "\n",
    "    if adapter_name == \"lora\":\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=task_type,\n",
    "            inference_mode=not is_trainable,\n",
    "            r=r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            lora_dropout=lora_dropout,\n",
    "            bias=\"none\",\n",
    "            init_lora_weights=True,\n",
    "            target_modules=lora_target_modules,\n",
    "        )\n",
    "\n",
    "    peft_model = get_peft_model(model, peft_config)\n",
    "    peft_model.print_trainable_parameters()\n",
    "    return peft_model\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer(\n",
    "    model_id: str, model_type: str, model_dtype: torch.dtype, attn_implementation: str, load_in_4bit: Optional[bool] = True\n",
    ") -> Tuple[PreTrainedModel, PreTrainedTokenizer]:\n",
    "    \"\"\"Load the model and tokenizer.\n",
    "\n",
    "    Args:\n",
    "    ----\n",
    "        model_id: the id for the pre-trained model.\n",
    "        model_type: causal lm or seq_to_seq_lm.\n",
    "        model_dtype: model data type.\n",
    "        load_in_4bit: Whether to load in 4 bit quantization.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "        The model and tokenizer.\n",
    "    \"\"\"\n",
    "    # load model\n",
    "    if model_type == \"causal_lm\":\n",
    "        ModelClass = AutoModelForCausalLM\n",
    "    elif model_type == \"seq_to_seq_lm\":\n",
    "        ModelClass = AutoModelForSeq2SeqLM\n",
    "    model_args: Dict[str, Any] = {\"use_cache\": False, \"torch_dtype\": model_dtype, \"attn_implementation\": attn_implementation}\n",
    "    if load_in_4bit:\n",
    "        quant_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=model_args[\"torch_dtype\"],\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "        model_args[\"quantization_config\"] = quant_config\n",
    "    model = ModelClass.from_pretrained(\n",
    "        model_id,\n",
    "        **model_args,\n",
    "    )\n",
    "\n",
    "    # load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.add_special_tokens(_EXTRA_TOKENS)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        # extend embeddings to a multiple so we use Tensor cores\n",
    "        multiple = 64 if \"A100\" in torch.cuda.get_device_name() else 8\n",
    "        model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=multiple)\n",
    "    else:\n",
    "        raise Exception(\"No CUDA Found!\")\n",
    "\n",
    "    # re-define token ids for the model.\n",
    "    for extra_token_key, extra_token_val in _EXTRA_TOKENS.items():\n",
    "        extra_token_id = tokenizer.convert_tokens_to_ids([extra_token_val])[0]\n",
    "        model.config.__setattr__(f\"{extra_token_key}_id\", extra_token_id)\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llama2QA(BaseLM):\n",
    "    \"\"\"Class to implement Llama2 for QA task.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        mode: str,\n",
    "        device: str,\n",
    "        seed: int = 42,\n",
    "    ) -> None:\n",
    "        super().__init__(device, \"main_lm\", seed)\n",
    "        self.device = device\n",
    "        model, tokenizer = load_model_and_tokenizer(\n",
    "            model_id=\"/model-weights/Llama-2-7b-chat-hf\",\n",
    "            model_type=\"causal_lm\",\n",
    "            model_dtype=torch.bfloat16,\n",
    "            attn_implementation=\"flash_attention_2\",\n",
    "            load_in_4bit=True,\n",
    "        )\n",
    "        peft_model = load_peft_model(\n",
    "            model=model,\n",
    "            adapter_name=\"lora\",\n",
    "            is_trainable=mode == \"train\",\n",
    "            model_type=\"causal_lm\",\n",
    "        )\n",
    "        self.model = peft_model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # to train the main lm, we update all of its parameters.\n",
    "        self.optimizer = PagedAdamW8bit(self.model.parameters(), lr=learning_rate)\n",
    "        self.scheduler = CosineAnnealingWarmRestarts(self.optimizer, T_0=10, eta_min=learning_rate / 5.0)\n",
    "\n",
    "    def prepare_text(self, texts: List[str], output_texts: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Convert texts to ids and return the dataset required for training\n",
    "        and inference.\"\"\"\n",
    "        instruction = \"In this task, you are given a context and question. \\\n",
    "            Provide a short phrase as the answer for the given question using only the information from the context. \\\n",
    "            If you do not know the answer from the context, generate 'no_answer' in the output. \\\n",
    "            Do not repeat the question in the output.\"\n",
    "        template = \"<s> [INST] <<SYS>> {instruction} <</SYS>> {input_text} [/INST]\"\n",
    "        # sample of the answers if possible.\n",
    "        sampled_answers = [random.choice(text.split(\"[<@>]\")) for text in output_texts]\n",
    "        answers = [f\"Answer: {answer}\" for answer in sampled_answers]\n",
    "\n",
    "        inputs_for_training = [\n",
    "            f\"{template.format(instruction=instruction, input_text=texts[idx])} {answers[idx]}\" for idx in range(len(texts))\n",
    "        ]\n",
    "        inputs_for_generation = [template.format(instruction=instruction, input_text=texts[idx]) for idx in range(len(texts))]\n",
    "\n",
    "        input_encodings = self.tokenizer(\n",
    "            inputs_for_training,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=lm_input_max_length + lm_output_max_length,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        input_encodings_for_generation = self.tokenizer(\n",
    "            inputs_for_generation,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=lm_input_max_length,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        data = {\n",
    "            \"output_texts\": output_texts,\n",
    "            \"lm_input_ids_for_train\": input_encodings.input_ids,\n",
    "            \"lm_attention_mask_for_train\": input_encodings.attention_mask,\n",
    "            \"lm_input_ids_for_generation\": input_encodings_for_generation.input_ids,\n",
    "            \"lm_attention_mask_for_generation\": input_encodings_for_generation.attention_mask,\n",
    "        }\n",
    "        return data\n",
    "\n",
    "    def train(self, batch: torch.utils.data.Dataset) -> torch.Tensor:\n",
    "        \"\"\"Using the Llama2, run a forward computation over the batch, compute\n",
    "        the log probability over the batch.\n",
    "\n",
    "        This will be used for training.\n",
    "        \"\"\"\n",
    "        self.train_mode_on()\n",
    "        loaded_batch = self.data_to_device(batch, keys=[\"lm_input_ids_for_train\", \"lm_attention_mask_for_train\"])\n",
    "        input_ids = loaded_batch[\"lm_input_ids_for_train\"]\n",
    "        attention_mask = loaded_batch[\"lm_attention_mask_for_train\"]\n",
    "        with torch.set_grad_enabled(True):\n",
    "            logits = lm_logits(\n",
    "                model=self.model,\n",
    "                input_ids=input_ids,\n",
    "                input_mask=attention_mask,\n",
    "            )\n",
    "            masked_labels = input_ids.masked_fill(input_ids == self.tokenizer.pad_token_id, -100)\n",
    "            return llama2_log_of_labels(logits=logits, labels=masked_labels, loss_func=self.loss_func)\n",
    "\n",
    "    def generation_pass(self, batch: torch.utils.data.Dataset) -> Tuple[List[str], torch.Tensor]:\n",
    "        \"\"\"Using the Llama2, generate new text.\n",
    "\n",
    "        This will be used for inference.\n",
    "        \"\"\"\n",
    "        self.predict_mode_on()\n",
    "        loaded_batch = self.data_to_device(batch, keys=[\"lm_input_ids_for_generation\", \"lm_attention_mask_for_generation\"])\n",
    "        input_ids = loaded_batch[\"lm_input_ids_for_generation\"]\n",
    "        attention_mask = loaded_batch[\"lm_attention_mask_for_generation\"]\n",
    "        with torch.no_grad():\n",
    "            # more look here:\n",
    "            # https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L130\n",
    "            predictions_output = self.model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                do_sample=True,\n",
    "                top_p=lm_top_p,\n",
    "                temperature=temperature,\n",
    "                max_length=lm_input_max_length + lm_output_max_length,\n",
    "                num_return_sequences=1,\n",
    "                output_logits=True,\n",
    "                return_dict_in_generate=True,\n",
    "                use_cache=True,\n",
    "                renormalize_logits=True,\n",
    "            )\n",
    "\n",
    "        prompt_len = input_ids.size()[1]\n",
    "        selected_samples = predictions_output.sequences[:, prompt_len:]\n",
    "        # all special tokens will be removed.\n",
    "        predictions_str = self.tokenizer.batch_decode(selected_samples, skip_special_tokens=True)\n",
    "        predictions_str = [pred.lstrip('\"').lstrip(\"'\").rstrip(\"'\").rstrip('\"').strip() for pred in predictions_str]\n",
    "\n",
    "        logits_list = list(predictions_output.logits)\n",
    "        logits = torch.stack(logits_list, dim=1)\n",
    "        labels_to_consider = selected_samples.masked_fill(selected_samples == self.tokenizer.pad_token_id, -100)\n",
    "        final_log_ps = mlm_log_of_labels(logits=logits, labels=labels_to_consider, loss_func=self.loss_func)\n",
    "        actual_lens = torch.sum(torch.where(labels_to_consider > 0, 1, 0), dim=1)\n",
    "        # Average log probs per token (length normalization).\n",
    "        return predictions_str, final_log_ps / actual_lens\n",
    "\n",
    "    def predict(self, batch: torch.utils.data.Dataset) -> Iterator[Dict[str, str]]:\n",
    "        \"\"\"The main prediction loop.\"\"\"\n",
    "        answers, log_ps = self.generation_pass(batch)\n",
    "        log_ps = log_ps.cpu().detach().numpy()\n",
    "        for idx, answer in enumerate(answers):\n",
    "            output_row = {\n",
    "                \"potential_answer\": answer,\n",
    "                \"prediction_score\": log_ps[idx],\n",
    "                \"gold_answer\": batch[\"output_texts\"][idx],\n",
    "            }\n",
    "            yield output_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gen_fewshot_file(file_path: str) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Load the fewshot files for QA task.\"\"\"\n",
    "    df = pd.read_csv(file_path, sep=\"\\t\")\n",
    "    input_texts = df.article.tolist()\n",
    "    output_texts = df.answer.tolist()\n",
    "    return input_texts, output_texts\n",
    "\n",
    "\n",
    "def create_dataloader(\n",
    "    model: Llama2QA,\n",
    "    train_file_name: Optional[str] = None,\n",
    "    dev_file_name: Optional[str] = None,\n",
    "    test_file_name: Optional[str] = None,\n",
    ") -> DataLoader:\n",
    "    \"\"\"Function to create the required dataloader to train the LM models.\"\"\"\n",
    "    if train_file_name is not None:\n",
    "        input_texts, output_texts = read_gen_fewshot_file(train_file_name)\n",
    "        shuffle = True\n",
    "        batch_size = train_batch_size\n",
    "\n",
    "    if dev_file_name is not None:\n",
    "        input_texts, output_texts = read_gen_fewshot_file(dev_file_name)\n",
    "        shuffle = False\n",
    "        batch_size = eval_batch_size\n",
    "\n",
    "    if test_file_name is not None:\n",
    "        input_texts, output_texts = read_gen_fewshot_file(test_file_name)\n",
    "        shuffle = False\n",
    "        batch_size = eval_batch_size\n",
    "\n",
    "    data = model.prepare_text(input_texts, output_texts)\n",
    "    dataset = DictDataset(data)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAMetricModel:\n",
    "    \"\"\"Load and cache a model used for evaluating generative text\n",
    "    generation.\"\"\"\n",
    "\n",
    "    model_id = \"sentence-transformers/sentence-t5-xl\"\n",
    "\n",
    "    def __init__(self, device: str = \"cuda:0\", batch_size: int = 16) -> None:\n",
    "        \"\"\"Save the gpu device and construct the model and cache it.\"\"\"\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.metric_model = SentenceTransformer(self.model_id, device=self.device).eval()\n",
    "\n",
    "    def compute_metric(self, predictions: List[str], references: List[List[str]]) -> float:\n",
    "        \"\"\"Compute the metric for the given predictions and multiple\n",
    "        references.\"\"\"\n",
    "        average_score = torch.tensor(0.0, device=self.device)\n",
    "        num_chunks = max(len(predictions) // self.batch_size, 1)\n",
    "        for chunk_i in range(num_chunks):\n",
    "            clear_cache()\n",
    "\n",
    "            if (chunk_i + 1) * self.batch_size <= len(predictions):\n",
    "                predictions_sub_arr = predictions[chunk_i * self.batch_size : (chunk_i + 1) * self.batch_size]\n",
    "                references_sub_arr = references[chunk_i * self.batch_size : (chunk_i + 1) * self.batch_size]\n",
    "            else:\n",
    "                predictions_sub_arr = predictions[chunk_i * self.batch_size :]\n",
    "                references_sub_arr = references[chunk_i * self.batch_size :]\n",
    "\n",
    "            # need to track multiple references.\n",
    "            ref_sub_arr_len = [len(ref_sub_arr) for ref_sub_arr in references_sub_arr]\n",
    "            references_sub_arr_flattened = []\n",
    "            for ref_sub_arr in references_sub_arr:\n",
    "                references_sub_arr_flattened.extend(ref_sub_arr)\n",
    "\n",
    "            prediction_embeddings = self.metric_model.encode(\n",
    "                predictions_sub_arr,\n",
    "                show_progress_bar=False,\n",
    "                batch_size=self.batch_size,\n",
    "                device=self.device,\n",
    "                normalize_embeddings=True,\n",
    "                convert_to_tensor=True,\n",
    "            )\n",
    "\n",
    "            references_embeddings = self.metric_model.encode(\n",
    "                references_sub_arr_flattened,\n",
    "                show_progress_bar=False,\n",
    "                batch_size=self.batch_size,\n",
    "                device=self.device,\n",
    "                normalize_embeddings=True,\n",
    "                convert_to_tensor=True,\n",
    "            )\n",
    "            dot_products = torch.matmul(prediction_embeddings, references_embeddings.t())\n",
    "            score_collector = torch.zeros_like(dot_products)\n",
    "            i = 0\n",
    "            j = 0\n",
    "            while i < len(predictions_sub_arr):\n",
    "                j_len = ref_sub_arr_len[i]\n",
    "                score_collector[i][j : j + j_len] = 1.0 / j_len\n",
    "                i += 1\n",
    "                j += j_len\n",
    "\n",
    "            average_score += torch.sum(dot_products * score_collector)\n",
    "        return (average_score / len(predictions)).item()\n",
    "\n",
    "\n",
    "qa_metric_model = None\n",
    "\n",
    "\n",
    "def postprocess_qa(label: str) -> str:\n",
    "    label = str(label)\n",
    "    label = label.lower()\n",
    "    label = label.replace(\"\\n\", \" \")\n",
    "    label = label.removesuffix(\"</s>\")\n",
    "    label = label.removeprefix(\"<s>\")\n",
    "    label = label.removeprefix(\"\\n\")\n",
    "    label = label.removesuffix(\"\\n\")\n",
    "    label = label.removeprefix(\".\")\n",
    "    label = label.removesuffix(\".\")\n",
    "    label = label.removeprefix(\"answer:\")\n",
    "    label = label.removeprefix(\",\")\n",
    "    label = label.strip()\n",
    "    if \"no answer\" in label or \"no_answer\" in label:\n",
    "        label = \"no answer\"\n",
    "    return label\n",
    "\n",
    "\n",
    "def qa_metric(prediction_file: str) -> Dict[str, float]:\n",
    "    \"\"\"Compute the metric for the qa task.\"\"\"\n",
    "    global qa_metric_model\n",
    "    if qa_metric_model is None:\n",
    "        qa_metric_model = QAMetricModel(device=metric_device, batch_size=metric_batch_size)\n",
    "\n",
    "    df = pd.read_csv(prediction_file, delimiter=\",\")\n",
    "\n",
    "    gold_answers = [postprocess_qa(label) for label in df[\"gold_answer\"].tolist()]\n",
    "\n",
    "    multiple_gold_answers = []\n",
    "    for answer in gold_answers:\n",
    "        multiple_gold_answers.append(answer.split(\"[<@>]\"))\n",
    "\n",
    "    return_metrics: Dict[str, float] = {}\n",
    "    metrics = {\n",
    "        \"potential_answer\": \"qa_score\",\n",
    "    }\n",
    "\n",
    "    for metric_column, metric in metrics.items():\n",
    "        if metric_column in df.columns:\n",
    "            predictions = [postprocess_qa(pred) for pred in df[metric_column].tolist()]\n",
    "            score = qa_metric_model.compute_metric(predictions, multiple_gold_answers)\n",
    "            return_metrics[metric] = score\n",
    "\n",
    "    return return_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:34<00:00, 17.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 16,777,216 || all params: 6,755,258,368 || trainable%: 0.24835787302339937\n"
     ]
    }
   ],
   "source": [
    "# Create model and start training.\n",
    "set_random_seed(42)\n",
    "\n",
    "model = Llama2QA(mode=\"train\", device=\"cuda:0\", seed=42)\n",
    "model.to_device()\n",
    "train_dataloader = create_dataloader(model, train_file_name=train_file_name)\n",
    "dev_dataloader = create_dataloader(model, dev_file_name=dev_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Prediction Step: 9.\n",
      "Prediction Step: 10.\n",
      "Prediction Step: 11.\n",
      "Prediction Step: 12.\n",
      "Prediction Step: 13.\n",
      "Prediction Step: 14.\n",
      "Prediction Step: 15.\n",
      "Prediction Step: 16.\n",
      "Prediction Step: 17.\n",
      "Prediction Step: 18.\n",
      "Prediction Step: 19.\n",
      "Prediction Step: 20.\n",
      "Prediction Step: 21.\n",
      "Prediction Step: 22.\n",
      "Prediction Step: 23.\n",
      "Prediction Step: 24.\n",
      "Prediction Step: 25.\n",
      "Prediction Step: 26.\n",
      "Prediction Step: 27.\n",
      "Prediction Step: 28.\n",
      "Prediction Step: 29.\n",
      "Prediction Step: 30.\n",
      "Prediction Step: 31.\n",
      "Prediction Step: 32.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /model-weights/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:0\n",
      "\n",
      "Epoch: 0 | Batch: 1 | Mean Loss: 896.2059936523438 | Epoch Loss: 896.2059936523438 | Loss: 896.2059936523438\n",
      "\n",
      "Epoch: 0 | Batch: 2 | Mean Loss: 888.8539123535156 | Epoch Loss: 888.8539123535156 | Loss: 881.5018310546875\n",
      "\n",
      "Epoch: 0 | Batch: 3 | Mean Loss: 757.6346842447916 | Epoch Loss: 757.6346842447916 | Loss: 495.19622802734375\n",
      "\n",
      "Epoch: 0 | Batch: 4 | Mean Loss: 704.382568359375 | Epoch Loss: 704.382568359375 | Loss: 544.626220703125\n",
      "\n",
      "Epoch: 0 | Batch: 5 | Mean Loss: 688.8373291015625 | Epoch Loss: 688.8373291015625 | Loss: 626.6563720703125\n",
      "\n",
      "Epoch: 0 | Batch: 6 | Mean Loss: 709.0307006835938 | Epoch Loss: 709.0307006835938 | Loss: 809.99755859375\n",
      "\n",
      "Epoch: 0 | Batch: 7 | Mean Loss: 679.402836390904 | Epoch Loss: 679.402836390904 | Loss: 501.6356506347656\n",
      "\n",
      "Epoch: 0 | Batch: 8 | Mean Loss: 649.112491607666 | Epoch Loss: 649.112491607666 | Loss: 437.080078125\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Prediction Step: 9.\n",
      "Prediction Step: 10.\n",
      "Prediction Step: 11.\n",
      "Prediction Step: 12.\n",
      "Prediction Step: 13.\n",
      "Prediction Step: 14.\n",
      "Prediction Step: 15.\n",
      "Prediction Step: 16.\n",
      "Prediction Step: 17.\n",
      "Prediction Step: 18.\n",
      "Prediction Step: 19.\n",
      "Prediction Step: 20.\n",
      "Prediction Step: 21.\n",
      "Prediction Step: 22.\n",
      "Prediction Step: 23.\n",
      "Prediction Step: 24.\n",
      "Prediction Step: 25.\n",
      "Prediction Step: 26.\n",
      "Prediction Step: 27.\n",
      "Prediction Step: 28.\n",
      "Prediction Step: 29.\n",
      "Prediction Step: 30.\n",
      "Prediction Step: 31.\n",
      "Prediction Step: 32.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /model-weights/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Batch: 9 | Mean Loss: 620.9685702853733 | Epoch Loss: 620.9685702853733 | Loss: 395.81719970703125\n",
      "\n",
      "Epoch: 0 | Batch: 10 | Mean Loss: 601.8680450439454 | Epoch Loss: 601.8680450439454 | Loss: 429.96331787109375\n",
      "\n",
      "Epoch: 0 | Batch: 11 | Mean Loss: 573.5077237215909 | Epoch Loss: 573.5077237215909 | Loss: 289.9045104980469\n",
      "\n",
      "Epoch: 0 | Batch: 12 | Mean Loss: 557.0836232503256 | Epoch Loss: 557.0836232503256 | Loss: 376.41851806640625\n",
      "\n",
      "Epoch: 0 | Batch: 13 | Mean Loss: 560.4674025315505 | Epoch Loss: 560.4674025315505 | Loss: 601.07275390625\n",
      "\n",
      "Epoch: 0 | Batch: 14 | Mean Loss: 549.3916822160993 | Epoch Loss: 549.3916822160993 | Loss: 405.4073181152344\n",
      "\n",
      "Epoch: 0 | Batch: 15 | Mean Loss: 529.8145884195964 | Epoch Loss: 529.8145884195964 | Loss: 255.7352752685547\n",
      "\n",
      "Epoch: 0 | Batch: 16 | Mean Loss: 520.4582185745239 | Epoch Loss: 520.4582185745239 | Loss: 380.1126708984375\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Prediction Step: 9.\n",
      "Prediction Step: 10.\n",
      "Prediction Step: 11.\n",
      "Prediction Step: 12.\n",
      "Prediction Step: 13.\n",
      "Prediction Step: 14.\n",
      "Prediction Step: 15.\n",
      "Prediction Step: 16.\n",
      "Prediction Step: 17.\n",
      "Prediction Step: 18.\n",
      "Prediction Step: 19.\n",
      "Prediction Step: 20.\n",
      "Prediction Step: 21.\n",
      "Prediction Step: 22.\n",
      "Prediction Step: 23.\n",
      "Prediction Step: 24.\n",
      "Prediction Step: 25.\n",
      "Prediction Step: 26.\n",
      "Prediction Step: 27.\n",
      "Prediction Step: 28.\n",
      "Prediction Step: 29.\n",
      "Prediction Step: 30.\n",
      "Prediction Step: 31.\n",
      "Prediction Step: 32.\n",
      "Epoch: 0 | Batch: 17 | Mean Loss: 513.0879004983341 | Epoch Loss: 513.0879004983341 | Loss: 395.1628112792969\n",
      "\n",
      "Epoch: 0 | Batch: 18 | Mean Loss: 510.30247751871747 | Epoch Loss: 510.30247751871747 | Loss: 462.9502868652344\n",
      "\n",
      "Epoch: 0 | Batch: 19 | Mean Loss: 510.29181791606703 | Epoch Loss: 510.29181791606703 | Loss: 510.0999450683594\n",
      "\n",
      "Epoch: 0 | Batch: 20 | Mean Loss: 512.3992118835449 | Epoch Loss: 512.3992118835449 | Loss: 552.439697265625\n",
      "\n",
      "Epoch: 0 | Batch: 21 | Mean Loss: 503.91070774623324 | Epoch Loss: 503.91070774623324 | Loss: 334.140625\n",
      "\n",
      "Epoch: 0 | Batch: 22 | Mean Loss: 502.9897398515181 | Epoch Loss: 502.9897398515181 | Loss: 483.6494140625\n",
      "\n",
      "Epoch: 0 | Batch: 23 | Mean Loss: 498.1526217253312 | Epoch Loss: 498.1526217253312 | Loss: 391.73602294921875\n",
      "\n",
      "Epoch: 0 | Batch: 24 | Mean Loss: 493.9855047861735 | Epoch Loss: 493.9855047861735 | Loss: 398.1418151855469\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Prediction Step: 9.\n",
      "Prediction Step: 10.\n",
      "Prediction Step: 11.\n",
      "Prediction Step: 12.\n",
      "Prediction Step: 13.\n",
      "Prediction Step: 14.\n",
      "Prediction Step: 15.\n",
      "Prediction Step: 16.\n",
      "Prediction Step: 17.\n",
      "Prediction Step: 18.\n",
      "Prediction Step: 19.\n",
      "Prediction Step: 20.\n",
      "Prediction Step: 21.\n",
      "Prediction Step: 22.\n",
      "Prediction Step: 23.\n",
      "Prediction Step: 24.\n",
      "Prediction Step: 25.\n",
      "Prediction Step: 26.\n",
      "Prediction Step: 27.\n",
      "Prediction Step: 28.\n",
      "Prediction Step: 29.\n",
      "Prediction Step: 30.\n",
      "Prediction Step: 31.\n",
      "Prediction Step: 32.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /model-weights/Llama-2-7b-chat-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Batch: 25 | Mean Loss: 486.18854064941405 | Epoch Loss: 486.18854064941405 | Loss: 299.0614013671875\n",
      "\n",
      "Epoch: 0 | Batch: 26 | Mean Loss: 478.96666248028095 | Epoch Loss: 478.96666248028095 | Loss: 298.4197082519531\n",
      "\n",
      "Epoch: 0 | Batch: 27 | Mean Loss: 475.356921725803 | Epoch Loss: 475.356921725803 | Loss: 381.503662109375\n",
      "\n",
      "Epoch: 0 | Batch: 28 | Mean Loss: 470.8517842973982 | Epoch Loss: 470.8517842973982 | Loss: 349.21307373046875\n",
      "\n",
      "Epoch: 0 | Batch: 29 | Mean Loss: 468.99045799518456 | Epoch Loss: 468.99045799518456 | Loss: 416.8733215332031\n",
      "\n",
      "Epoch: 0 | Batch: 30 | Mean Loss: 464.14368540445963 | Epoch Loss: 464.14368540445963 | Loss: 323.5872802734375\n",
      "\n",
      "Epoch: 0 | Batch: 31 | Mean Loss: 461.2584292504095 | Epoch Loss: 461.2584292504095 | Loss: 374.70074462890625\n",
      "\n",
      "Epoch: 0 | Batch: 32 | Mean Loss: 461.9281430244446 | Epoch Loss: 461.9281430244446 | Loss: 482.68927001953125\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Prediction Step: 9.\n",
      "Prediction Step: 10.\n",
      "Prediction Step: 11.\n",
      "Prediction Step: 12.\n",
      "Prediction Step: 13.\n",
      "Prediction Step: 14.\n",
      "Prediction Step: 15.\n",
      "Prediction Step: 16.\n",
      "Prediction Step: 17.\n",
      "Prediction Step: 18.\n",
      "Prediction Step: 19.\n",
      "Prediction Step: 20.\n",
      "Prediction Step: 21.\n",
      "Prediction Step: 22.\n",
      "Prediction Step: 23.\n",
      "Prediction Step: 24.\n",
      "Prediction Step: 25.\n",
      "Prediction Step: 26.\n",
      "Prediction Step: 27.\n",
      "Prediction Step: 28.\n",
      "Prediction Step: 29.\n",
      "Prediction Step: 30.\n",
      "Prediction Step: 31.\n",
      "Prediction Step: 32.\n",
      "\n",
      "Epoch:1\n",
      "\n",
      "Epoch: 1 | Batch: 1 | Mean Loss: 460.45158663663 | Epoch Loss: 413.2017822265625 | Loss: 413.2017822265625\n",
      "\n",
      "Epoch: 1 | Batch: 2 | Mean Loss: 463.2631248025333 | Epoch Loss: 484.6228332519531 | Loss: 556.0438842773438\n",
      "\n",
      "Epoch: 1 | Batch: 3 | Mean Loss: 459.54974757603236 | Epoch Loss: 434.18019612630206 | Loss: 333.294921875\n",
      "\n",
      "Epoch: 1 | Batch: 4 | Mean Loss: 455.66098488701715 | Epoch Loss: 405.52371978759766 | Loss: 319.5542907714844\n",
      "\n",
      "Epoch: 1 | Batch: 5 | Mean Loss: 455.67555979135875 | Epoch Loss: 415.6590270996094 | Loss: 456.20025634765625\n",
      "\n",
      "Epoch: 1 | Batch: 6 | Mean Loss: 449.9828358700401 | Epoch Loss: 386.2745310465495 | Loss: 239.35205078125\n",
      "\n",
      "Epoch: 1 | Batch: 7 | Mean Loss: 445.4942497840294 | Epoch Loss: 370.36788068498885 | Loss: 274.927978515625\n",
      "\n",
      "Epoch: 1 | Batch: 8 | Mean Loss: 448.07374687194823 | Epoch Loss: 392.6561622619629 | Loss: 548.6741333007812\n",
      "\n",
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n"
     ]
    }
   ],
   "source": [
    "train_loop(\n",
    "    model=model,\n",
    "    mode=\"train\",\n",
    "    model_path=model_path,\n",
    "    metric_to_save=\"qa_score\",\n",
    "    max_epochs=10,\n",
    "    training_steps=100000,  # not important\n",
    "    steps_per_checkpoint=8,\n",
    "    metric=qa_metric,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=dev_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Prediction Step: 9.\n",
      "Prediction Step: 10.\n",
      "Prediction Step: 11.\n",
      "Prediction Step: 12.\n",
      "Prediction Step: 13.\n",
      "Prediction Step: 14.\n",
      "Prediction Step: 15.\n",
      "Prediction Step: 16.\n",
      "Prediction Step: 17.\n",
      "Prediction Step: 18.\n",
      "Prediction Step: 19.\n",
      "Prediction Step: 20.\n",
      "Prediction Step: 21.\n",
      "Prediction Step: 22.\n",
      "Prediction Step: 23.\n",
      "Prediction Step: 24.\n",
      "Prediction Step: 25.\n",
      "Prediction Step: 26.\n",
      "Prediction Step: 27.\n",
      "Prediction Step: 28.\n",
      "Prediction Step: 29.\n",
      "Prediction Step: 30.\n",
      "Prediction Step: 31.\n",
      "Prediction Step: 32.\n",
      "Prediction Step: 33.\n",
      "Prediction Step: 34.\n",
      "Prediction Step: 35.\n",
      "Prediction Step: 36.\n",
      "Prediction Step: 37.\n",
      "Prediction Step: 38.\n",
      "Prediction Step: 39.\n",
      "Prediction Step: 40.\n",
      "Prediction Step: 41.\n",
      "Prediction Step: 42.\n",
      "Prediction Step: 43.\n",
      "Prediction Step: 44.\n",
      "Prediction Step: 45.\n",
      "Prediction Step: 46.\n",
      "Prediction Step: 47.\n",
      "Prediction Step: 48.\n",
      "Prediction Step: 49.\n",
      "Prediction Step: 50.\n",
      "Prediction Step: 51.\n",
      "Prediction Step: 52.\n",
      "Prediction Step: 53.\n",
      "Prediction Step: 54.\n",
      "Prediction Step: 55.\n",
      "Prediction Step: 56.\n",
      "Prediction Step: 57.\n",
      "Prediction Step: 58.\n",
      "Prediction Step: 59.\n",
      "Prediction Step: 60.\n",
      "Prediction Step: 61.\n",
      "Prediction Step: 62.\n",
      "Prediction Step: 63.\n",
      "Prediction Step: 64.\n",
      "Prediction Step: 65.\n",
      "Prediction Step: 66.\n",
      "Prediction Step: 67.\n",
      "Prediction Step: 68.\n",
      "Prediction Step: 69.\n",
      "Prediction Step: 70.\n",
      "Prediction Step: 71.\n",
      "Prediction Step: 72.\n",
      "Prediction Step: 73.\n",
      "Prediction Step: 74.\n",
      "Prediction Step: 75.\n",
      "Prediction Step: 76.\n",
      "Prediction Step: 77.\n",
      "Prediction Step: 78.\n",
      "Prediction Step: 79.\n",
      "Prediction Step: 80.\n",
      "Prediction Step: 81.\n",
      "Prediction Step: 82.\n",
      "Prediction Step: 83.\n",
      "Prediction Step: 84.\n",
      "Prediction Step: 85.\n",
      "Prediction Step: 86.\n",
      "Prediction Step: 87.\n",
      "Prediction Step: 88.\n",
      "Prediction Step: 89.\n",
      "Prediction Step: 90.\n",
      "Prediction Step: 91.\n",
      "Prediction Step: 92.\n",
      "Prediction Step: 93.\n",
      "Prediction Step: 94.\n",
      "Prediction Step: 95.\n",
      "Prediction Step: 96.\n",
      "Prediction Step: 97.\n",
      "Prediction Step: 98.\n",
      "Prediction Step: 99.\n",
      "Prediction Step: 100.\n",
      "Prediction Step: 101.\n",
      "Prediction Step: 102.\n",
      "Prediction Step: 103.\n",
      "Prediction Step: 104.\n",
      "Prediction Step: 105.\n",
      "Prediction Step: 106.\n",
      "Prediction Step: 107.\n",
      "Prediction Step: 108.\n",
      "Prediction Step: 109.\n",
      "Prediction Step: 110.\n",
      "Prediction Step: 111.\n",
      "Prediction Step: 112.\n",
      "Prediction Step: 113.\n",
      "Prediction Step: 114.\n",
      "Prediction Step: 115.\n",
      "Prediction Step: 116.\n",
      "Prediction Step: 117.\n",
      "Prediction Step: 118.\n",
      "Prediction Step: 119.\n",
      "Prediction Step: 120.\n",
      "Prediction Step: 121.\n",
      "Prediction Step: 122.\n",
      "Prediction Step: 123.\n",
      "Prediction Step: 124.\n",
      "Prediction Step: 125.\n",
      "Prediction Step: 126.\n",
      "Prediction Step: 127.\n",
      "Prediction Step: 128.\n",
      "Prediction Step: 129.\n",
      "Prediction Step: 130.\n",
      "Prediction Step: 131.\n",
      "Prediction Step: 132.\n",
      "Prediction Step: 133.\n",
      "Prediction Step: 134.\n",
      "Prediction Step: 135.\n",
      "Prediction Step: 136.\n",
      "Prediction Step: 137.\n",
      "Prediction Step: 138.\n",
      "Prediction Step: 139.\n",
      "Prediction Step: 140.\n",
      "Prediction Step: 141.\n",
      "Prediction Step: 142.\n",
      "Prediction Step: 143.\n",
      "Prediction Step: 144.\n",
      "Prediction Step: 145.\n",
      "Prediction Step: 146.\n",
      "Prediction Step: 147.\n",
      "Prediction Step: 148.\n",
      "Prediction Step: 149.\n",
      "Prediction Step: 150.\n",
      "Prediction Step: 151.\n",
      "Prediction Step: 152.\n",
      "Prediction Step: 153.\n",
      "Prediction Step: 154.\n",
      "Prediction Step: 155.\n",
      "Prediction Step: 156.\n",
      "Prediction Step: 157.\n",
      "Prediction Step: 158.\n",
      "Prediction Step: 159.\n",
      "Prediction Step: 160.\n",
      "Prediction Step: 161.\n",
      "Prediction Step: 162.\n",
      "Prediction Step: 163.\n",
      "Prediction Step: 164.\n",
      "Prediction Step: 165.\n",
      "Prediction Step: 166.\n",
      "Prediction Step: 167.\n",
      "Prediction Step: 168.\n",
      "Prediction Step: 169.\n",
      "Prediction Step: 170.\n",
      "Prediction Step: 171.\n",
      "Prediction Step: 172.\n",
      "Prediction Step: 173.\n",
      "Prediction Step: 174.\n",
      "Prediction Step: 175.\n",
      "Prediction Step: 176.\n",
      "Prediction Step: 177.\n",
      "Prediction Step: 178.\n",
      "Prediction Step: 179.\n",
      "Prediction Step: 180.\n",
      "Prediction Step: 181.\n",
      "Prediction Step: 182.\n",
      "Prediction Step: 183.\n",
      "Prediction Step: 184.\n",
      "Prediction Step: 185.\n",
      "Prediction Step: 186.\n",
      "Prediction Step: 187.\n",
      "Prediction Step: 188.\n",
      "Prediction Step: 189.\n",
      "Prediction Step: 190.\n",
      "Prediction Step: 191.\n",
      "Prediction Step: 192.\n",
      "Prediction Step: 193.\n",
      "Prediction Step: 194.\n",
      "Prediction Step: 195.\n",
      "Prediction Step: 196.\n",
      "Prediction Step: 197.\n",
      "Prediction Step: 198.\n",
      "Prediction Step: 199.\n",
      "Prediction Step: 200.\n",
      "Prediction Step: 201.\n",
      "Prediction Step: 202.\n",
      "Prediction Step: 203.\n",
      "Prediction Step: 204.\n",
      "Prediction Step: 205.\n",
      "Prediction Step: 206.\n",
      "Prediction Step: 207.\n",
      "Prediction Step: 208.\n",
      "Prediction Step: 209.\n",
      "Prediction Step: 210.\n",
      "Prediction Step: 211.\n",
      "Prediction Step: 212.\n",
      "Prediction Step: 213.\n",
      "Prediction Step: 214.\n",
      "Prediction Step: 215.\n",
      "Prediction Step: 216.\n",
      "Prediction Step: 217.\n",
      "Prediction Step: 218.\n",
      "Prediction Step: 219.\n",
      "Prediction Step: 220.\n",
      "Prediction Step: 221.\n",
      "Prediction Step: 222.\n",
      "Prediction Step: 223.\n",
      "Prediction Step: 224.\n",
      "Prediction Step: 225.\n",
      "Prediction Step: 226.\n",
      "Prediction Step: 227.\n",
      "Prediction Step: 228.\n",
      "Prediction Step: 229.\n",
      "Prediction Step: 230.\n",
      "Prediction Step: 231.\n",
      "Prediction Step: 232.\n",
      "Prediction Step: 233.\n",
      "Prediction Step: 234.\n",
      "Prediction Step: 235.\n",
      "Prediction Step: 236.\n",
      "Prediction Step: 237.\n",
      "Prediction Step: 238.\n",
      "Prediction Step: 239.\n",
      "Prediction Step: 240.\n",
      "Prediction Step: 241.\n",
      "Prediction Step: 242.\n",
      "Prediction Step: 243.\n",
      "Prediction Step: 244.\n",
      "Prediction Step: 245.\n",
      "Prediction Step: 246.\n",
      "Prediction Step: 247.\n",
      "Prediction Step: 248.\n",
      "Prediction Step: 249.\n",
      "Prediction Step: 250.\n",
      "Prediction Step: 251.\n",
      "Prediction Step: 252.\n",
      "Prediction Step: 253.\n",
      "Prediction Step: 254.\n",
      "Prediction Step: 255.\n",
      "Prediction Step: 256.\n",
      "Prediction Step: 257.\n",
      "Prediction Step: 258.\n",
      "Prediction Step: 259.\n",
      "Prediction Step: 260.\n",
      "Prediction Step: 261.\n",
      "Prediction Step: 262.\n",
      "Prediction Step: 263.\n",
      "Prediction Step: 264.\n",
      "Prediction Step: 265.\n",
      "Prediction Step: 266.\n",
      "Prediction Step: 267.\n",
      "Prediction Step: 268.\n",
      "Prediction Step: 269.\n",
      "Prediction Step: 270.\n",
      "Prediction Step: 271.\n",
      "Prediction Step: 272.\n",
      "Prediction Step: 273.\n",
      "Prediction Step: 274.\n",
      "Prediction Step: 275.\n",
      "Prediction Step: 276.\n",
      "Prediction Step: 277.\n",
      "Prediction Step: 278.\n",
      "Prediction Step: 279.\n",
      "Prediction Step: 280.\n",
      "Prediction Step: 281.\n",
      "Prediction Step: 282.\n",
      "Prediction Step: 283.\n",
      "Prediction Step: 284.\n",
      "Prediction Step: 285.\n",
      "Prediction Step: 286.\n",
      "Prediction Step: 287.\n",
      "Prediction Step: 288.\n",
      "Prediction Step: 289.\n",
      "Prediction Step: 290.\n",
      "Prediction Step: 291.\n",
      "Prediction Step: 292.\n",
      "Prediction Step: 293.\n",
      "Prediction Step: 294.\n",
      "Prediction Step: 295.\n",
      "Prediction Step: 296.\n",
      "Prediction Step: 297.\n",
      "Prediction Step: 298.\n",
      "Prediction Step: 299.\n",
      "Prediction Step: 300.\n",
      "Prediction Step: 301.\n",
      "Prediction Step: 302.\n",
      "Prediction Step: 303.\n",
      "Prediction Step: 304.\n",
      "Prediction Step: 305.\n",
      "Prediction Step: 306.\n",
      "Prediction Step: 307.\n",
      "Prediction Step: 308.\n",
      "Prediction Step: 309.\n",
      "Prediction Step: 310.\n",
      "Prediction Step: 311.\n",
      "Prediction Step: 312.\n",
      "Prediction Step: 313.\n",
      "Prediction Step: 314.\n",
      "Prediction Step: 315.\n",
      "Prediction Step: 316.\n",
      "Prediction Step: 317.\n",
      "Prediction Step: 318.\n",
      "Prediction Step: 319.\n",
      "Prediction Step: 320.\n",
      "Prediction Step: 321.\n",
      "Prediction Step: 322.\n",
      "Prediction Step: 323.\n",
      "Prediction Step: 324.\n",
      "Prediction Step: 325.\n",
      "Prediction Step: 326.\n",
      "Prediction Step: 327.\n",
      "Prediction Step: 328.\n",
      "Prediction Step: 329.\n",
      "Prediction Step: 330.\n",
      "Prediction Step: 331.\n",
      "Prediction Step: 332.\n",
      "Prediction Step: 333.\n",
      "Prediction Step: 334.\n",
      "Prediction Step: 335.\n",
      "Prediction Step: 336.\n",
      "Prediction Step: 337.\n",
      "Prediction Step: 338.\n",
      "Prediction Step: 339.\n",
      "Prediction Step: 340.\n",
      "Prediction Step: 341.\n",
      "Prediction Step: 342.\n",
      "Prediction Step: 343.\n",
      "Prediction Step: 344.\n",
      "Prediction Step: 345.\n",
      "Prediction Step: 346.\n",
      "Prediction Step: 347.\n",
      "Prediction Step: 348.\n",
      "Prediction Step: 349.\n",
      "Prediction Step: 350.\n",
      "Prediction Step: 351.\n",
      "Prediction Step: 352.\n",
      "Prediction Step: 353.\n",
      "Prediction Step: 354.\n",
      "Prediction Step: 355.\n",
      "Prediction Step: 356.\n",
      "Prediction Step: 357.\n",
      "Prediction Step: 358.\n",
      "Prediction Step: 359.\n",
      "Prediction Step: 360.\n",
      "Prediction Step: 361.\n",
      "Prediction Step: 362.\n",
      "Prediction Step: 363.\n",
      "Prediction Step: 364.\n",
      "Prediction Step: 365.\n",
      "Prediction Step: 366.\n",
      "Prediction Step: 367.\n",
      "Prediction Step: 368.\n",
      "Prediction Step: 369.\n",
      "Prediction Step: 370.\n",
      "Prediction Step: 371.\n",
      "Prediction Step: 372.\n",
      "Prediction Step: 373.\n",
      "Prediction Step: 374.\n",
      "Prediction Step: 375.\n",
      "Prediction Step: 376.\n",
      "Prediction Step: 377.\n",
      "Prediction Step: 378.\n",
      "Prediction Step: 379.\n",
      "Prediction Step: 380.\n",
      "Prediction Step: 381.\n",
      "Prediction Step: 382.\n",
      "Prediction Step: 383.\n",
      "Prediction Step: 384.\n",
      "Prediction Step: 385.\n",
      "Prediction Step: 386.\n",
      "Prediction Step: 387.\n",
      "Prediction Step: 388.\n",
      "Prediction Step: 389.\n",
      "Prediction Step: 390.\n",
      "Prediction Step: 391.\n",
      "Prediction Step: 392.\n",
      "Prediction Step: 393.\n",
      "Prediction Step: 394.\n",
      "Prediction Step: 395.\n",
      "Prediction Step: 396.\n",
      "Prediction Step: 397.\n",
      "Prediction Step: 398.\n",
      "Prediction Step: 399.\n",
      "Prediction Step: 400.\n",
      "Prediction Step: 401.\n",
      "Prediction Step: 402.\n",
      "Prediction Step: 403.\n",
      "Prediction Step: 404.\n",
      "Prediction Step: 405.\n",
      "Prediction Step: 406.\n",
      "Prediction Step: 407.\n",
      "Prediction Step: 408.\n",
      "Prediction Step: 409.\n",
      "Prediction Step: 410.\n",
      "Prediction Step: 411.\n",
      "Prediction Step: 412.\n",
      "Prediction Step: 413.\n",
      "Prediction Step: 414.\n",
      "Prediction Step: 415.\n",
      "Prediction Step: 416.\n",
      "Prediction Step: 417.\n",
      "Prediction Step: 418.\n",
      "Prediction Step: 419.\n",
      "Prediction Step: 420.\n",
      "Prediction Step: 421.\n",
      "Prediction Step: 422.\n",
      "Prediction Step: 423.\n",
      "Prediction Step: 424.\n",
      "Prediction Step: 425.\n",
      "Prediction Step: 426.\n",
      "Prediction Step: 427.\n",
      "Prediction Step: 428.\n",
      "Prediction Step: 429.\n",
      "Prediction Step: 430.\n",
      "Prediction Step: 431.\n",
      "Prediction Step: 432.\n",
      "Prediction Step: 433.\n",
      "Prediction Step: 434.\n",
      "Prediction Step: 435.\n",
      "Prediction Step: 436.\n",
      "Prediction Step: 437.\n",
      "Prediction Step: 438.\n",
      "Prediction Step: 439.\n",
      "Prediction Step: 440.\n",
      "Prediction Step: 441.\n",
      "Prediction Step: 442.\n",
      "Prediction Step: 443.\n",
      "Prediction Step: 444.\n",
      "Prediction Step: 445.\n",
      "Prediction Step: 446.\n",
      "Prediction Step: 447.\n",
      "Prediction Step: 448.\n",
      "Prediction Step: 449.\n",
      "Prediction Step: 450.\n",
      "Prediction Step: 451.\n",
      "Prediction Step: 452.\n",
      "Prediction Step: 453.\n",
      "Prediction Step: 454.\n",
      "Prediction Step: 455.\n",
      "Prediction Step: 456.\n",
      "Prediction Step: 457.\n",
      "Prediction Step: 458.\n",
      "Prediction Step: 459.\n",
      "Prediction Step: 460.\n",
      "Prediction Step: 461.\n",
      "Prediction Step: 462.\n",
      "Prediction Step: 463.\n",
      "Prediction Step: 464.\n",
      "Prediction Step: 465.\n",
      "Prediction Step: 466.\n",
      "Prediction Step: 467.\n",
      "Prediction Step: 468.\n",
      "Prediction Step: 469.\n",
      "Prediction Step: 470.\n",
      "Prediction Step: 471.\n",
      "Prediction Step: 472.\n",
      "Prediction Step: 473.\n",
      "Prediction Step: 474.\n",
      "Prediction Step: 475.\n",
      "Prediction Step: 476.\n",
      "Prediction Step: 477.\n",
      "Prediction Step: 478.\n",
      "Prediction Step: 479.\n",
      "Prediction Step: 480.\n",
      "Prediction Step: 481.\n",
      "Prediction Step: 482.\n",
      "Prediction Step: 483.\n",
      "Prediction Step: 484.\n",
      "Prediction Step: 485.\n",
      "Prediction Step: 486.\n",
      "Prediction Step: 487.\n",
      "Prediction Step: 488.\n",
      "Prediction Step: 489.\n",
      "Prediction Step: 490.\n",
      "Prediction Step: 491.\n",
      "Prediction Step: 492.\n",
      "Prediction Step: 493.\n",
      "Prediction Step: 494.\n",
      "Prediction Step: 495.\n",
      "Prediction Step: 496.\n",
      "Prediction Step: 497.\n",
      "Prediction Step: 498.\n",
      "Prediction Step: 499.\n",
      "Prediction Step: 500.\n",
      "Prediction Step: 501.\n",
      "Prediction Step: 502.\n",
      "Prediction Step: 503.\n",
      "Prediction Step: 504.\n",
      "Prediction Step: 505.\n",
      "Prediction Step: 506.\n",
      "Prediction Step: 507.\n",
      "Prediction Step: 508.\n",
      "Prediction Step: 509.\n",
      "Prediction Step: 510.\n",
      "Prediction Step: 511.\n",
      "Prediction Step: 512.\n",
      "Prediction Step: 513.\n",
      "Prediction Step: 514.\n",
      "Prediction Step: 515.\n",
      "Prediction Step: 516.\n",
      "Prediction Step: 517.\n",
      "Prediction Step: 518.\n",
      "Prediction Step: 519.\n",
      "Prediction Step: 520.\n",
      "Prediction Step: 521.\n",
      "Prediction Step: 522.\n",
      "Prediction Step: 523.\n",
      "Prediction Step: 524.\n",
      "Prediction Step: 525.\n",
      "Prediction Step: 526.\n",
      "Prediction Step: 527.\n",
      "Prediction Step: 528.\n",
      "Prediction Step: 529.\n",
      "Prediction Step: 530.\n",
      "Prediction Step: 531.\n",
      "Prediction Step: 532.\n",
      "Prediction Step: 533.\n",
      "Prediction Step: 534.\n",
      "Prediction Step: 535.\n",
      "Prediction Step: 536.\n",
      "Prediction Step: 537.\n",
      "Prediction Step: 538.\n",
      "Prediction Step: 539.\n",
      "Prediction Step: 540.\n",
      "Prediction Step: 541.\n",
      "Prediction Step: 542.\n",
      "Prediction Step: 543.\n",
      "Prediction Step: 544.\n",
      "Prediction Step: 545.\n",
      "Prediction Step: 546.\n",
      "Prediction Step: 547.\n",
      "Prediction Step: 548.\n",
      "Prediction Step: 549.\n",
      "Prediction Step: 550.\n",
      "Prediction Step: 551.\n",
      "Prediction Step: 552.\n",
      "Prediction Step: 553.\n",
      "Prediction Step: 554.\n",
      "Prediction Step: 555.\n",
      "Prediction Step: 556.\n",
      "Prediction Step: 557.\n",
      "Prediction Step: 558.\n",
      "Prediction Step: 559.\n",
      "Prediction Step: 560.\n",
      "Prediction Step: 561.\n",
      "Prediction Step: 562.\n",
      "Prediction Step: 563.\n",
      "Prediction Step: 564.\n",
      "Prediction Step: 565.\n",
      "Prediction Step: 566.\n",
      "Prediction Step: 567.\n",
      "Prediction Step: 568.\n",
      "Prediction Step: 569.\n",
      "Prediction Step: 570.\n",
      "Prediction Step: 571.\n",
      "Prediction Step: 572.\n",
      "Prediction Step: 573.\n",
      "Prediction Step: 574.\n",
      "Prediction Step: 575.\n",
      "Prediction Step: 576.\n",
      "Prediction Step: 577.\n",
      "Prediction Step: 578.\n",
      "Prediction Step: 579.\n",
      "Prediction Step: 580.\n",
      "Prediction Step: 581.\n",
      "Prediction Step: 582.\n",
      "Prediction Step: 583.\n",
      "Prediction Step: 584.\n",
      "Prediction Step: 585.\n",
      "Prediction Step: 586.\n",
      "Prediction Step: 587.\n",
      "Prediction Step: 588.\n",
      "Prediction Step: 589.\n",
      "Prediction Step: 590.\n",
      "Prediction Step: 591.\n",
      "Prediction Step: 592.\n",
      "Prediction Step: 593.\n",
      "Prediction Step: 594.\n",
      "Prediction Step: 595.\n",
      "Prediction Step: 596.\n",
      "Prediction Step: 597.\n",
      "Prediction Step: 598.\n",
      "Prediction Step: 599.\n",
      "Prediction Step: 600.\n",
      "Prediction Step: 601.\n",
      "Prediction Step: 602.\n",
      "Prediction Step: 603.\n",
      "Prediction Step: 604.\n",
      "Prediction Step: 605.\n",
      "Prediction Step: 606.\n",
      "Prediction Step: 607.\n",
      "Prediction Step: 608.\n",
      "Prediction Step: 609.\n",
      "Prediction Step: 610.\n",
      "Prediction Step: 611.\n",
      "Prediction Step: 612.\n",
      "Prediction Step: 613.\n",
      "Prediction Step: 614.\n",
      "Prediction Step: 615.\n",
      "Prediction Step: 616.\n",
      "Prediction Step: 617.\n",
      "Prediction Step: 618.\n",
      "Prediction Step: 619.\n",
      "Prediction Step: 620.\n",
      "Prediction Step: 621.\n",
      "Prediction Step: 622.\n",
      "Prediction Step: 623.\n",
      "Prediction Step: 624.\n",
      "Prediction Step: 625.\n",
      "Prediction Step: 626.\n",
      "Prediction Step: 627.\n",
      "Prediction Step: 628.\n",
      "Prediction Step: 629.\n",
      "Prediction Step: 630.\n",
      "Prediction Step: 631.\n",
      "Prediction Step: 632.\n",
      "Prediction Step: 633.\n",
      "Prediction Step: 634.\n",
      "Prediction Step: 635.\n",
      "Prediction Step: 636.\n",
      "Prediction Step: 637.\n",
      "Prediction Step: 638.\n",
      "Prediction Step: 639.\n",
      "Prediction Step: 640.\n",
      "Prediction Step: 641.\n",
      "Prediction Step: 642.\n",
      "Prediction Step: 643.\n",
      "Prediction Step: 644.\n",
      "Prediction Step: 645.\n",
      "Prediction Step: 646.\n",
      "Prediction Step: 647.\n",
      "Prediction Step: 648.\n",
      "Prediction Step: 649.\n",
      "Prediction Step: 650.\n",
      "Prediction Step: 651.\n",
      "Prediction Step: 652.\n",
      "Prediction Step: 653.\n",
      "Prediction Step: 654.\n",
      "Prediction Step: 655.\n",
      "Prediction Step: 656.\n",
      "Prediction Step: 657.\n",
      "Prediction Step: 658.\n",
      "Prediction Step: 659.\n",
      "Prediction Step: 660.\n",
      "Prediction Step: 661.\n",
      "Prediction Step: 662.\n",
      "Prediction Step: 663.\n",
      "Prediction Step: 664.\n",
      "Prediction Step: 665.\n",
      "Prediction Step: 666.\n",
      "Prediction Step: 667.\n",
      "Prediction Step: 668.\n",
      "Prediction Step: 669.\n",
      "Prediction Step: 670.\n",
      "Prediction Step: 671.\n",
      "Prediction Step: 672.\n",
      "Prediction Step: 673.\n",
      "Prediction Step: 674.\n",
      "Prediction Step: 675.\n",
      "Prediction Step: 676.\n",
      "Prediction Step: 677.\n",
      "Prediction Step: 678.\n",
      "Prediction Step: 679.\n",
      "Prediction Step: 680.\n",
      "Prediction Step: 681.\n",
      "Prediction Step: 682.\n",
      "Prediction Step: 683.\n",
      "Prediction Step: 684.\n",
      "Prediction Step: 685.\n",
      "Prediction Step: 686.\n",
      "Prediction Step: 687.\n",
      "Prediction Step: 688.\n",
      "Prediction Step: 689.\n",
      "Prediction Step: 690.\n",
      "Prediction Step: 691.\n",
      "Prediction Step: 692.\n",
      "Prediction Step: 693.\n",
      "Prediction Step: 694.\n",
      "Prediction Step: 695.\n",
      "Prediction Step: 696.\n",
      "Prediction Step: 697.\n",
      "Prediction Step: 698.\n",
      "Prediction Step: 699.\n",
      "Prediction Step: 700.\n",
      "Prediction Step: 701.\n",
      "Prediction Step: 702.\n",
      "Prediction Step: 703.\n",
      "Prediction Step: 704.\n",
      "Prediction Step: 705.\n",
      "Prediction Step: 706.\n",
      "Prediction Step: 707.\n",
      "Prediction Step: 708.\n",
      "Prediction Step: 709.\n",
      "Prediction Step: 710.\n",
      "Prediction Step: 711.\n",
      "Prediction Step: 712.\n",
      "Prediction Step: 713.\n",
      "Prediction Step: 714.\n",
      "Prediction Step: 715.\n",
      "Prediction Step: 716.\n",
      "Prediction Step: 717.\n",
      "Prediction Step: 718.\n",
      "Prediction Step: 719.\n",
      "Prediction Step: 720.\n",
      "Prediction Step: 721.\n",
      "Prediction Step: 722.\n",
      "Prediction Step: 723.\n",
      "Prediction Step: 724.\n",
      "Prediction Step: 725.\n",
      "Prediction Step: 726.\n",
      "Prediction Step: 727.\n",
      "Prediction Step: 728.\n",
      "Prediction Step: 729.\n",
      "Prediction Step: 730.\n",
      "Prediction Step: 731.\n",
      "Prediction Step: 732.\n",
      "Prediction Step: 733.\n",
      "Prediction Step: 734.\n",
      "Prediction Step: 735.\n",
      "Prediction Step: 736.\n",
      "Prediction Step: 737.\n",
      "Prediction Step: 738.\n",
      "Prediction Step: 739.\n",
      "Prediction Step: 740.\n",
      "Prediction Step: 741.\n",
      "Prediction Step: 742.\n",
      "Prediction Step: 743.\n",
      "Prediction Step: 744.\n",
      "Prediction Step: 745.\n",
      "Prediction Step: 746.\n",
      "Prediction Step: 747.\n",
      "Prediction Step: 748.\n",
      "Prediction Step: 749.\n",
      "Prediction Step: 750.\n",
      "Prediction Step: 751.\n",
      "Prediction Step: 752.\n",
      "Prediction Step: 753.\n",
      "Prediction Step: 754.\n",
      "Prediction Step: 755.\n",
      "Prediction Step: 756.\n",
      "Prediction Step: 757.\n",
      "Prediction Step: 758.\n",
      "Prediction Step: 759.\n",
      "Prediction Step: 760.\n",
      "Prediction Step: 761.\n",
      "Prediction Step: 762.\n",
      "Prediction Step: 763.\n",
      "Prediction Step: 764.\n",
      "Prediction Step: 765.\n",
      "Prediction Step: 766.\n",
      "Prediction Step: 767.\n",
      "Prediction Step: 768.\n",
      "Prediction Step: 769.\n",
      "Prediction Step: 770.\n",
      "Prediction Step: 771.\n",
      "Prediction Step: 772.\n",
      "Prediction Step: 773.\n",
      "Prediction Step: 774.\n",
      "Prediction Step: 775.\n",
      "Prediction Step: 776.\n",
      "Prediction Step: 777.\n",
      "Prediction Step: 778.\n",
      "Prediction Step: 779.\n",
      "Prediction Step: 780.\n",
      "Prediction Step: 781.\n",
      "Prediction Step: 782.\n",
      "Prediction Step: 783.\n",
      "Prediction Step: 784.\n",
      "Prediction Step: 785.\n",
      "Prediction Step: 786.\n",
      "Prediction Step: 787.\n",
      "Prediction Step: 788.\n",
      "Prediction Step: 789.\n",
      "Prediction Step: 790.\n",
      "Prediction Step: 791.\n",
      "Prediction Step: 792.\n",
      "Prediction Step: 793.\n",
      "Prediction Step: 794.\n",
      "Prediction Step: 795.\n",
      "Prediction Step: 796.\n",
      "Prediction Step: 797.\n",
      "Prediction Step: 798.\n",
      "Prediction Step: 799.\n",
      "Prediction Step: 800.\n",
      "Prediction Step: 801.\n",
      "Prediction Step: 802.\n",
      "Prediction Step: 803.\n",
      "Prediction Step: 804.\n",
      "Prediction Step: 805.\n",
      "Prediction Step: 806.\n",
      "Prediction Step: 807.\n",
      "Prediction Step: 808.\n",
      "Prediction Step: 809.\n",
      "Prediction Step: 810.\n",
      "Prediction Step: 811.\n",
      "Prediction Step: 812.\n",
      "Prediction Step: 813.\n",
      "Prediction Step: 814.\n",
      "Prediction Step: 815.\n",
      "Prediction Step: 816.\n",
      "Prediction Step: 817.\n",
      "Prediction Step: 818.\n",
      "Prediction Step: 819.\n",
      "Prediction Step: 820.\n",
      "Prediction Step: 821.\n",
      "Prediction Step: 822.\n",
      "Prediction Step: 823.\n",
      "Prediction Step: 824.\n",
      "Prediction Step: 825.\n",
      "Prediction Step: 826.\n",
      "Prediction Step: 827.\n",
      "Prediction Step: 828.\n",
      "Prediction Step: 829.\n",
      "Prediction Step: 830.\n",
      "Prediction Step: 831.\n",
      "Prediction Step: 832.\n",
      "Prediction Step: 833.\n",
      "Prediction Step: 834.\n",
      "Prediction Step: 835.\n",
      "Prediction Step: 836.\n",
      "Prediction Step: 837.\n",
      "Prediction Step: 838.\n",
      "Prediction Step: 839.\n",
      "Prediction Step: 840.\n",
      "Prediction Step: 841.\n",
      "Prediction Step: 842.\n",
      "Prediction Step: 843.\n",
      "Prediction Step: 844.\n",
      "Prediction Step: 845.\n",
      "Prediction Step: 846.\n",
      "Prediction Step: 847.\n",
      "Prediction Step: 848.\n",
      "Prediction Step: 849.\n",
      "Prediction Step: 850.\n",
      "Prediction Step: 851.\n",
      "Prediction Step: 852.\n",
      "Prediction Step: 853.\n",
      "Prediction Step: 854.\n",
      "Prediction Step: 855.\n",
      "Prediction Step: 856.\n",
      "Prediction Step: 857.\n",
      "Prediction Step: 858.\n",
      "Prediction Step: 859.\n",
      "Prediction Step: 860.\n",
      "Prediction Step: 861.\n",
      "Prediction Step: 862.\n",
      "Prediction Step: 863.\n",
      "Prediction Step: 864.\n",
      "Prediction Step: 865.\n",
      "Prediction Step: 866.\n",
      "Prediction Step: 867.\n",
      "Prediction Step: 868.\n",
      "Prediction Step: 869.\n",
      "Prediction Step: 870.\n",
      "Prediction Step: 871.\n",
      "Prediction Step: 872.\n",
      "Prediction Step: 873.\n",
      "Prediction Step: 874.\n",
      "Prediction Step: 875.\n",
      "Prediction Step: 876.\n",
      "Prediction Step: 877.\n",
      "Prediction Step: 878.\n",
      "Prediction Step: 879.\n",
      "Prediction Step: 880.\n",
      "Prediction Step: 881.\n",
      "Prediction Step: 882.\n",
      "Prediction Step: 883.\n",
      "Prediction Step: 884.\n",
      "Prediction Step: 885.\n",
      "Prediction Step: 886.\n",
      "Prediction Step: 887.\n",
      "Prediction Step: 888.\n",
      "Prediction Step: 889.\n",
      "Prediction Step: 890.\n",
      "Prediction Step: 891.\n",
      "Prediction Step: 892.\n",
      "Prediction Step: 893.\n",
      "Prediction Step: 894.\n",
      "Prediction Step: 895.\n",
      "Prediction Step: 896.\n",
      "Prediction Step: 897.\n",
      "Prediction Step: 898.\n",
      "Prediction Step: 899.\n",
      "Prediction Step: 900.\n",
      "Prediction Step: 901.\n",
      "Prediction Step: 902.\n",
      "Prediction Step: 903.\n",
      "Prediction Step: 904.\n",
      "Prediction Step: 905.\n",
      "Prediction Step: 906.\n",
      "Prediction Step: 907.\n",
      "Prediction Step: 908.\n",
      "Prediction Step: 909.\n",
      "Prediction Step: 910.\n",
      "Prediction Step: 911.\n",
      "Prediction Step: 912.\n",
      "Prediction Step: 913.\n",
      "Prediction Step: 914.\n",
      "Prediction Step: 915.\n",
      "Prediction Step: 916.\n",
      "Prediction Step: 917.\n",
      "Prediction Step: 918.\n",
      "Prediction Step: 919.\n",
      "Prediction Step: 920.\n",
      "Prediction Step: 921.\n",
      "Prediction Step: 922.\n",
      "Prediction Step: 923.\n",
      "Prediction Step: 924.\n",
      "Prediction Step: 925.\n",
      "Prediction Step: 926.\n",
      "Prediction Step: 927.\n",
      "Prediction Step: 928.\n",
      "Prediction Step: 929.\n",
      "Prediction Step: 930.\n",
      "Prediction Step: 931.\n",
      "Prediction Step: 932.\n",
      "Prediction Step: 933.\n",
      "Prediction Step: 934.\n",
      "Prediction Step: 935.\n",
      "Prediction Step: 936.\n",
      "Prediction Step: 937.\n",
      "Prediction Step: 938.\n",
      "Prediction Step: 939.\n",
      "Prediction Step: 940.\n",
      "Prediction Step: 941.\n",
      "Prediction Step: 942.\n",
      "Prediction Step: 943.\n",
      "Prediction Step: 944.\n",
      "Prediction Step: 945.\n",
      "Prediction Step: 946.\n",
      "Prediction Step: 947.\n",
      "Prediction Step: 948.\n",
      "Prediction Step: 949.\n",
      "Prediction Step: 950.\n",
      "Prediction Step: 951.\n",
      "Prediction Step: 952.\n",
      "Prediction Step: 953.\n",
      "Prediction Step: 954.\n",
      "Prediction Step: 955.\n",
      "Prediction Step: 956.\n",
      "Prediction Step: 957.\n",
      "Prediction Step: 958.\n",
      "Prediction Step: 959.\n",
      "Prediction Step: 960.\n",
      "Prediction Step: 961.\n",
      "Prediction Step: 962.\n",
      "Prediction Step: 963.\n",
      "Prediction Step: 964.\n",
      "Prediction Step: 965.\n",
      "Prediction Step: 966.\n",
      "Prediction Step: 967.\n",
      "Prediction Step: 968.\n",
      "Prediction Step: 969.\n",
      "Prediction Step: 970.\n",
      "Prediction Step: 971.\n",
      "Prediction Step: 972.\n",
      "Prediction Step: 973.\n",
      "Prediction Step: 974.\n",
      "Prediction Step: 975.\n",
      "Prediction Step: 976.\n",
      "Prediction Step: 977.\n",
      "Prediction Step: 978.\n",
      "Prediction Step: 979.\n",
      "Prediction Step: 980.\n",
      "Prediction Step: 981.\n",
      "Prediction Step: 982.\n",
      "Prediction Step: 983.\n",
      "Prediction Step: 984.\n",
      "Prediction Step: 985.\n",
      "Prediction Step: 986.\n",
      "Prediction Step: 987.\n",
      "Prediction Step: 988.\n",
      "Prediction Step: 989.\n",
      "Prediction Step: 990.\n",
      "Prediction Step: 991.\n",
      "Prediction Step: 992.\n",
      "Prediction Step: 993.\n",
      "Prediction Step: 994.\n",
      "Prediction Step: 995.\n",
      "Prediction Step: 996.\n",
      "Prediction Step: 997.\n",
      "Prediction Step: 998.\n",
      "Prediction Step: 999.\n",
      "Prediction Step: 1000.\n",
      "Prediction Step: 1001.\n",
      "Prediction Step: 1002.\n",
      "Prediction Step: 1003.\n",
      "Prediction Step: 1004.\n",
      "Prediction Step: 1005.\n",
      "Prediction Step: 1006.\n",
      "Prediction Step: 1007.\n",
      "Prediction Step: 1008.\n",
      "Prediction Step: 1009.\n",
      "Prediction Step: 1010.\n",
      "Prediction Step: 1011.\n",
      "Prediction Step: 1012.\n",
      "Prediction Step: 1013.\n",
      "Prediction Step: 1014.\n",
      "Prediction Step: 1015.\n",
      "Prediction Step: 1016.\n",
      "Prediction Step: 1017.\n",
      "Prediction Step: 1018.\n",
      "Prediction Step: 1019.\n",
      "Prediction Step: 1020.\n",
      "Prediction Step: 1021.\n",
      "Prediction Step: 1022.\n",
      "Prediction Step: 1023.\n",
      "Prediction Step: 1024.\n",
      "Prediction Step: 1025.\n",
      "Prediction Step: 1026.\n",
      "Prediction Step: 1027.\n",
      "Prediction Step: 1028.\n",
      "Prediction Step: 1029.\n",
      "Prediction Step: 1030.\n",
      "Prediction Step: 1031.\n",
      "Prediction Step: 1032.\n",
      "Prediction Step: 1033.\n",
      "Prediction Step: 1034.\n",
      "Prediction Step: 1035.\n",
      "Prediction Step: 1036.\n",
      "Prediction Step: 1037.\n",
      "Prediction Step: 1038.\n",
      "Prediction Step: 1039.\n",
      "Prediction Step: 1040.\n",
      "Prediction Step: 1041.\n",
      "Prediction Step: 1042.\n",
      "Prediction Step: 1043.\n",
      "Prediction Step: 1044.\n",
      "Prediction Step: 1045.\n",
      "Prediction Step: 1046.\n",
      "Prediction Step: 1047.\n",
      "Prediction Step: 1048.\n",
      "Prediction Step: 1049.\n",
      "Prediction Step: 1050.\n",
      "Prediction Step: 1051.\n",
      "Prediction Step: 1052.\n",
      "Prediction Step: 1053.\n",
      "Prediction Step: 1054.\n",
      "Prediction Step: 1055.\n",
      "Prediction Step: 1056.\n",
      "Prediction Step: 1057.\n",
      "Prediction Step: 1058.\n",
      "Prediction Step: 1059.\n",
      "Prediction Step: 1060.\n",
      "Prediction Step: 1061.\n",
      "Prediction Step: 1062.\n",
      "Prediction Step: 1063.\n",
      "Prediction Step: 1064.\n",
      "Prediction Step: 1065.\n",
      "Prediction Step: 1066.\n",
      "Prediction Step: 1067.\n",
      "Prediction Step: 1068.\n",
      "Prediction Step: 1069.\n",
      "Prediction Step: 1070.\n",
      "Prediction Step: 1071.\n",
      "Prediction Step: 1072.\n",
      "Prediction Step: 1073.\n",
      "Prediction Step: 1074.\n",
      "Prediction Step: 1075.\n",
      "Prediction Step: 1076.\n",
      "Prediction Step: 1077.\n",
      "Prediction Step: 1078.\n",
      "Prediction Step: 1079.\n",
      "Prediction Step: 1080.\n",
      "Prediction Step: 1081.\n",
      "Prediction Step: 1082.\n",
      "Prediction Step: 1083.\n",
      "Prediction Step: 1084.\n",
      "Prediction Step: 1085.\n",
      "Prediction Step: 1086.\n",
      "Prediction Step: 1087.\n",
      "Prediction Step: 1088.\n",
      "Prediction Step: 1089.\n",
      "Prediction Step: 1090.\n",
      "Prediction Step: 1091.\n",
      "Prediction Step: 1092.\n",
      "Prediction Step: 1093.\n",
      "Prediction Step: 1094.\n",
      "Prediction Step: 1095.\n",
      "Prediction Step: 1096.\n",
      "Prediction Step: 1097.\n",
      "Prediction Step: 1098.\n",
      "Prediction Step: 1099.\n",
      "Prediction Step: 1100.\n",
      "Prediction Step: 1101.\n",
      "Prediction Step: 1102.\n",
      "Prediction Step: 1103.\n",
      "Prediction Step: 1104.\n",
      "Prediction Step: 1105.\n",
      "Prediction Step: 1106.\n",
      "Prediction Step: 1107.\n",
      "Prediction Step: 1108.\n",
      "Prediction Step: 1109.\n",
      "Prediction Step: 1110.\n",
      "Prediction Step: 1111.\n",
      "Prediction Step: 1112.\n",
      "Prediction Step: 1113.\n",
      "Prediction Step: 1114.\n",
      "Prediction Step: 1115.\n",
      "Prediction Step: 1116.\n",
      "Prediction Step: 1117.\n",
      "Prediction Step: 1118.\n",
      "Prediction Step: 1119.\n",
      "Prediction Step: 1120.\n",
      "Prediction Step: 1121.\n",
      "Prediction Step: 1122.\n",
      "Prediction Step: 1123.\n",
      "Prediction Step: 1124.\n",
      "Prediction Step: 1125.\n",
      "Prediction Step: 1126.\n",
      "Prediction Step: 1127.\n",
      "Prediction Step: 1128.\n",
      "Prediction Step: 1129.\n",
      "Prediction Step: 1130.\n",
      "Prediction Step: 1131.\n",
      "Prediction Step: 1132.\n",
      "Prediction Step: 1133.\n",
      "Prediction Step: 1134.\n",
      "Prediction Step: 1135.\n",
      "Prediction Step: 1136.\n",
      "Prediction Step: 1137.\n",
      "Prediction Step: 1138.\n",
      "Prediction Step: 1139.\n",
      "Prediction Step: 1140.\n",
      "Prediction Step: 1141.\n",
      "Prediction Step: 1142.\n",
      "Prediction Step: 1143.\n",
      "Prediction Step: 1144.\n",
      "Prediction Step: 1145.\n",
      "Prediction Step: 1146.\n",
      "Prediction Step: 1147.\n",
      "Prediction Step: 1148.\n",
      "Prediction Step: 1149.\n",
      "Prediction Step: 1150.\n",
      "Prediction Step: 1151.\n",
      "Prediction Step: 1152.\n",
      "Prediction Step: 1153.\n",
      "Prediction Step: 1154.\n",
      "Prediction Step: 1155.\n",
      "Prediction Step: 1156.\n",
      "Prediction Step: 1157.\n",
      "Prediction Step: 1158.\n",
      "Prediction Step: 1159.\n",
      "Prediction Step: 1160.\n",
      "Prediction Step: 1161.\n",
      "Prediction Step: 1162.\n",
      "Prediction Step: 1163.\n",
      "Prediction Step: 1164.\n",
      "Prediction Step: 1165.\n",
      "Prediction Step: 1166.\n",
      "Prediction Step: 1167.\n",
      "Prediction Step: 1168.\n",
      "Prediction Step: 1169.\n",
      "Prediction Step: 1170.\n",
      "Prediction Step: 1171.\n",
      "Prediction Step: 1172.\n",
      "Prediction Step: 1173.\n",
      "Prediction Step: 1174.\n",
      "Prediction Step: 1175.\n",
      "Prediction Step: 1176.\n",
      "Prediction Step: 1177.\n",
      "Prediction Step: 1178.\n",
      "Prediction Step: 1179.\n",
      "Prediction Step: 1180.\n",
      "Prediction Step: 1181.\n",
      "Prediction Step: 1182.\n",
      "Prediction Step: 1183.\n",
      "Prediction Step: 1184.\n",
      "Prediction Step: 1185.\n",
      "Prediction Step: 1186.\n",
      "Prediction Step: 1187.\n",
      "Prediction Step: 1188.\n",
      "Prediction Step: 1189.\n",
      "Prediction Step: 1190.\n",
      "Prediction Step: 1191.\n",
      "Prediction Step: 1192.\n",
      "Prediction Step: 1193.\n",
      "Prediction Step: 1194.\n",
      "Prediction Step: 1195.\n",
      "Prediction Step: 1196.\n",
      "Prediction Step: 1197.\n",
      "Prediction Step: 1198.\n",
      "Prediction Step: 1199.\n",
      "Prediction Step: 1200.\n",
      "Prediction Step: 1201.\n",
      "Prediction Step: 1202.\n",
      "Prediction Step: 1203.\n",
      "Prediction Step: 1204.\n",
      "Prediction Step: 1205.\n",
      "Prediction Step: 1206.\n",
      "Prediction Step: 1207.\n",
      "Prediction Step: 1208.\n",
      "Prediction Step: 1209.\n",
      "Prediction Step: 1210.\n",
      "Prediction Step: 1211.\n",
      "Prediction Step: 1212.\n",
      "Prediction Step: 1213.\n",
      "Prediction Step: 1214.\n",
      "Prediction Step: 1215.\n",
      "Prediction Step: 1216.\n",
      "Prediction Step: 1217.\n",
      "Prediction Step: 1218.\n",
      "Prediction Step: 1219.\n",
      "Prediction Step: 1220.\n",
      "Prediction Step: 1221.\n",
      "Prediction Step: 1222.\n",
      "Prediction Step: 1223.\n",
      "Prediction Step: 1224.\n",
      "Prediction Step: 1225.\n",
      "Prediction Step: 1226.\n",
      "Prediction Step: 1227.\n",
      "Prediction Step: 1228.\n",
      "Prediction Step: 1229.\n",
      "Prediction Step: 1230.\n",
      "Prediction Step: 1231.\n",
      "Prediction Step: 1232.\n",
      "Prediction Step: 1233.\n",
      "Prediction Step: 1234.\n",
      "Prediction Step: 1235.\n",
      "Prediction Step: 1236.\n",
      "Prediction Step: 1237.\n",
      "Prediction Step: 1238.\n",
      "Prediction Step: 1239.\n",
      "Prediction Step: 1240.\n",
      "Prediction Step: 1241.\n",
      "Prediction Step: 1242.\n",
      "Prediction Step: 1243.\n",
      "Prediction Step: 1244.\n",
      "Prediction Step: 1245.\n",
      "Prediction Step: 1246.\n",
      "Prediction Step: 1247.\n",
      "Prediction Step: 1248.\n",
      "Prediction Step: 1249.\n",
      "Prediction Step: 1250.\n",
      "Prediction Step: 1251.\n",
      "Prediction Step: 1252.\n",
      "Prediction Step: 1253.\n",
      "Prediction Step: 1254.\n",
      "Prediction Step: 1255.\n",
      "Prediction Step: 1256.\n",
      "Prediction Step: 1257.\n",
      "Prediction Step: 1258.\n",
      "Prediction Step: 1259.\n",
      "Prediction Step: 1260.\n",
      "Prediction Step: 1261.\n",
      "Prediction Step: 1262.\n",
      "Prediction Step: 1263.\n",
      "Prediction Step: 1264.\n",
      "Prediction Step: 1265.\n",
      "Prediction Step: 1266.\n",
      "Prediction Step: 1267.\n",
      "Prediction Step: 1268.\n",
      "Prediction Step: 1269.\n",
      "Prediction Step: 1270.\n",
      "Prediction Step: 1271.\n",
      "Prediction Step: 1272.\n",
      "Prediction Step: 1273.\n",
      "Prediction Step: 1274.\n",
      "Prediction Step: 1275.\n",
      "Prediction Step: 1276.\n",
      "Prediction Step: 1277.\n",
      "Prediction Step: 1278.\n",
      "Prediction Step: 1279.\n",
      "Prediction Step: 1280.\n",
      "Prediction Step: 1281.\n",
      "Prediction Step: 1282.\n",
      "Prediction Step: 1283.\n",
      "Prediction Step: 1284.\n",
      "Prediction Step: 1285.\n",
      "Prediction Step: 1286.\n",
      "Prediction Step: 1287.\n",
      "Prediction Step: 1288.\n",
      "Prediction Step: 1289.\n",
      "Prediction Step: 1290.\n",
      "Prediction Step: 1291.\n",
      "Prediction Step: 1292.\n",
      "Prediction Step: 1293.\n",
      "Prediction Step: 1294.\n",
      "Prediction Step: 1295.\n",
      "Prediction Step: 1296.\n",
      "Prediction Step: 1297.\n",
      "Prediction Step: 1298.\n",
      "Prediction Step: 1299.\n",
      "Prediction Step: 1300.\n",
      "Prediction Step: 1301.\n",
      "Prediction Step: 1302.\n",
      "Prediction Step: 1303.\n",
      "Prediction Step: 1304.\n",
      "Prediction Step: 1305.\n",
      "Prediction Step: 1306.\n",
      "Prediction Step: 1307.\n",
      "Prediction Step: 1308.\n",
      "Prediction Step: 1309.\n",
      "Prediction Step: 1310.\n",
      "Prediction Step: 1311.\n",
      "Prediction Step: 1312.\n",
      "Prediction Step: 1313.\n",
      "Prediction Step: 1314.\n",
      "Prediction Step: 1315.\n",
      "Prediction Step: 1316.\n",
      "Prediction Step: 1317.\n",
      "Prediction Step: 1318.\n",
      "Prediction Step: 1319.\n",
      "Prediction Step: 1320.\n",
      "Prediction Step: 1321.\n",
      "Prediction Step: 1322.\n",
      "Prediction Step: 1323.\n",
      "Prediction Step: 1324.\n",
      "Prediction Step: 1325.\n",
      "Prediction Step: 1326.\n",
      "Prediction Step: 1327.\n",
      "Prediction Step: 1328.\n",
      "Prediction Step: 1329.\n",
      "Prediction Step: 1330.\n",
      "Prediction Step: 1331.\n",
      "Prediction Step: 1332.\n",
      "Prediction Step: 1333.\n",
      "Prediction Step: 1334.\n",
      "Prediction Step: 1335.\n",
      "Prediction Step: 1336.\n",
      "Prediction Step: 1337.\n",
      "Prediction Step: 1338.\n",
      "Prediction Step: 1339.\n",
      "Prediction Step: 1340.\n",
      "Prediction Step: 1341.\n",
      "Prediction Step: 1342.\n",
      "Prediction Step: 1343.\n",
      "Prediction Step: 1344.\n",
      "Prediction Step: 1345.\n",
      "Prediction Step: 1346.\n",
      "Prediction Step: 1347.\n",
      "Prediction Step: 1348.\n",
      "Prediction Step: 1349.\n",
      "Prediction Step: 1350.\n",
      "Prediction Step: 1351.\n",
      "Prediction Step: 1352.\n",
      "Prediction Step: 1353.\n",
      "Prediction Step: 1354.\n",
      "Prediction Step: 1355.\n",
      "Prediction Step: 1356.\n",
      "Prediction Step: 1357.\n",
      "Prediction Step: 1358.\n",
      "Prediction Step: 1359.\n",
      "Prediction Step: 1360.\n",
      "Prediction Step: 1361.\n",
      "Prediction Step: 1362.\n",
      "Prediction Step: 1363.\n",
      "Prediction Step: 1364.\n",
      "Prediction Step: 1365.\n",
      "Prediction Step: 1366.\n",
      "Prediction Step: 1367.\n",
      "Prediction Step: 1368.\n",
      "Prediction Step: 1369.\n",
      "Prediction Step: 1370.\n",
      "Prediction Step: 1371.\n",
      "Prediction Step: 1372.\n",
      "Prediction Step: 1373.\n",
      "Prediction Step: 1374.\n",
      "Prediction Step: 1375.\n",
      "Prediction Step: 1376.\n",
      "Prediction Step: 1377.\n",
      "Prediction Step: 1378.\n",
      "Prediction Step: 1379.\n",
      "Prediction Step: 1380.\n",
      "Prediction Step: 1381.\n",
      "Prediction Step: 1382.\n",
      "Prediction Step: 1383.\n",
      "Prediction Step: 1384.\n",
      "Prediction Step: 1385.\n",
      "Prediction Step: 1386.\n",
      "Prediction Step: 1387.\n",
      "Prediction Step: 1388.\n",
      "Prediction Step: 1389.\n",
      "Prediction Step: 1390.\n",
      "Prediction Step: 1391.\n",
      "Prediction Step: 1392.\n",
      "Prediction Step: 1393.\n",
      "Prediction Step: 1394.\n",
      "Prediction Step: 1395.\n",
      "Prediction Step: 1396.\n",
      "Prediction Step: 1397.\n",
      "Prediction Step: 1398.\n",
      "Prediction Step: 1399.\n",
      "Prediction Step: 1400.\n",
      "Prediction Step: 1401.\n",
      "Prediction Step: 1402.\n",
      "Prediction Step: 1403.\n",
      "Prediction Step: 1404.\n",
      "Prediction Step: 1405.\n",
      "Prediction Step: 1406.\n",
      "Prediction Step: 1407.\n",
      "Prediction Step: 1408.\n",
      "Prediction Step: 1409.\n",
      "Prediction Step: 1410.\n",
      "Prediction Step: 1411.\n",
      "Prediction Step: 1412.\n",
      "Prediction Step: 1413.\n",
      "Prediction Step: 1414.\n",
      "Prediction Step: 1415.\n",
      "Prediction Step: 1416.\n",
      "Prediction Step: 1417.\n",
      "Prediction Step: 1418.\n",
      "Prediction Step: 1419.\n",
      "Prediction Step: 1420.\n",
      "Prediction Step: 1421.\n",
      "Prediction Step: 1422.\n",
      "Prediction Step: 1423.\n",
      "Prediction Step: 1424.\n",
      "Prediction Step: 1425.\n",
      "Prediction Step: 1426.\n",
      "Prediction Step: 1427.\n",
      "Prediction Step: 1428.\n",
      "Prediction Step: 1429.\n",
      "Prediction Step: 1430.\n",
      "Prediction Step: 1431.\n",
      "Prediction Step: 1432.\n",
      "Prediction Step: 1433.\n",
      "Prediction Step: 1434.\n",
      "Prediction Step: 1435.\n",
      "Prediction Step: 1436.\n",
      "Prediction Step: 1437.\n",
      "Prediction Step: 1438.\n",
      "Prediction Step: 1439.\n",
      "Prediction Step: 1440.\n",
      "Prediction Step: 1441.\n",
      "Prediction Step: 1442.\n",
      "Prediction Step: 1443.\n",
      "Prediction Step: 1444.\n",
      "Prediction Step: 1445.\n",
      "Prediction Step: 1446.\n",
      "Prediction Step: 1447.\n",
      "Prediction Step: 1448.\n",
      "Prediction Step: 1449.\n",
      "Prediction Step: 1450.\n",
      "Prediction Step: 1451.\n",
      "Prediction Step: 1452.\n",
      "Prediction Step: 1453.\n",
      "Prediction Step: 1454.\n",
      "Prediction Step: 1455.\n",
      "Prediction Step: 1456.\n",
      "Prediction Step: 1457.\n",
      "Prediction Step: 1458.\n",
      "Prediction Step: 1459.\n",
      "Prediction Step: 1460.\n",
      "Prediction Step: 1461.\n",
      "Prediction Step: 1462.\n",
      "Prediction Step: 1463.\n",
      "Prediction Step: 1464.\n",
      "Prediction Step: 1465.\n",
      "Prediction Step: 1466.\n",
      "Prediction Step: 1467.\n",
      "Prediction Step: 1468.\n",
      "Prediction Step: 1469.\n",
      "Prediction Step: 1470.\n",
      "Prediction Step: 1471.\n",
      "Prediction Step: 1472.\n",
      "Prediction Step: 1473.\n",
      "Prediction Step: 1474.\n",
      "Prediction Step: 1475.\n",
      "Prediction Step: 1476.\n",
      "Prediction Step: 1477.\n",
      "Prediction Step: 1478.\n",
      "Prediction Step: 1479.\n",
      "Prediction Step: 1480.\n",
      "Prediction Step: 1481.\n",
      "Prediction Step: 1482.\n",
      "Prediction Step: 1483.\n",
      "Prediction Step: 1484.\n",
      "Prediction Step: 1485.\n"
     ]
    }
   ],
   "source": [
    "# Run on the Test Data.\n",
    "model.load_from_checkpoint(model_path, \"best_step\")\n",
    "model.to_device()\n",
    "test_dataloader = create_dataloader(model, test_file_name=test_file_name)\n",
    "test_loop(\n",
    "    model=model,\n",
    "    mode=\"test\",\n",
    "    model_path=model_path,\n",
    "    prediction_file_name=\"test.predicted.tsv\",\n",
    "    test_dataloader=test_dataloader,\n",
    "    metric=qa_metric,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
