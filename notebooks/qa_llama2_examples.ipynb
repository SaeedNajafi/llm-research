{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-03-10 11:42:13.583134: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-10 11:42:13.656808: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-10 11:42:13.656863: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-10 11:42:13.663452: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-10 11:42:13.677998: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-10 11:42:15.500791: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from typing import Any, Dict, Iterator, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from bitsandbytes.optim.adamw import PagedAdamW8bit\n",
    "from src.galore_torch import GaLoreAdamW8bit\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    ")\n",
    "\n",
    "from src.base_lm import BaseLM\n",
    "from src.general_utils import DictDataset, test_loop, train_loop\n",
    "from src.model_utils import clear_cache, llama2_log_of_labels, lm_logits, mlm_log_of_labels, set_random_seed\n",
    "from src.general_utils import white_space_fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar 10 11:42:16 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A40          On   | 00000000:86:00.0 Off |                    0 |\n",
      "|  0%   38C    P8    33W / 300W |      2MiB / 46068MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A40          On   | 00000000:D8:00.0 Off |                    0 |\n",
      "|  0%   28C    P8    28W / 300W |      2MiB / 46068MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (0.21.3)\n",
      "Requirement already satisfied: filelock in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from huggingface_hub) (2024.2.0)\n",
      "Requirement already satisfied: requests in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from huggingface_hub) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from huggingface_hub) (4.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from requests->huggingface_hub) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from requests->huggingface_hub) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /h/snajafi/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token=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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 8\n",
    "eval_batch_size = 8\n",
    "lm_input_max_length = 1024\n",
    "lm_output_max_length = 128\n",
    "lm_top_p = 0.9\n",
    "temperature = 0.6\n",
    "metric_device = \"cuda:1\"\n",
    "metric_batch_size = 8\n",
    "learning_rate = 0.00005\n",
    "train_file_name = \"128-shot-datasets/squad/128-42-train.tsv\"\n",
    "dev_file_name = \"128-shot-datasets/squad/128-42-dev.tsv\"\n",
    "test_file_name = \"128-shot-datasets/squad/test.tsv\"\n",
    "\n",
    "# folder to store models and predictions.\n",
    "model_path = \"/scratch/ssd004/scratch/snajafi/checkpoints/llama2-lora\"\n",
    "\n",
    "# related to lora\n",
    "r = 16\n",
    "lora_alpha = 8\n",
    "lora_dropout = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load LM efficiently.\"\"\"\n",
    "\n",
    "# Make sure we have some tokens defined for the LM, if not defined in the model.\n",
    "_EXTRA_TOKENS = {\n",
    "    \"pad_token\": \"<pad>\",\n",
    "    \"mask_token\": \"<mask>\",\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"</s>\",\n",
    "    \"unk_token\": \"<unk>\",\n",
    "    \"cls_token\": \"<CLS>\",\n",
    "}\n",
    "\n",
    "target_modules = [\"q_proj\", \"v_proj\", \"o_proj\", \"k_proj\"]\n",
    "\n",
    "\n",
    "def load_peft_model(\n",
    "    model: PreTrainedModel,\n",
    "    adapter_name: str = \"lora\",\n",
    "    is_trainable: bool = False,\n",
    "    model_type: str = \"causal_lm\",\n",
    "    lora_target_modules: List[str] = target_modules,\n",
    ") -> torch.nn.Module:\n",
    "    \"\"\"Load a trained PEFT adapter to the base model and return the PeftModel.\n",
    "\n",
    "    Args:\n",
    "    ----\n",
    "        model: the main model.\n",
    "        num_quantized_bits: number of bits in the loaded model.\n",
    "        adapter_name: e.g. lora.\n",
    "        is_trainable: train or inference mode.\n",
    "        model_type: causal lm or seq-to-seq.\n",
    "        lora_target_modules: which modules to train with lora.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "        The PEFT model and tokenizer.\n",
    "    \"\"\"\n",
    "    if model_type == \"causal_lm\":\n",
    "        task_type = TaskType.CAUSAL_LM\n",
    "    elif model_type == \"seq_to_seq_lm\":\n",
    "        task_type = TaskType.SEQ_2_SEQ_LM\n",
    "\n",
    "    if adapter_name == \"lora\":\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=task_type,\n",
    "            inference_mode=not is_trainable,\n",
    "            r=r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            lora_dropout=lora_dropout,\n",
    "            bias=\"none\",\n",
    "            init_lora_weights=True,\n",
    "            target_modules=lora_target_modules,\n",
    "        )\n",
    "\n",
    "    peft_model = get_peft_model(model, peft_config)\n",
    "    peft_model.print_trainable_parameters()\n",
    "    return peft_model\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer(\n",
    "    model_id: str, model_type: str, model_dtype: torch.dtype, attn_implementation: str, load_in_4bit: Optional[bool] = True\n",
    ") -> Tuple[PreTrainedModel, PreTrainedTokenizer]:\n",
    "    \"\"\"Load the model and tokenizer.\n",
    "\n",
    "    Args:\n",
    "    ----\n",
    "        model_id: the id for the pre-trained model.\n",
    "        model_type: causal lm or seq_to_seq_lm.\n",
    "        model_dtype: model data type.\n",
    "        load_in_4bit: Whether to load in 4 bit quantization.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "        The model and tokenizer.\n",
    "    \"\"\"\n",
    "    # load model\n",
    "    if model_type == \"causal_lm\":\n",
    "        ModelClass = AutoModelForCausalLM\n",
    "    elif model_type == \"seq_to_seq_lm\":\n",
    "        ModelClass = AutoModelForSeq2SeqLM\n",
    "    model_args: Dict[str, Any] = {\"use_cache\": False, \"torch_dtype\": model_dtype, \"attn_implementation\": attn_implementation}\n",
    "    if load_in_4bit:\n",
    "        quant_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=model_args[\"torch_dtype\"],\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "        model_args[\"quantization_config\"] = quant_config\n",
    "    model = ModelClass.from_pretrained(\n",
    "        model_id,\n",
    "        **model_args,\n",
    "    )\n",
    "\n",
    "    # load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.add_special_tokens(_EXTRA_TOKENS)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        # extend embeddings to a multiple so we use Tensor cores\n",
    "        multiple = 64 if \"A100\" in torch.cuda.get_device_name() else 8\n",
    "        model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=multiple)\n",
    "    else:\n",
    "        raise Exception(\"No CUDA Found!\")\n",
    "\n",
    "    # re-define token ids for the model.\n",
    "    for extra_token_key, extra_token_val in _EXTRA_TOKENS.items():\n",
    "        extra_token_id = tokenizer.convert_tokens_to_ids([extra_token_val])[0]\n",
    "        model.config.__setattr__(f\"{extra_token_key}_id\", extra_token_id)\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llama2QA(BaseLM):\n",
    "    \"\"\"Class to implement Llama2 for QA task.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        mode: str,\n",
    "        device: str,\n",
    "        seed: int = 42,\n",
    "    ) -> None:\n",
    "        super().__init__(device, \"main_lm\", seed)\n",
    "        self.device = device\n",
    "        model, tokenizer = load_model_and_tokenizer(\n",
    "            model_id=\"/model-weights/Llama-2-7b-chat-hf\",\n",
    "            model_type=\"causal_lm\",\n",
    "            model_dtype=torch.bfloat16,\n",
    "            attn_implementation=\"flash_attention_2\",\n",
    "            load_in_4bit=True,\n",
    "        )\n",
    "        peft_model = load_peft_model(\n",
    "            model=model,\n",
    "            adapter_name=\"lora\",\n",
    "            is_trainable=mode == \"train\",\n",
    "            model_type=\"causal_lm\",\n",
    "        )\n",
    "        self.model = peft_model\n",
    "        self.tokenizer = tokenizer\n",
    "        '''\n",
    "        # to train the main lm, we update all of its parameters.\n",
    "        galore_params = []\n",
    "        target_modules_list = [\"attn\", \"mlp\"]\n",
    "        for module_name, module in self.model.named_modules():\n",
    "            if not isinstance(module, torch.nn.Linear):\n",
    "                continue\n",
    "            if not any(target_key in module_name for target_key in target_modules_list):\n",
    "                continue\n",
    "            print('enable GaLore for weights in module: ', module_name)\n",
    "            galore_params.append(module.weight)\n",
    "        id_galore_params = [id(p) for p in galore_params]\n",
    "        # make parameters without \"rank\" to another group\n",
    "        regular_params = [p for p in self.model.parameters() if id(p) not in id_galore_params]\n",
    "        # then call galore_adamw\n",
    "        param_groups = [{'params': regular_params}, \n",
    "                        {'params': galore_params, 'rank': 128, 'update_proj_gap': 16, 'scale': 0.25, 'proj_type': 'std'}]\n",
    "        self.optimizer = GaLoreAdamW8bit(param_groups, lr=learning_rate)\n",
    "        '''\n",
    "        self.optimizer = PagedAdamW8bit(self.model.parameters(), lr=learning_rate)\n",
    "        self.scheduler = CosineAnnealingWarmRestarts(self.optimizer, T_0=10, eta_min=learning_rate / 5.0)\n",
    "\n",
    "    def prepare_text(self, texts: List[str], output_texts: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Convert texts to ids and return the dataset required for training\n",
    "        and inference.\"\"\"\n",
    "        instruction = \"In this task, you are given a context and question. \\\n",
    "            Provide a short phrase as the answer for the given question using only the information from the context. \\\n",
    "            If you do not know the answer from the context, generate 'no_answer' in the output. \\\n",
    "            Do not repeat the question in the output.\"\n",
    "        template = \"<s> [INST] <<SYS>> {instruction} <</SYS>> {input_text} [/INST]\"\n",
    "        # sample of the answers if possible.\n",
    "        sampled_answers = [random.choice(text.split(\"[<@>]\")) for text in output_texts]\n",
    "        answers = [f\"Answer: {answer}\" for answer in sampled_answers]\n",
    "\n",
    "        inputs_for_training = [\n",
    "            white_space_fix(f\"{template.format(instruction=instruction, input_text=texts[idx])} {answers[idx]}\") for idx in range(len(texts))\n",
    "        ]\n",
    "        inputs_for_generation = [white_space_fix(template.format(instruction=instruction, input_text=texts[idx])) for idx in range(len(texts))]\n",
    "\n",
    "        input_encodings = self.tokenizer(\n",
    "            inputs_for_training,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=lm_input_max_length + lm_output_max_length,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        input_encodings_for_generation = self.tokenizer(\n",
    "            inputs_for_generation,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=lm_input_max_length,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        data = {\n",
    "            \"output_texts\": output_texts,\n",
    "            \"lm_input_ids_for_train\": input_encodings.input_ids,\n",
    "            \"lm_attention_mask_for_train\": input_encodings.attention_mask,\n",
    "            \"lm_input_ids_for_generation\": input_encodings_for_generation.input_ids,\n",
    "            \"lm_attention_mask_for_generation\": input_encodings_for_generation.attention_mask,\n",
    "        }\n",
    "        return data\n",
    "\n",
    "    def train(self, batch: torch.utils.data.Dataset) -> torch.Tensor:\n",
    "        \"\"\"Using the Llama2, run a forward computation over the batch, compute\n",
    "        the log probability over the batch.\n",
    "\n",
    "        This will be used for training.\n",
    "        \"\"\"\n",
    "        self.train_mode_on()\n",
    "        loaded_batch = self.data_to_device(batch, keys=[\"lm_input_ids_for_train\", \"lm_attention_mask_for_train\",\n",
    "                                                        \"lm_attention_mask_for_generation\"])\n",
    "        input_ids = loaded_batch[\"lm_input_ids_for_train\"]\n",
    "        attention_mask = loaded_batch[\"lm_attention_mask_for_train\"]\n",
    "        original_len_without_answer = torch.sum(loaded_batch[\"lm_attention_mask_for_generation\"], dim=1)\n",
    "        with torch.set_grad_enabled(True):\n",
    "            logits = lm_logits(\n",
    "                model=self.model,\n",
    "                input_ids=input_ids,\n",
    "                input_mask=attention_mask,\n",
    "            )\n",
    "            batch_size, seq_len = input_ids.size()\n",
    "            masked_labels = input_ids.masked_fill(input_ids == self.tokenizer.pad_token_id, -100)\n",
    "            prompt_mask = torch.arange(seq_len, device=self.device).expand(batch_size, seq_len) < original_len_without_answer.unsqueeze(1)\n",
    "            masked_labels = masked_labels.masked_fill(prompt_mask == 1, -100)\n",
    "            return llama2_log_of_labels(logits=logits, labels=masked_labels, loss_func=self.loss_func)\n",
    "\n",
    "    def generation_pass(self, batch: torch.utils.data.Dataset) -> Tuple[List[str], torch.Tensor]:\n",
    "        \"\"\"Using the Llama2, generate new text.\n",
    "\n",
    "        This will be used for inference.\n",
    "        \"\"\"\n",
    "        self.predict_mode_on()\n",
    "        loaded_batch = self.data_to_device(batch, keys=[\"lm_input_ids_for_generation\", \"lm_attention_mask_for_generation\"])\n",
    "        input_ids = loaded_batch[\"lm_input_ids_for_generation\"]\n",
    "        attention_mask = loaded_batch[\"lm_attention_mask_for_generation\"]\n",
    "        with torch.no_grad():\n",
    "            # more look here:\n",
    "            # https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L130\n",
    "            predictions_output = self.model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                do_sample=True,\n",
    "                top_p=lm_top_p,\n",
    "                temperature=temperature,\n",
    "                max_length=lm_input_max_length + lm_output_max_length,\n",
    "                num_return_sequences=1,\n",
    "                output_logits=True,\n",
    "                return_dict_in_generate=True,\n",
    "                use_cache=True,\n",
    "                renormalize_logits=True,\n",
    "            )\n",
    "\n",
    "        prompt_len = input_ids.size()[1]\n",
    "        selected_samples = predictions_output.sequences[:, prompt_len:]\n",
    "        # all special tokens will be removed.\n",
    "        predictions_str = self.tokenizer.batch_decode(selected_samples, skip_special_tokens=True)\n",
    "        predictions_str = [pred.lstrip('\"').lstrip(\"'\").rstrip(\"'\").rstrip('\"').strip() for pred in predictions_str]\n",
    "\n",
    "        logits_list = list(predictions_output.logits)\n",
    "        logits = torch.stack(logits_list, dim=1)\n",
    "        labels_to_consider = selected_samples.masked_fill(selected_samples == self.tokenizer.pad_token_id, -100)\n",
    "        final_log_ps = mlm_log_of_labels(logits=logits, labels=labels_to_consider, loss_func=self.loss_func)\n",
    "        actual_lens = torch.sum(torch.where(labels_to_consider > 0, 1, 0), dim=1)\n",
    "        # Average log probs per token (length normalization).\n",
    "        return predictions_str, final_log_ps / actual_lens\n",
    "\n",
    "    def predict(self, batch: torch.utils.data.Dataset) -> Iterator[Dict[str, str]]:\n",
    "        \"\"\"The main prediction loop.\"\"\"\n",
    "        answers, log_ps = self.generation_pass(batch)\n",
    "        log_ps = log_ps.cpu().detach().numpy()\n",
    "        for idx, answer in enumerate(answers):\n",
    "            output_row = {\n",
    "                \"potential_answer\": answer,\n",
    "                \"prediction_score\": log_ps[idx],\n",
    "                \"gold_answer\": batch[\"output_texts\"][idx],\n",
    "            }\n",
    "            yield output_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gen_fewshot_file(file_path: str) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Load the fewshot files for QA task.\"\"\"\n",
    "    df = pd.read_csv(file_path, sep=\"\\t\")\n",
    "    input_texts = df.article.tolist()\n",
    "    output_texts = df.answer.tolist()\n",
    "    return input_texts, output_texts\n",
    "\n",
    "\n",
    "def create_dataloader(\n",
    "    model: Llama2QA,\n",
    "    train_file_name: Optional[str] = None,\n",
    "    dev_file_name: Optional[str] = None,\n",
    "    test_file_name: Optional[str] = None,\n",
    ") -> DataLoader:\n",
    "    \"\"\"Function to create the required dataloader to train the LM models.\"\"\"\n",
    "    if train_file_name is not None:\n",
    "        input_texts, output_texts = read_gen_fewshot_file(train_file_name)\n",
    "        shuffle = True\n",
    "        batch_size = train_batch_size\n",
    "\n",
    "    if dev_file_name is not None:\n",
    "        input_texts, output_texts = read_gen_fewshot_file(dev_file_name)\n",
    "        shuffle = False\n",
    "        batch_size = eval_batch_size\n",
    "\n",
    "    if test_file_name is not None:\n",
    "        input_texts, output_texts = read_gen_fewshot_file(test_file_name)\n",
    "        shuffle = False\n",
    "        batch_size = eval_batch_size\n",
    "\n",
    "    data = model.prepare_text(input_texts, output_texts)\n",
    "    dataset = DictDataset(data)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAMetricModel:\n",
    "    \"\"\"Load and cache a model used for evaluating generative text\n",
    "    generation.\"\"\"\n",
    "\n",
    "    model_id = \"sentence-transformers/sentence-t5-xxl\"\n",
    "\n",
    "    def __init__(self, device: str = \"cuda:0\", batch_size: int = 16) -> None:\n",
    "        \"\"\"Save the gpu device and construct the model and cache it.\"\"\"\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.metric_model = SentenceTransformer(self.model_id, device=self.device).eval()\n",
    "\n",
    "    def compute_metric(self, predictions: List[str], references: List[List[str]]) -> float:\n",
    "        \"\"\"Compute the metric for the given predictions and multiple\n",
    "        references.\"\"\"\n",
    "        average_score = torch.tensor(0.0, device=self.device)\n",
    "        num_chunks = max(len(predictions) // self.batch_size, 1)\n",
    "        for chunk_i in range(num_chunks):\n",
    "            clear_cache()\n",
    "\n",
    "            if (chunk_i + 1) * self.batch_size <= len(predictions):\n",
    "                predictions_sub_arr = predictions[chunk_i * self.batch_size : (chunk_i + 1) * self.batch_size]\n",
    "                references_sub_arr = references[chunk_i * self.batch_size : (chunk_i + 1) * self.batch_size]\n",
    "            else:\n",
    "                predictions_sub_arr = predictions[chunk_i * self.batch_size :]\n",
    "                references_sub_arr = references[chunk_i * self.batch_size :]\n",
    "\n",
    "            # need to track multiple references.\n",
    "            ref_sub_arr_len = [len(ref_sub_arr) for ref_sub_arr in references_sub_arr]\n",
    "            references_sub_arr_flattened = []\n",
    "            for ref_sub_arr in references_sub_arr:\n",
    "                references_sub_arr_flattened.extend(ref_sub_arr)\n",
    "\n",
    "            prediction_embeddings = self.metric_model.encode(\n",
    "                predictions_sub_arr,\n",
    "                show_progress_bar=False,\n",
    "                batch_size=self.batch_size,\n",
    "                device=self.device,\n",
    "                normalize_embeddings=True,\n",
    "                convert_to_tensor=True,\n",
    "            )\n",
    "\n",
    "            references_embeddings = self.metric_model.encode(\n",
    "                references_sub_arr_flattened,\n",
    "                show_progress_bar=False,\n",
    "                batch_size=self.batch_size,\n",
    "                device=self.device,\n",
    "                normalize_embeddings=True,\n",
    "                convert_to_tensor=True,\n",
    "            )\n",
    "            dot_products = torch.matmul(prediction_embeddings, references_embeddings.t())\n",
    "            score_collector = torch.zeros_like(dot_products)\n",
    "            i = 0\n",
    "            j = 0\n",
    "            while i < len(predictions_sub_arr):\n",
    "                j_len = ref_sub_arr_len[i]\n",
    "                score_collector[i][j : j + j_len] = 1.0 / j_len\n",
    "                i += 1\n",
    "                j += j_len\n",
    "\n",
    "            average_score += torch.sum(dot_products * score_collector)\n",
    "        return (average_score / len(predictions)).item()\n",
    "\n",
    "\n",
    "qa_metric_model = None\n",
    "\n",
    "\n",
    "def postprocess_qa(label: str) -> str:\n",
    "    label = str(label)\n",
    "    label = label.lower()\n",
    "    label = label.replace(\"\\n\", \" \")\n",
    "    label = label.removesuffix(\"</s>\")\n",
    "    label = label.removeprefix(\"<s>\")\n",
    "    label = label.removeprefix(\"\\n\")\n",
    "    label = label.removesuffix(\"\\n\")\n",
    "    label = label.removeprefix(\".\")\n",
    "    label = label.removesuffix(\".\")\n",
    "    label = label.removeprefix(\"answer:\")\n",
    "    label = label.removeprefix(\",\")\n",
    "    label = label.strip()\n",
    "    if \"no answer\" in label or \"no_answer\" in label:\n",
    "        label = \"no answer\"\n",
    "    return label\n",
    "\n",
    "\n",
    "def qa_metric(prediction_file: str) -> Dict[str, float]:\n",
    "    \"\"\"Compute the metric for the qa task.\"\"\"\n",
    "    global qa_metric_model\n",
    "    if qa_metric_model is None:\n",
    "        qa_metric_model = QAMetricModel(device=metric_device, batch_size=metric_batch_size)\n",
    "\n",
    "    df = pd.read_csv(prediction_file, delimiter=\",\")\n",
    "\n",
    "    gold_answers = [postprocess_qa(label) for label in df[\"gold_answer\"].tolist()]\n",
    "\n",
    "    multiple_gold_answers = []\n",
    "    for answer in gold_answers:\n",
    "        multiple_gold_answers.append(answer.split(\"[<@>]\"))\n",
    "\n",
    "    return_metrics: Dict[str, float] = {}\n",
    "    metrics = {\n",
    "        \"potential_answer\": \"qa_score\",\n",
    "    }\n",
    "\n",
    "    for metric_column, metric in metrics.items():\n",
    "        if metric_column in df.columns:\n",
    "            predictions = [postprocess_qa(pred) for pred in df[metric_column].tolist()]\n",
    "            score = qa_metric_model.compute_metric(predictions, multiple_gold_answers)\n",
    "            return_metrics[metric] = score\n",
    "\n",
    "    return return_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 13.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 16,777,216 || all params: 6,755,258,368 || trainable%: 0.24835787302339937\n"
     ]
    }
   ],
   "source": [
    "# Create model and start training.\n",
    "set_random_seed(42)\n",
    "\n",
    "model = Llama2QA(mode=\"train\", device=\"cuda:0\", seed=42)\n",
    "model.to_device()\n",
    "train_dataloader = create_dataloader(model, train_file_name=train_file_name)\n",
    "dev_dataloader = create_dataloader(model, dev_file_name=dev_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Prediction Step: 9.\n",
      "Prediction Step: 10.\n",
      "Prediction Step: 11.\n",
      "Prediction Step: 12.\n",
      "Prediction Step: 13.\n",
      "Prediction Step: 14.\n",
      "Prediction Step: 15.\n",
      "Prediction Step: 16.\n",
      "\n",
      "Epoch:0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loop(\n",
    "    model=model,\n",
    "    mode=\"train\",\n",
    "    model_path=model_path,\n",
    "    metric_to_save=\"qa_score\",\n",
    "    max_epochs=10,\n",
    "    training_steps=100000,  # not important\n",
    "    steps_per_checkpoint=16,\n",
    "    metric=qa_metric,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=dev_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Prediction Step: 9.\n",
      "Prediction Step: 10.\n",
      "Prediction Step: 11.\n",
      "Prediction Step: 12.\n",
      "Prediction Step: 13.\n",
      "Prediction Step: 14.\n",
      "Prediction Step: 15.\n",
      "Prediction Step: 16.\n",
      "Prediction Step: 17.\n",
      "Prediction Step: 18.\n",
      "Prediction Step: 19.\n",
      "Prediction Step: 20.\n",
      "Prediction Step: 21.\n",
      "Prediction Step: 22.\n",
      "Prediction Step: 23.\n",
      "Prediction Step: 24.\n",
      "Prediction Step: 25.\n",
      "Prediction Step: 26.\n",
      "Prediction Step: 27.\n",
      "Prediction Step: 28.\n",
      "Prediction Step: 29.\n",
      "Prediction Step: 30.\n",
      "Prediction Step: 31.\n",
      "Prediction Step: 32.\n",
      "Prediction Step: 33.\n",
      "Prediction Step: 34.\n",
      "Prediction Step: 35.\n",
      "Prediction Step: 36.\n",
      "Prediction Step: 37.\n",
      "Prediction Step: 38.\n",
      "Prediction Step: 39.\n",
      "Prediction Step: 40.\n",
      "Prediction Step: 41.\n",
      "Prediction Step: 42.\n",
      "Prediction Step: 43.\n",
      "Prediction Step: 44.\n",
      "Prediction Step: 45.\n",
      "Prediction Step: 46.\n",
      "Prediction Step: 47.\n",
      "Prediction Step: 48.\n",
      "Prediction Step: 49.\n",
      "Prediction Step: 50.\n",
      "Prediction Step: 51.\n",
      "Prediction Step: 52.\n",
      "Prediction Step: 53.\n",
      "Prediction Step: 54.\n",
      "Prediction Step: 55.\n",
      "Prediction Step: 56.\n",
      "Prediction Step: 57.\n",
      "Prediction Step: 58.\n",
      "Prediction Step: 59.\n",
      "Prediction Step: 60.\n",
      "Prediction Step: 61.\n",
      "Prediction Step: 62.\n",
      "Prediction Step: 63.\n",
      "Prediction Step: 64.\n",
      "Prediction Step: 65.\n",
      "Prediction Step: 66.\n",
      "Prediction Step: 67.\n",
      "Prediction Step: 68.\n",
      "Prediction Step: 69.\n",
      "Prediction Step: 70.\n",
      "Prediction Step: 71.\n",
      "Prediction Step: 72.\n",
      "Prediction Step: 73.\n",
      "Prediction Step: 74.\n",
      "Prediction Step: 75.\n",
      "Prediction Step: 76.\n",
      "Prediction Step: 77.\n",
      "Prediction Step: 78.\n",
      "Prediction Step: 79.\n",
      "Prediction Step: 80.\n",
      "Prediction Step: 81.\n",
      "Prediction Step: 82.\n",
      "Prediction Step: 83.\n",
      "Prediction Step: 84.\n",
      "Prediction Step: 85.\n",
      "Prediction Step: 86.\n",
      "Prediction Step: 87.\n",
      "Prediction Step: 88.\n",
      "Prediction Step: 89.\n",
      "Prediction Step: 90.\n",
      "Prediction Step: 91.\n",
      "Prediction Step: 92.\n",
      "Prediction Step: 93.\n",
      "Prediction Step: 94.\n",
      "Prediction Step: 95.\n",
      "Prediction Step: 96.\n",
      "Prediction Step: 97.\n",
      "Prediction Step: 98.\n",
      "Prediction Step: 99.\n",
      "Prediction Step: 100.\n",
      "Prediction Step: 101.\n",
      "Prediction Step: 102.\n",
      "Prediction Step: 103.\n",
      "Prediction Step: 104.\n",
      "Prediction Step: 105.\n",
      "Prediction Step: 106.\n",
      "Prediction Step: 107.\n",
      "Prediction Step: 108.\n",
      "Prediction Step: 109.\n",
      "Prediction Step: 110.\n",
      "Prediction Step: 111.\n",
      "Prediction Step: 112.\n",
      "Prediction Step: 113.\n"
     ]
    }
   ],
   "source": [
    "# Run on the Test Data.\n",
    "model.load_from_checkpoint(model_path, \"best_step\", peft_load=True, is_trainable=False)\n",
    "model.to_device()\n",
    "test_dataloader = create_dataloader(model, test_file_name=test_file_name)\n",
    "test_loop(\n",
    "    model=model,\n",
    "    mode=\"test\",\n",
    "    model_path=model_path,\n",
    "    prediction_file_name=\"test.predicted.tsv\",\n",
    "    test_dataloader=test_dataloader,\n",
    "    metric=qa_metric,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
