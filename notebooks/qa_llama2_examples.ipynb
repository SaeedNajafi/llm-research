{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-03-05 18:57:54.021442: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-05 18:57:54.027324: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-05 18:57:54.073848: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-05 18:57:54.073880: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-05 18:57:54.075482: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-05 18:57:54.085815: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-05 18:57:55.605974: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from typing import Any, Dict, Iterator, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from bitsandbytes.optim.adamw import PagedAdamW8bit\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    ")\n",
    "\n",
    "from src.base_lm import BaseLM\n",
    "from src.general_utils import DictDataset, test_loop, train_loop\n",
    "from src.model_utils import clear_cache, llama2_log_of_labels, lm_logits, mlm_log_of_labels, set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (0.21.3)\n",
      "Requirement already satisfied: filelock in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from huggingface_hub) (2024.2.0)\n",
      "Requirement already satisfied: requests in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from huggingface_hub) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from huggingface_hub) (4.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from requests->huggingface_hub) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from requests->huggingface_hub) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /h/snajafi/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token=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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 4\n",
    "eval_batch_size = 4\n",
    "lm_input_max_length = 1024\n",
    "lm_output_max_length = 128\n",
    "lm_top_p = 0.9\n",
    "temperature = 0.6\n",
    "metric_device = \"cuda:1\"\n",
    "metric_batch_size = 16\n",
    "learning_rate = 0.001\n",
    "train_file_name = \"128-shot-datasets/squad/128-42-train.tsv\"\n",
    "dev_file_name = \"128-shot-datasets/squad/128-42-dev.tsv\"\n",
    "test_file_name = \"128-shot-datasets/squad/test.tsv\"\n",
    "\n",
    "# folder to store models and predictions.\n",
    "model_path = \"/scratch/ssd004/scratch/snajafi/checkpoints/llama2\"\n",
    "\n",
    "# related to lora\n",
    "r = 16\n",
    "lora_alpha = 8\n",
    "lora_dropout = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load LM efficiently.\"\"\"\n",
    "\n",
    "# Make sure we have some tokens defined for the LM, if not defined in the model.\n",
    "_EXTRA_TOKENS = {\n",
    "    \"pad_token\": \"<pad>\",\n",
    "    \"mask_token\": \"<mask>\",\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"</s>\",\n",
    "    \"unk_token\": \"<unk>\",\n",
    "    \"cls_token\": \"<CLS>\",\n",
    "}\n",
    "\n",
    "target_modules = [\"q_proj\", \"v_proj\", \"o_proj\", \"k_proj\"]\n",
    "\n",
    "\n",
    "def load_peft_model(\n",
    "    model: PreTrainedModel,\n",
    "    adapter_name: str = \"lora\",\n",
    "    is_trainable: bool = False,\n",
    "    model_type: str = \"causal_lm\",\n",
    "    lora_target_modules: List[str] = target_modules,\n",
    ") -> torch.nn.Module:\n",
    "    \"\"\"Load a trained PEFT adapter to the base model and return the PeftModel.\n",
    "\n",
    "    Args:\n",
    "    ----\n",
    "        model: the main model.\n",
    "        num_quantized_bits: number of bits in the loaded model.\n",
    "        adapter_name: e.g. lora.\n",
    "        is_trainable: train or inference mode.\n",
    "        model_type: causal lm or seq-to-seq.\n",
    "        lora_target_modules: which modules to train with lora.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "        The PEFT model and tokenizer.\n",
    "    \"\"\"\n",
    "    if model_type == \"causal_lm\":\n",
    "        task_type = TaskType.CAUSAL_LM\n",
    "    elif model_type == \"seq_to_seq_lm\":\n",
    "        task_type = TaskType.SEQ_2_SEQ_LM\n",
    "\n",
    "    if adapter_name == \"lora\":\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=task_type,\n",
    "            inference_mode=not is_trainable,\n",
    "            r=r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            lora_dropout=lora_dropout,\n",
    "            bias=\"none\",\n",
    "            init_lora_weights=True,\n",
    "            target_modules=lora_target_modules,\n",
    "        )\n",
    "\n",
    "    peft_model = get_peft_model(model, peft_config)\n",
    "    peft_model.print_trainable_parameters()\n",
    "    return peft_model\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer(\n",
    "    model_id: str, model_type: str, model_dtype: torch.dtype, attn_implementation: str, load_in_4bit: Optional[bool] = True\n",
    ") -> Tuple[PreTrainedModel, PreTrainedTokenizer]:\n",
    "    \"\"\"Load the model and tokenizer.\n",
    "\n",
    "    Args:\n",
    "    ----\n",
    "        model_id: the id for the pre-trained model.\n",
    "        model_type: causal lm or seq_to_seq_lm.\n",
    "        model_dtype: model data type.\n",
    "        load_in_4bit: Whether to load in 4 bit quantization.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "        The model and tokenizer.\n",
    "    \"\"\"\n",
    "    # load model\n",
    "    if model_type == \"causal_lm\":\n",
    "        ModelClass = AutoModelForCausalLM\n",
    "    elif model_type == \"seq_to_seq_lm\":\n",
    "        ModelClass = AutoModelForSeq2SeqLM\n",
    "    model_args: Dict[str, Any] = {\"use_cache\": False, \"torch_dtype\": model_dtype, \"attn_implementation\": attn_implementation}\n",
    "    if load_in_4bit:\n",
    "        quant_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=model_args[\"torch_dtype\"],\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "        model_args[\"quantization_config\"] = quant_config\n",
    "    model = ModelClass.from_pretrained(\n",
    "        model_id,\n",
    "        **model_args,\n",
    "    )\n",
    "\n",
    "    # load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.add_special_tokens(_EXTRA_TOKENS)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        # extend embeddings to a multiple so we use Tensor cores\n",
    "        multiple = 64 if \"A100\" in torch.cuda.get_device_name() else 8\n",
    "        model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=multiple)\n",
    "    else:\n",
    "        raise Exception(\"No CUDA Found!\")\n",
    "\n",
    "    # re-define token ids for the model.\n",
    "    for extra_token_key, extra_token_val in _EXTRA_TOKENS.items():\n",
    "        extra_token_id = tokenizer.convert_tokens_to_ids([extra_token_val])[0]\n",
    "        model.config.__setattr__(f\"{extra_token_key}_id\", extra_token_id)\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llama2QA(BaseLM):\n",
    "    \"\"\"Class to implement Llama2 for QA task.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        mode: str,\n",
    "        device: str,\n",
    "        seed: int = 42,\n",
    "    ) -> None:\n",
    "        super().__init__(device, \"main_lm\", seed)\n",
    "        self.device = device\n",
    "        model, tokenizer = load_model_and_tokenizer(\n",
    "            model_id=\"/model-weights/Llama-2-13b-chat-hf\",\n",
    "            model_type=\"causal_lm\",\n",
    "            model_dtype=torch.bfloat16,\n",
    "            # attn_implementation=\"flash_attention_2\",\n",
    "            attn_implementation=\"eager\",\n",
    "            load_in_4bit=True,\n",
    "        )\n",
    "        peft_model = load_peft_model(\n",
    "            model=model,\n",
    "            adapter_name=\"lora\",\n",
    "            is_trainable=mode == \"train\",\n",
    "            model_type=\"causal_lm\",\n",
    "        )\n",
    "        self.model = peft_model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # to train the main lm, we update all of its parameters.\n",
    "        self.optimizer = PagedAdamW8bit(self.model.parameters(), lr=learning_rate)\n",
    "        self.scheduler = CosineAnnealingWarmRestarts(self.optimizer, T_0=10, eta_min=learning_rate / 5.0)\n",
    "\n",
    "    def prepare_text(self, texts: List[str], output_texts: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Convert texts to ids and return the dataset required for training\n",
    "        and inference.\"\"\"\n",
    "        instruction = \"In this task, you are given a context and question. \\\n",
    "            Provide a short phrase as the answer for the given question using only the information from the context. \\\n",
    "            If you do not know the answer from the context, generate 'no_answer' in the output. \\\n",
    "            Do not repeat the question in the output.\"\n",
    "        template = \"<s> [INST] <<SYS>> {instruction} <</SYS>> {input_text} [/INST]\"\n",
    "        # sample of the answers if possible.\n",
    "        sampled_answers = [random.choice(text.split(\"[<@>]\")) for text in output_texts]\n",
    "        answers = [f\"Answer: {answer}\" for answer in sampled_answers]\n",
    "\n",
    "        inputs_for_training = [\n",
    "            f\"{template.format(instruction=instruction, input_text=texts[idx])} {answers[idx]}\" for idx in range(len(texts))\n",
    "        ]\n",
    "        inputs_for_generation = [template.format(instruction=instruction, input_text=texts[idx]) for idx in range(len(texts))]\n",
    "\n",
    "        input_encodings = self.tokenizer(\n",
    "            inputs_for_training,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=lm_input_max_length + lm_output_max_length,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        input_encodings_for_generation = self.tokenizer(\n",
    "            inputs_for_generation,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=lm_input_max_length,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        data = {\n",
    "            \"output_texts\": output_texts,\n",
    "            \"lm_input_ids_for_train\": input_encodings.input_ids,\n",
    "            \"lm_attention_mask_for_train\": input_encodings.attention_mask,\n",
    "            \"lm_input_ids_for_generation\": input_encodings_for_generation.input_ids,\n",
    "            \"lm_attention_mask_for_generation\": input_encodings_for_generation.attention_mask,\n",
    "        }\n",
    "        return data\n",
    "\n",
    "    def train(self, batch: torch.utils.data.Dataset) -> torch.Tensor:\n",
    "        \"\"\"Using the Llama2, run a forward computation over the batch, compute\n",
    "        the log probability over the batch.\n",
    "\n",
    "        This will be used for training.\n",
    "        \"\"\"\n",
    "        self.train_mode_on()\n",
    "        loaded_batch = self.data_to_device(batch, keys=[\"lm_input_ids_for_train\", \"lm_attention_mask_for_train\"])\n",
    "        input_ids = loaded_batch[\"lm_input_ids_for_train\"]\n",
    "        attention_mask = loaded_batch[\"lm_attention_mask_for_train\"]\n",
    "        with torch.set_grad_enabled(True):\n",
    "            logits = lm_logits(\n",
    "                model=self.model,\n",
    "                input_ids=input_ids,\n",
    "                input_mask=attention_mask,\n",
    "            )\n",
    "            masked_labels = input_ids.masked_fill(input_ids == self.tokenizer.pad_token_id, -100)\n",
    "            return llama2_log_of_labels(logits=logits, labels=masked_labels, loss_func=self.loss_func)\n",
    "\n",
    "    def generation_pass(self, batch: torch.utils.data.Dataset) -> Tuple[List[str], torch.Tensor]:\n",
    "        \"\"\"Using the Llama2, generate new text.\n",
    "\n",
    "        This will be used for inference.\n",
    "        \"\"\"\n",
    "        self.predict_mode_on()\n",
    "        loaded_batch = self.data_to_device(batch, keys=[\"lm_input_ids_for_generation\", \"lm_attention_mask_for_generation\"])\n",
    "        input_ids = loaded_batch[\"lm_input_ids_for_generation\"]\n",
    "        attention_mask = loaded_batch[\"lm_attention_mask_for_generation\"]\n",
    "        with torch.no_grad():\n",
    "            # more look here:\n",
    "            # https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L130\n",
    "            predictions_output = self.model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                do_sample=True,\n",
    "                top_p=lm_top_p,\n",
    "                temperature=temperature,\n",
    "                max_length=lm_input_max_length + lm_output_max_length,\n",
    "                num_return_sequences=1,\n",
    "                output_logits=True,\n",
    "                return_dict_in_generate=True,\n",
    "                use_cache=True,\n",
    "                renormalize_logits=True,\n",
    "            )\n",
    "\n",
    "        prompt_len = input_ids.size()[1]\n",
    "        selected_samples = predictions_output.sequences[:, prompt_len:]\n",
    "        # all special tokens will be removed.\n",
    "        predictions_str = self.tokenizer.batch_decode(selected_samples, skip_special_tokens=True)\n",
    "        predictions_str = [pred.lstrip('\"').lstrip(\"'\").rstrip(\"'\").rstrip('\"').strip() for pred in predictions_str]\n",
    "\n",
    "        logits_list = list(predictions_output.logits)\n",
    "        logits = torch.stack(logits_list, dim=1)\n",
    "        labels_to_consider = selected_samples.masked_fill(selected_samples == self.tokenizer.pad_token_id, -100)\n",
    "        final_log_ps = mlm_log_of_labels(logits=logits, labels=labels_to_consider, loss_func=self.loss_func)\n",
    "        actual_lens = torch.sum(torch.where(labels_to_consider > 0, 1, 0), dim=1)\n",
    "        # Average log probs per token (length normalization).\n",
    "        return predictions_str, final_log_ps / actual_lens\n",
    "\n",
    "    def predict(self, batch: torch.utils.data.Dataset) -> Iterator[Dict[str, str]]:\n",
    "        \"\"\"The main prediction loop.\"\"\"\n",
    "        answers, log_ps = self.generation_pass(batch)\n",
    "        log_ps = log_ps.cpu().detach().numpy()\n",
    "        for idx, answer in enumerate(answers):\n",
    "            output_row = {\n",
    "                \"potential_answer\": answer,\n",
    "                \"prediction_score\": log_ps[idx],\n",
    "                \"gold_answer\": batch[\"output_texts\"][idx],\n",
    "            }\n",
    "            yield output_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gen_fewshot_file(file_path: str) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Load the fewshot files for QA task.\"\"\"\n",
    "    df = pd.read_csv(file_path, sep=\"\\t\")\n",
    "    input_texts = df.article.tolist()\n",
    "    output_texts = df.answer.tolist()\n",
    "    return input_texts, output_texts\n",
    "\n",
    "\n",
    "def create_dataloader(\n",
    "    model: Llama2QA,\n",
    "    train_file_name: Optional[str] = None,\n",
    "    dev_file_name: Optional[str] = None,\n",
    "    test_file_name: Optional[str] = None,\n",
    ") -> DataLoader:\n",
    "    \"\"\"Function to create the required dataloader to train the LM models.\"\"\"\n",
    "    if train_file_name is not None:\n",
    "        input_texts, output_texts = read_gen_fewshot_file(train_file_name)\n",
    "        shuffle = True\n",
    "        batch_size = train_batch_size\n",
    "\n",
    "    if dev_file_name is not None:\n",
    "        input_texts, output_texts = read_gen_fewshot_file(dev_file_name)\n",
    "        shuffle = False\n",
    "        batch_size = eval_batch_size\n",
    "\n",
    "    if test_file_name is not None:\n",
    "        input_texts, output_texts = read_gen_fewshot_file(test_file_name)\n",
    "        shuffle = False\n",
    "        batch_size = eval_batch_size\n",
    "\n",
    "    data = model.prepare_text(input_texts, output_texts)\n",
    "    dataset = DictDataset(data)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAMetricModel:\n",
    "    \"\"\"Load and cache a model used for evaluating generative text\n",
    "    generation.\"\"\"\n",
    "\n",
    "    model_id = \"sentence-transformers/sentence-t5-xl\"\n",
    "\n",
    "    def __init__(self, device: str = \"cuda:0\", batch_size: int = 16) -> None:\n",
    "        \"\"\"Save the gpu device and construct the model and cache it.\"\"\"\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.metric_model = SentenceTransformer(self.model_id, device=self.device).eval()\n",
    "\n",
    "    def compute_metric(self, predictions: List[str], references: List[List[str]]) -> float:\n",
    "        \"\"\"Compute the metric for the given predictions and multiple\n",
    "        references.\"\"\"\n",
    "        average_score = torch.tensor(0.0, device=self.device)\n",
    "        num_chunks = max(len(predictions) // self.batch_size, 1)\n",
    "        for chunk_i in range(num_chunks):\n",
    "            clear_cache()\n",
    "\n",
    "            if (chunk_i + 1) * self.batch_size <= len(predictions):\n",
    "                predictions_sub_arr = predictions[chunk_i * self.batch_size : (chunk_i + 1) * self.batch_size]\n",
    "                references_sub_arr = references[chunk_i * self.batch_size : (chunk_i + 1) * self.batch_size]\n",
    "            else:\n",
    "                predictions_sub_arr = predictions[chunk_i * self.batch_size :]\n",
    "                references_sub_arr = references[chunk_i * self.batch_size :]\n",
    "\n",
    "            # need to track multiple references.\n",
    "            ref_sub_arr_len = [len(ref_sub_arr) for ref_sub_arr in references_sub_arr]\n",
    "            references_sub_arr_flattened = []\n",
    "            for ref_sub_arr in references_sub_arr:\n",
    "                references_sub_arr_flattened.extend(ref_sub_arr)\n",
    "\n",
    "            prediction_embeddings = self.metric_model.encode(\n",
    "                predictions_sub_arr,\n",
    "                show_progress_bar=False,\n",
    "                batch_size=self.batch_size,\n",
    "                device=self.device,\n",
    "                normalize_embeddings=True,\n",
    "                convert_to_tensor=True,\n",
    "            )\n",
    "\n",
    "            references_embeddings = self.metric_model.encode(\n",
    "                references_sub_arr_flattened,\n",
    "                show_progress_bar=False,\n",
    "                batch_size=self.batch_size,\n",
    "                device=self.device,\n",
    "                normalize_embeddings=True,\n",
    "                convert_to_tensor=True,\n",
    "            )\n",
    "            dot_products = torch.matmul(prediction_embeddings, references_embeddings.t())\n",
    "            score_collector = torch.zeros_like(dot_products)\n",
    "            i = 0\n",
    "            j = 0\n",
    "            while i < len(predictions_sub_arr):\n",
    "                j_len = ref_sub_arr_len[i]\n",
    "                score_collector[i][j : j + j_len] = 1.0 / j_len\n",
    "                i += 1\n",
    "                j += j_len\n",
    "\n",
    "            average_score += torch.sum(dot_products * score_collector)\n",
    "        return (average_score / len(predictions)).item()\n",
    "\n",
    "\n",
    "qa_metric_model = None\n",
    "\n",
    "\n",
    "def postprocess_qa(label: str) -> str:\n",
    "    label = str(label)\n",
    "    label = label.lower()\n",
    "    label = label.replace(\"\\n\", \" \")\n",
    "    label = label.removesuffix(\"</s>\")\n",
    "    label = label.removeprefix(\"<s>\")\n",
    "    label = label.removeprefix(\"\\n\")\n",
    "    label = label.removesuffix(\"\\n\")\n",
    "    label = label.removeprefix(\".\")\n",
    "    label = label.removesuffix(\".\")\n",
    "    label = label.removeprefix(\"answer:\")\n",
    "    label = label.removeprefix(\",\")\n",
    "    label = label.strip()\n",
    "    if \"no answer\" in label or \"no_answer\" in label:\n",
    "        label = \"no answer\"\n",
    "    return label\n",
    "\n",
    "\n",
    "def qa_metric(prediction_file: str) -> Dict[str, float]:\n",
    "    \"\"\"Compute the metric for the qa task.\"\"\"\n",
    "    global qa_metric_model\n",
    "    if qa_metric_model is None:\n",
    "        qa_metric_model = QAMetricModel(device=metric_device, batch_size=metric_batch_size)\n",
    "\n",
    "    df = pd.read_csv(prediction_file, delimiter=\",\")\n",
    "\n",
    "    gold_answers = [postprocess_qa(label) for label in df[\"gold_answer\"].tolist()]\n",
    "\n",
    "    multiple_gold_answers = []\n",
    "    for answer in gold_answers:\n",
    "        multiple_gold_answers.append(answer.split(\"[<@>]\"))\n",
    "\n",
    "    return_metrics: Dict[str, float] = {}\n",
    "    metrics = {\n",
    "        \"potential_answer\": \"qa_score\",\n",
    "    }\n",
    "\n",
    "    for metric_column, metric in metrics.items():\n",
    "        if metric_column in df.columns:\n",
    "            predictions = [postprocess_qa(pred) for pred in df[metric_column].tolist()]\n",
    "            score = qa_metric_model.compute_metric(predictions, multiple_gold_answers)\n",
    "            return_metrics[metric] = score\n",
    "\n",
    "    return return_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 314.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 31.88 MiB is free. Including non-PyTorch memory, this process has 22.12 GiB memory in use. Of the allocated memory 20.59 GiB is allocated by PyTorch, and 792.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create model and start training.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m set_random_seed(\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLlama2QA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mto_device()\n\u001b[1;32m      6\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m create_dataloader(model, train_file_name\u001b[38;5;241m=\u001b[39mtrain_file_name)\n",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m, in \u001b[0;36mLlama2QA.__init__\u001b[0;34m(self, mode, device, seed)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(device, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_lm\u001b[39m\u001b[38;5;124m\"\u001b[39m, seed)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device\n\u001b[0;32m---> 11\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_and_tokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/model-weights/Llama-2-13b-chat-hf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcausal_lm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#attn_implementation=\"flash_attention_2\",\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meager\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m peft_model \u001b[38;5;241m=\u001b[39m load_peft_model(\n\u001b[1;32m     20\u001b[0m             model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     21\u001b[0m             adapter_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlora\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m             is_trainable\u001b[38;5;241m=\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     23\u001b[0m             model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcausal_lm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m peft_model\n",
      "Cell \u001b[0;32mIn[5], line 90\u001b[0m, in \u001b[0;36mload_model_and_tokenizer\u001b[0;34m(model_id, model_type, model_dtype, attn_implementation, load_in_4bit)\u001b[0m\n\u001b[1;32m     83\u001b[0m     quant_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m     84\u001b[0m         load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     85\u001b[0m         bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     86\u001b[0m         bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mmodel_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     87\u001b[0m         bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     88\u001b[0m     )\n\u001b[1;32m     89\u001b[0m     model_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m quant_config\n\u001b[0;32m---> 90\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mModelClass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# load tokenizer\u001b[39;00m\n\u001b[1;32m     96\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/transformers/modeling_utils.py:3507\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3498\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3499\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3500\u001b[0m     (\n\u001b[1;32m   3501\u001b[0m         model,\n\u001b[1;32m   3502\u001b[0m         missing_keys,\n\u001b[1;32m   3503\u001b[0m         unexpected_keys,\n\u001b[1;32m   3504\u001b[0m         mismatched_keys,\n\u001b[1;32m   3505\u001b[0m         offload_index,\n\u001b[1;32m   3506\u001b[0m         error_msgs,\n\u001b[0;32m-> 3507\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3513\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3514\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3515\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3518\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3519\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3523\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3525\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3526\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/transformers/modeling_utils.py:3932\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3930\u001b[0m                     hf_quantizer\u001b[38;5;241m.\u001b[39mcreate_quantized_param(model, param, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, state_dict)\n\u001b[1;32m   3931\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3932\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3933\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3934\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3935\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloaded_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3936\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3937\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3938\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3939\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3940\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3941\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3942\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3943\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3944\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3945\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3946\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3947\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3948\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3949\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   3950\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/transformers/modeling_utils.py:805\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys)\u001b[0m\n\u001b[1;32m    798\u001b[0m     state_dict_index \u001b[38;5;241m=\u001b[39m offload_weight(param, param_name, model, state_dict_folder, state_dict_index)\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    800\u001b[0m     hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m hf_quantizer\u001b[38;5;241m.\u001b[39mrequires_parameters_quantization)\n\u001b[1;32m    802\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m hf_quantizer\u001b[38;5;241m.\u001b[39mcheck_quantized_param(model, param, param_name, state_dict))\n\u001b[1;32m    803\u001b[0m ):\n\u001b[1;32m    804\u001b[0m     \u001b[38;5;66;03m# For backward compatibility with older versions of `accelerate` and for non-quantized params\u001b[39;00m\n\u001b[0;32m--> 805\u001b[0m     \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mset_module_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    807\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mcreate_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/accelerate/utils/modeling.py:384\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    382\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 384\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 314.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 31.88 MiB is free. Including non-PyTorch memory, this process has 22.12 GiB memory in use. Of the allocated memory 20.59 GiB is allocated by PyTorch, and 792.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Create model and start training.\n",
    "set_random_seed(42)\n",
    "\n",
    "model = Llama2QA(mode=\"train\", device=\"cuda:0\", seed=42)\n",
    "model.to_device()\n",
    "train_dataloader = create_dataloader(model, train_file_name=train_file_name)\n",
    "dev_dataloader = create_dataloader(model, dev_file_name=dev_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 31.88 MiB is free. Including non-PyTorch memory, this process has 22.12 GiB memory in use. Of the allocated memory 20.59 GiB is allocated by PyTorch, and 792.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_to_save\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mqa_score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# not important\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqa_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdev_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/src/general_utils.py:72\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(model, mode, model_path, metric_to_save, max_epochs, training_steps, steps_per_checkpoint, metric, train_dataloader, eval_dataloader)\u001b[0m\n\u001b[1;32m     70\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     71\u001b[0m eval_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemp_eval.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 72\u001b[0m \u001b[43mstart_predicting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m scores \u001b[38;5;241m=\u001b[39m metric(eval_file)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m score_name, score_val \u001b[38;5;129;01min\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/src/general_utils.py:31\u001b[0m, in \u001b[0;36mstart_predicting\u001b[0;34m(model, dataloader, prediction_file)\u001b[0m\n\u001b[1;32m     29\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ret_row \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mpredict(batch):\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m header_written:\n\u001b[1;32m     33\u001b[0m             headers \u001b[38;5;241m=\u001b[39m ret_row\u001b[38;5;241m.\u001b[39mkeys()\n",
      "Cell \u001b[0;32mIn[6], line 133\u001b[0m, in \u001b[0;36mLlama2QA.predict\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]:\n\u001b[1;32m    132\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The main prediction loop.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     answers, log_ps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneration_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m     log_ps \u001b[38;5;241m=\u001b[39m log_ps\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, answer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(answers):\n",
      "Cell \u001b[0;32mIn[6], line 103\u001b[0m, in \u001b[0;36mLlama2QA.generation_pass\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     99\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m loaded_batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlm_attention_mask_for_generation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# more look here:\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L130\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     predictions_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlm_top_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlm_input_max_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlm_output_max_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrenormalize_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m prompt_len \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    118\u001b[0m selected_samples \u001b[38;5;241m=\u001b[39m predictions_output\u001b[38;5;241m.\u001b[39msequences[:, prompt_len:]\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/peft/peft_model.py:1148\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgeneration_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1148\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prepare_inputs_for_generation\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/transformers/generation/utils.py:1597\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1590\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1591\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1592\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1593\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1594\u001b[0m     )\n\u001b[1;32m   1596\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1597\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1609\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1612\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1613\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1614\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1615\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1616\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1621\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1622\u001b[0m     )\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/transformers/generation/utils.py:2711\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2708\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2710\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2711\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2712\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2714\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2715\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2716\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2719\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1192\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1189\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1192\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1205\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1001\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m position_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    999\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m cache_position\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m-> 1001\u001b[0m causal_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_causal_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;66;03m# embed positions\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1086\u001b[0m, in \u001b[0;36mLlamaModel._update_causal_mask\u001b[0;34m(self, attention_mask, input_tensor)\u001b[0m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;66;03m# We use the current dtype to avoid any overflows\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m min_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfinfo(dtype)\u001b[38;5;241m.\u001b[39mmin\n\u001b[0;32m-> 1086\u001b[0m causal_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(dtype) \u001b[38;5;241m*\u001b[39m min_dtype\n\u001b[1;32m   1088\u001b[0m causal_mask \u001b[38;5;241m=\u001b[39m causal_mask\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m attention_mask\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 31.88 MiB is free. Including non-PyTorch memory, this process has 22.12 GiB memory in use. Of the allocated memory 20.59 GiB is allocated by PyTorch, and 792.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "train_loop(\n",
    "    model=model,\n",
    "    mode=\"train\",\n",
    "    model_path=model_path,\n",
    "    metric_to_save=\"qa_score\",\n",
    "    max_epochs=10,\n",
    "    training_steps=100000,  # not important\n",
    "    steps_per_checkpoint=16,\n",
    "    metric=qa_metric,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=dev_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Prediction Step: 9.\n",
      "Prediction Step: 10.\n",
      "Prediction Step: 11.\n",
      "Prediction Step: 12.\n",
      "Prediction Step: 13.\n",
      "Prediction Step: 14.\n",
      "Prediction Step: 15.\n",
      "Prediction Step: 16.\n",
      "Prediction Step: 17.\n",
      "Prediction Step: 18.\n",
      "Prediction Step: 19.\n",
      "Prediction Step: 20.\n",
      "Prediction Step: 21.\n",
      "Prediction Step: 22.\n",
      "Prediction Step: 23.\n",
      "Prediction Step: 24.\n",
      "Prediction Step: 25.\n",
      "Prediction Step: 26.\n",
      "Prediction Step: 27.\n",
      "Prediction Step: 28.\n",
      "Prediction Step: 29.\n",
      "Prediction Step: 30.\n",
      "Prediction Step: 31.\n",
      "Prediction Step: 32.\n",
      "Prediction Step: 33.\n",
      "Prediction Step: 34.\n",
      "Prediction Step: 35.\n",
      "Prediction Step: 36.\n",
      "Prediction Step: 37.\n",
      "Prediction Step: 38.\n",
      "Prediction Step: 39.\n",
      "Prediction Step: 40.\n",
      "Prediction Step: 41.\n",
      "Prediction Step: 42.\n",
      "Prediction Step: 43.\n",
      "Prediction Step: 44.\n",
      "Prediction Step: 45.\n",
      "Prediction Step: 46.\n",
      "Prediction Step: 47.\n",
      "Prediction Step: 48.\n",
      "Prediction Step: 49.\n",
      "Prediction Step: 50.\n",
      "Prediction Step: 51.\n",
      "Prediction Step: 52.\n",
      "Prediction Step: 53.\n",
      "Prediction Step: 54.\n",
      "Prediction Step: 55.\n",
      "Prediction Step: 56.\n",
      "Prediction Step: 57.\n",
      "Prediction Step: 58.\n",
      "Prediction Step: 59.\n",
      "Prediction Step: 60.\n",
      "Prediction Step: 61.\n",
      "Prediction Step: 62.\n",
      "Prediction Step: 63.\n",
      "Prediction Step: 64.\n",
      "Prediction Step: 65.\n",
      "Prediction Step: 66.\n",
      "Prediction Step: 67.\n",
      "Prediction Step: 68.\n",
      "Prediction Step: 69.\n",
      "Prediction Step: 70.\n",
      "Prediction Step: 71.\n",
      "Prediction Step: 72.\n",
      "Prediction Step: 73.\n",
      "Prediction Step: 74.\n",
      "Prediction Step: 75.\n",
      "Prediction Step: 76.\n",
      "Prediction Step: 77.\n",
      "Prediction Step: 78.\n",
      "Prediction Step: 79.\n",
      "Prediction Step: 80.\n",
      "Prediction Step: 81.\n",
      "Prediction Step: 82.\n",
      "Prediction Step: 83.\n",
      "Prediction Step: 84.\n",
      "Prediction Step: 85.\n",
      "Prediction Step: 86.\n",
      "Prediction Step: 87.\n",
      "Prediction Step: 88.\n",
      "Prediction Step: 89.\n",
      "Prediction Step: 90.\n",
      "Prediction Step: 91.\n",
      "Prediction Step: 92.\n",
      "Prediction Step: 93.\n",
      "Prediction Step: 94.\n",
      "Prediction Step: 95.\n",
      "Prediction Step: 96.\n",
      "Prediction Step: 97.\n",
      "Prediction Step: 98.\n",
      "Prediction Step: 99.\n",
      "Prediction Step: 100.\n",
      "Prediction Step: 101.\n",
      "Prediction Step: 102.\n",
      "Prediction Step: 103.\n",
      "Prediction Step: 104.\n",
      "Prediction Step: 105.\n",
      "Prediction Step: 106.\n",
      "Prediction Step: 107.\n",
      "Prediction Step: 108.\n",
      "Prediction Step: 109.\n",
      "Prediction Step: 110.\n",
      "Prediction Step: 111.\n",
      "Prediction Step: 112.\n",
      "Prediction Step: 113.\n",
      "Prediction Step: 114.\n",
      "Prediction Step: 115.\n",
      "Prediction Step: 116.\n",
      "Prediction Step: 117.\n",
      "Prediction Step: 118.\n",
      "Prediction Step: 119.\n",
      "Prediction Step: 120.\n",
      "Prediction Step: 121.\n",
      "Prediction Step: 122.\n",
      "Prediction Step: 123.\n",
      "Prediction Step: 124.\n",
      "Prediction Step: 125.\n",
      "Prediction Step: 126.\n",
      "Prediction Step: 127.\n",
      "Prediction Step: 128.\n",
      "Prediction Step: 129.\n",
      "Prediction Step: 130.\n",
      "Prediction Step: 131.\n",
      "Prediction Step: 132.\n",
      "Prediction Step: 133.\n",
      "Prediction Step: 134.\n",
      "Prediction Step: 135.\n",
      "Prediction Step: 136.\n",
      "Prediction Step: 137.\n",
      "Prediction Step: 138.\n",
      "Prediction Step: 139.\n",
      "Prediction Step: 140.\n",
      "Prediction Step: 141.\n",
      "Prediction Step: 142.\n",
      "Prediction Step: 143.\n",
      "Prediction Step: 144.\n",
      "Prediction Step: 145.\n",
      "Prediction Step: 146.\n",
      "Prediction Step: 147.\n",
      "Prediction Step: 148.\n",
      "Prediction Step: 149.\n",
      "Prediction Step: 150.\n",
      "Prediction Step: 151.\n",
      "Prediction Step: 152.\n",
      "Prediction Step: 153.\n",
      "Prediction Step: 154.\n",
      "Prediction Step: 155.\n",
      "Prediction Step: 156.\n",
      "Prediction Step: 157.\n",
      "Prediction Step: 158.\n",
      "Prediction Step: 159.\n",
      "Prediction Step: 160.\n",
      "Prediction Step: 161.\n",
      "Prediction Step: 162.\n",
      "Prediction Step: 163.\n",
      "Prediction Step: 164.\n",
      "Prediction Step: 165.\n",
      "Prediction Step: 166.\n",
      "Prediction Step: 167.\n",
      "Prediction Step: 168.\n",
      "Prediction Step: 169.\n",
      "Prediction Step: 170.\n",
      "Prediction Step: 171.\n",
      "Prediction Step: 172.\n",
      "Prediction Step: 173.\n",
      "Prediction Step: 174.\n",
      "Prediction Step: 175.\n",
      "Prediction Step: 176.\n",
      "Prediction Step: 177.\n",
      "Prediction Step: 178.\n",
      "Prediction Step: 179.\n",
      "Prediction Step: 180.\n",
      "Prediction Step: 181.\n",
      "Prediction Step: 182.\n",
      "Prediction Step: 183.\n",
      "Prediction Step: 184.\n",
      "Prediction Step: 185.\n",
      "Prediction Step: 186.\n",
      "Prediction Step: 187.\n",
      "Prediction Step: 188.\n",
      "Prediction Step: 189.\n",
      "Prediction Step: 190.\n",
      "Prediction Step: 191.\n",
      "Prediction Step: 192.\n",
      "Prediction Step: 193.\n",
      "Prediction Step: 194.\n",
      "Prediction Step: 195.\n",
      "Prediction Step: 196.\n",
      "Prediction Step: 197.\n",
      "Prediction Step: 198.\n",
      "Prediction Step: 199.\n",
      "Prediction Step: 200.\n",
      "Prediction Step: 201.\n",
      "Prediction Step: 202.\n",
      "Prediction Step: 203.\n",
      "Prediction Step: 204.\n",
      "Prediction Step: 205.\n",
      "Prediction Step: 206.\n",
      "Prediction Step: 207.\n",
      "Prediction Step: 208.\n",
      "Prediction Step: 209.\n",
      "Prediction Step: 210.\n",
      "Prediction Step: 211.\n",
      "Prediction Step: 212.\n",
      "Prediction Step: 213.\n",
      "Prediction Step: 214.\n",
      "Prediction Step: 215.\n",
      "Prediction Step: 216.\n",
      "Prediction Step: 217.\n",
      "Prediction Step: 218.\n",
      "Prediction Step: 219.\n",
      "Prediction Step: 220.\n",
      "Prediction Step: 221.\n",
      "Prediction Step: 222.\n",
      "Prediction Step: 223.\n",
      "Prediction Step: 224.\n",
      "Prediction Step: 225.\n",
      "Prediction Step: 226.\n",
      "Prediction Step: 227.\n",
      "Prediction Step: 228.\n",
      "Prediction Step: 229.\n",
      "Prediction Step: 230.\n",
      "Prediction Step: 231.\n",
      "Prediction Step: 232.\n",
      "Prediction Step: 233.\n",
      "Prediction Step: 234.\n",
      "Prediction Step: 235.\n",
      "Prediction Step: 236.\n",
      "Prediction Step: 237.\n",
      "Prediction Step: 238.\n",
      "Prediction Step: 239.\n",
      "Prediction Step: 240.\n",
      "Prediction Step: 241.\n",
      "Prediction Step: 242.\n",
      "Prediction Step: 243.\n",
      "Prediction Step: 244.\n",
      "Prediction Step: 245.\n",
      "Prediction Step: 246.\n",
      "Prediction Step: 247.\n",
      "Prediction Step: 248.\n",
      "Prediction Step: 249.\n",
      "Prediction Step: 250.\n",
      "Prediction Step: 251.\n",
      "Prediction Step: 252.\n",
      "Prediction Step: 253.\n",
      "Prediction Step: 254.\n",
      "Prediction Step: 255.\n",
      "Prediction Step: 256.\n",
      "Prediction Step: 257.\n",
      "Prediction Step: 258.\n",
      "Prediction Step: 259.\n",
      "Prediction Step: 260.\n",
      "Prediction Step: 261.\n",
      "Prediction Step: 262.\n",
      "Prediction Step: 263.\n",
      "Prediction Step: 264.\n",
      "Prediction Step: 265.\n",
      "Prediction Step: 266.\n",
      "Prediction Step: 267.\n",
      "Prediction Step: 268.\n",
      "Prediction Step: 269.\n",
      "Prediction Step: 270.\n",
      "Prediction Step: 271.\n",
      "Prediction Step: 272.\n",
      "Prediction Step: 273.\n",
      "Prediction Step: 274.\n",
      "Prediction Step: 275.\n",
      "Prediction Step: 276.\n",
      "Prediction Step: 277.\n",
      "Prediction Step: 278.\n",
      "Prediction Step: 279.\n",
      "Prediction Step: 280.\n",
      "Prediction Step: 281.\n",
      "Prediction Step: 282.\n",
      "Prediction Step: 283.\n",
      "Prediction Step: 284.\n",
      "Prediction Step: 285.\n",
      "Prediction Step: 286.\n",
      "Prediction Step: 287.\n",
      "Prediction Step: 288.\n",
      "Prediction Step: 289.\n",
      "Prediction Step: 290.\n",
      "Prediction Step: 291.\n",
      "Prediction Step: 292.\n",
      "Prediction Step: 293.\n",
      "Prediction Step: 294.\n",
      "Prediction Step: 295.\n",
      "Prediction Step: 296.\n",
      "Prediction Step: 297.\n",
      "Prediction Step: 298.\n",
      "Prediction Step: 299.\n",
      "Prediction Step: 300.\n",
      "Prediction Step: 301.\n",
      "Prediction Step: 302.\n",
      "Prediction Step: 303.\n",
      "Prediction Step: 304.\n",
      "Prediction Step: 305.\n",
      "Prediction Step: 306.\n",
      "Prediction Step: 307.\n",
      "Prediction Step: 308.\n",
      "Prediction Step: 309.\n",
      "Prediction Step: 310.\n",
      "Prediction Step: 311.\n",
      "Prediction Step: 312.\n",
      "Prediction Step: 313.\n",
      "Prediction Step: 314.\n",
      "Prediction Step: 315.\n",
      "Prediction Step: 316.\n",
      "Prediction Step: 317.\n",
      "Prediction Step: 318.\n",
      "Prediction Step: 319.\n",
      "Prediction Step: 320.\n",
      "Prediction Step: 321.\n",
      "Prediction Step: 322.\n",
      "Prediction Step: 323.\n",
      "Prediction Step: 324.\n",
      "Prediction Step: 325.\n",
      "Prediction Step: 326.\n",
      "Prediction Step: 327.\n",
      "Prediction Step: 328.\n",
      "Prediction Step: 329.\n",
      "Prediction Step: 330.\n",
      "Prediction Step: 331.\n",
      "Prediction Step: 332.\n",
      "Prediction Step: 333.\n",
      "Prediction Step: 334.\n",
      "Prediction Step: 335.\n",
      "Prediction Step: 336.\n",
      "Prediction Step: 337.\n",
      "Prediction Step: 338.\n",
      "Prediction Step: 339.\n",
      "Prediction Step: 340.\n",
      "Prediction Step: 341.\n",
      "Prediction Step: 342.\n",
      "Prediction Step: 343.\n",
      "Prediction Step: 344.\n",
      "Prediction Step: 345.\n",
      "Prediction Step: 346.\n",
      "Prediction Step: 347.\n",
      "Prediction Step: 348.\n",
      "Prediction Step: 349.\n",
      "Prediction Step: 350.\n",
      "Prediction Step: 351.\n",
      "Prediction Step: 352.\n",
      "Prediction Step: 353.\n",
      "Prediction Step: 354.\n",
      "Prediction Step: 355.\n",
      "Prediction Step: 356.\n",
      "Prediction Step: 357.\n",
      "Prediction Step: 358.\n",
      "Prediction Step: 359.\n",
      "Prediction Step: 360.\n",
      "Prediction Step: 361.\n",
      "Prediction Step: 362.\n",
      "Prediction Step: 363.\n",
      "Prediction Step: 364.\n",
      "Prediction Step: 365.\n",
      "Prediction Step: 366.\n",
      "Prediction Step: 367.\n",
      "Prediction Step: 368.\n",
      "Prediction Step: 369.\n",
      "Prediction Step: 370.\n",
      "Prediction Step: 371.\n",
      "Prediction Step: 372.\n",
      "Prediction Step: 373.\n",
      "Prediction Step: 374.\n",
      "Prediction Step: 375.\n",
      "Prediction Step: 376.\n",
      "Prediction Step: 377.\n",
      "Prediction Step: 378.\n",
      "Prediction Step: 379.\n",
      "Prediction Step: 380.\n",
      "Prediction Step: 381.\n",
      "Prediction Step: 382.\n",
      "Prediction Step: 383.\n",
      "Prediction Step: 384.\n",
      "Prediction Step: 385.\n",
      "Prediction Step: 386.\n",
      "Prediction Step: 387.\n",
      "Prediction Step: 388.\n",
      "Prediction Step: 389.\n",
      "Prediction Step: 390.\n",
      "Prediction Step: 391.\n",
      "Prediction Step: 392.\n",
      "Prediction Step: 393.\n",
      "Prediction Step: 394.\n",
      "Prediction Step: 395.\n",
      "Prediction Step: 396.\n",
      "Prediction Step: 397.\n",
      "Prediction Step: 398.\n",
      "Prediction Step: 399.\n",
      "Prediction Step: 400.\n",
      "Prediction Step: 401.\n",
      "Prediction Step: 402.\n",
      "Prediction Step: 403.\n",
      "Prediction Step: 404.\n",
      "Prediction Step: 405.\n",
      "Prediction Step: 406.\n",
      "Prediction Step: 407.\n",
      "Prediction Step: 408.\n",
      "Prediction Step: 409.\n",
      "Prediction Step: 410.\n",
      "Prediction Step: 411.\n",
      "Prediction Step: 412.\n",
      "Prediction Step: 413.\n",
      "Prediction Step: 414.\n",
      "Prediction Step: 415.\n",
      "Prediction Step: 416.\n",
      "Prediction Step: 417.\n",
      "Prediction Step: 418.\n",
      "Prediction Step: 419.\n",
      "Prediction Step: 420.\n",
      "Prediction Step: 421.\n",
      "Prediction Step: 422.\n",
      "Prediction Step: 423.\n",
      "Prediction Step: 424.\n",
      "Prediction Step: 425.\n",
      "Prediction Step: 426.\n",
      "Prediction Step: 427.\n",
      "Prediction Step: 428.\n",
      "Prediction Step: 429.\n",
      "Prediction Step: 430.\n",
      "Prediction Step: 431.\n",
      "Prediction Step: 432.\n",
      "Prediction Step: 433.\n",
      "Prediction Step: 434.\n",
      "Prediction Step: 435.\n",
      "Prediction Step: 436.\n",
      "Prediction Step: 437.\n",
      "Prediction Step: 438.\n",
      "Prediction Step: 439.\n",
      "Prediction Step: 440.\n",
      "Prediction Step: 441.\n",
      "Prediction Step: 442.\n",
      "Prediction Step: 443.\n",
      "Prediction Step: 444.\n",
      "Prediction Step: 445.\n",
      "Prediction Step: 446.\n",
      "Prediction Step: 447.\n",
      "Prediction Step: 448.\n",
      "Prediction Step: 449.\n",
      "Prediction Step: 450.\n",
      "Prediction Step: 451.\n",
      "Prediction Step: 452.\n",
      "Prediction Step: 453.\n",
      "Prediction Step: 454.\n",
      "Prediction Step: 455.\n",
      "Prediction Step: 456.\n",
      "Prediction Step: 457.\n",
      "Prediction Step: 458.\n",
      "Prediction Step: 459.\n",
      "Prediction Step: 460.\n",
      "Prediction Step: 461.\n",
      "Prediction Step: 462.\n",
      "Prediction Step: 463.\n",
      "Prediction Step: 464.\n",
      "Prediction Step: 465.\n",
      "Prediction Step: 466.\n",
      "Prediction Step: 467.\n",
      "Prediction Step: 468.\n",
      "Prediction Step: 469.\n",
      "Prediction Step: 470.\n",
      "Prediction Step: 471.\n",
      "Prediction Step: 472.\n",
      "Prediction Step: 473.\n",
      "Prediction Step: 474.\n",
      "Prediction Step: 475.\n",
      "Prediction Step: 476.\n",
      "Prediction Step: 477.\n",
      "Prediction Step: 478.\n",
      "Prediction Step: 479.\n",
      "Prediction Step: 480.\n",
      "Prediction Step: 481.\n",
      "Prediction Step: 482.\n",
      "Prediction Step: 483.\n",
      "Prediction Step: 484.\n",
      "Prediction Step: 485.\n",
      "Prediction Step: 486.\n",
      "Prediction Step: 487.\n",
      "Prediction Step: 488.\n",
      "Prediction Step: 489.\n",
      "Prediction Step: 490.\n",
      "Prediction Step: 491.\n",
      "Prediction Step: 492.\n",
      "Prediction Step: 493.\n",
      "Prediction Step: 494.\n",
      "Prediction Step: 495.\n",
      "Prediction Step: 496.\n",
      "Prediction Step: 497.\n",
      "Prediction Step: 498.\n",
      "Prediction Step: 499.\n",
      "Prediction Step: 500.\n",
      "Prediction Step: 501.\n",
      "Prediction Step: 502.\n",
      "Prediction Step: 503.\n",
      "Prediction Step: 504.\n",
      "Prediction Step: 505.\n",
      "Prediction Step: 506.\n",
      "Prediction Step: 507.\n",
      "Prediction Step: 508.\n",
      "Prediction Step: 509.\n",
      "Prediction Step: 510.\n",
      "Prediction Step: 511.\n",
      "Prediction Step: 512.\n",
      "Prediction Step: 513.\n",
      "Prediction Step: 514.\n",
      "Prediction Step: 515.\n",
      "Prediction Step: 516.\n",
      "Prediction Step: 517.\n",
      "Prediction Step: 518.\n",
      "Prediction Step: 519.\n",
      "Prediction Step: 520.\n",
      "Prediction Step: 521.\n",
      "Prediction Step: 522.\n",
      "Prediction Step: 523.\n",
      "Prediction Step: 524.\n",
      "Prediction Step: 525.\n",
      "Prediction Step: 526.\n",
      "Prediction Step: 527.\n",
      "Prediction Step: 528.\n",
      "Prediction Step: 529.\n",
      "Prediction Step: 530.\n",
      "Prediction Step: 531.\n",
      "Prediction Step: 532.\n",
      "Prediction Step: 533.\n",
      "Prediction Step: 534.\n",
      "Prediction Step: 535.\n",
      "Prediction Step: 536.\n",
      "Prediction Step: 537.\n",
      "Prediction Step: 538.\n",
      "Prediction Step: 539.\n",
      "Prediction Step: 540.\n",
      "Prediction Step: 541.\n",
      "Prediction Step: 542.\n",
      "Prediction Step: 543.\n",
      "Prediction Step: 544.\n",
      "Prediction Step: 545.\n",
      "Prediction Step: 546.\n",
      "Prediction Step: 547.\n",
      "Prediction Step: 548.\n",
      "Prediction Step: 549.\n",
      "Prediction Step: 550.\n",
      "Prediction Step: 551.\n",
      "Prediction Step: 552.\n",
      "Prediction Step: 553.\n",
      "Prediction Step: 554.\n",
      "Prediction Step: 555.\n",
      "Prediction Step: 556.\n",
      "Prediction Step: 557.\n",
      "Prediction Step: 558.\n",
      "Prediction Step: 559.\n",
      "Prediction Step: 560.\n",
      "Prediction Step: 561.\n",
      "Prediction Step: 562.\n",
      "Prediction Step: 563.\n",
      "Prediction Step: 564.\n",
      "Prediction Step: 565.\n",
      "Prediction Step: 566.\n",
      "Prediction Step: 567.\n",
      "Prediction Step: 568.\n",
      "Prediction Step: 569.\n",
      "Prediction Step: 570.\n",
      "Prediction Step: 571.\n",
      "Prediction Step: 572.\n",
      "Prediction Step: 573.\n",
      "Prediction Step: 574.\n",
      "Prediction Step: 575.\n",
      "Prediction Step: 576.\n",
      "Prediction Step: 577.\n",
      "Prediction Step: 578.\n",
      "Prediction Step: 579.\n",
      "Prediction Step: 580.\n",
      "Prediction Step: 581.\n",
      "Prediction Step: 582.\n",
      "Prediction Step: 583.\n",
      "Prediction Step: 584.\n",
      "Prediction Step: 585.\n",
      "Prediction Step: 586.\n",
      "Prediction Step: 587.\n",
      "Prediction Step: 588.\n",
      "Prediction Step: 589.\n",
      "Prediction Step: 590.\n",
      "Prediction Step: 591.\n",
      "Prediction Step: 592.\n",
      "Prediction Step: 593.\n",
      "Prediction Step: 594.\n",
      "Prediction Step: 595.\n",
      "Prediction Step: 596.\n",
      "Prediction Step: 597.\n",
      "Prediction Step: 598.\n",
      "Prediction Step: 599.\n",
      "Prediction Step: 600.\n",
      "Prediction Step: 601.\n",
      "Prediction Step: 602.\n",
      "Prediction Step: 603.\n",
      "Prediction Step: 604.\n",
      "Prediction Step: 605.\n",
      "Prediction Step: 606.\n",
      "Prediction Step: 607.\n",
      "Prediction Step: 608.\n",
      "Prediction Step: 609.\n",
      "Prediction Step: 610.\n",
      "Prediction Step: 611.\n",
      "Prediction Step: 612.\n",
      "Prediction Step: 613.\n",
      "Prediction Step: 614.\n",
      "Prediction Step: 615.\n",
      "Prediction Step: 616.\n",
      "Prediction Step: 617.\n",
      "Prediction Step: 618.\n",
      "Prediction Step: 619.\n",
      "Prediction Step: 620.\n",
      "Prediction Step: 621.\n",
      "Prediction Step: 622.\n",
      "Prediction Step: 623.\n",
      "Prediction Step: 624.\n",
      "Prediction Step: 625.\n",
      "Prediction Step: 626.\n",
      "Prediction Step: 627.\n",
      "Prediction Step: 628.\n",
      "Prediction Step: 629.\n",
      "Prediction Step: 630.\n",
      "Prediction Step: 631.\n",
      "Prediction Step: 632.\n",
      "Prediction Step: 633.\n",
      "Prediction Step: 634.\n",
      "Prediction Step: 635.\n",
      "Prediction Step: 636.\n",
      "Prediction Step: 637.\n",
      "Prediction Step: 638.\n",
      "Prediction Step: 639.\n",
      "Prediction Step: 640.\n",
      "Prediction Step: 641.\n",
      "Prediction Step: 642.\n",
      "Prediction Step: 643.\n",
      "Prediction Step: 644.\n",
      "Prediction Step: 645.\n",
      "Prediction Step: 646.\n",
      "Prediction Step: 647.\n",
      "Prediction Step: 648.\n",
      "Prediction Step: 649.\n",
      "Prediction Step: 650.\n",
      "Prediction Step: 651.\n",
      "Prediction Step: 652.\n",
      "Prediction Step: 653.\n",
      "Prediction Step: 654.\n",
      "Prediction Step: 655.\n",
      "Prediction Step: 656.\n",
      "Prediction Step: 657.\n",
      "Prediction Step: 658.\n",
      "Prediction Step: 659.\n",
      "Prediction Step: 660.\n",
      "Prediction Step: 661.\n",
      "Prediction Step: 662.\n",
      "Prediction Step: 663.\n",
      "Prediction Step: 664.\n",
      "Prediction Step: 665.\n",
      "Prediction Step: 666.\n",
      "Prediction Step: 667.\n",
      "Prediction Step: 668.\n",
      "Prediction Step: 669.\n",
      "Prediction Step: 670.\n",
      "Prediction Step: 671.\n",
      "Prediction Step: 672.\n",
      "Prediction Step: 673.\n",
      "Prediction Step: 674.\n",
      "Prediction Step: 675.\n",
      "Prediction Step: 676.\n",
      "Prediction Step: 677.\n",
      "Prediction Step: 678.\n",
      "Prediction Step: 679.\n",
      "Prediction Step: 680.\n",
      "Prediction Step: 681.\n",
      "Prediction Step: 682.\n",
      "Prediction Step: 683.\n",
      "Prediction Step: 684.\n",
      "Prediction Step: 685.\n",
      "Prediction Step: 686.\n",
      "Prediction Step: 687.\n",
      "Prediction Step: 688.\n",
      "Prediction Step: 689.\n",
      "Prediction Step: 690.\n",
      "Prediction Step: 691.\n",
      "Prediction Step: 692.\n",
      "Prediction Step: 693.\n",
      "Prediction Step: 694.\n",
      "Prediction Step: 695.\n",
      "Prediction Step: 696.\n",
      "Prediction Step: 697.\n",
      "Prediction Step: 698.\n",
      "Prediction Step: 699.\n",
      "Prediction Step: 700.\n",
      "Prediction Step: 701.\n",
      "Prediction Step: 702.\n",
      "Prediction Step: 703.\n",
      "Prediction Step: 704.\n",
      "Prediction Step: 705.\n",
      "Prediction Step: 706.\n",
      "Prediction Step: 707.\n",
      "Prediction Step: 708.\n",
      "Prediction Step: 709.\n",
      "Prediction Step: 710.\n",
      "Prediction Step: 711.\n",
      "Prediction Step: 712.\n",
      "Prediction Step: 713.\n",
      "Prediction Step: 714.\n",
      "Prediction Step: 715.\n",
      "Prediction Step: 716.\n",
      "Prediction Step: 717.\n",
      "Prediction Step: 718.\n",
      "Prediction Step: 719.\n",
      "Prediction Step: 720.\n",
      "Prediction Step: 721.\n",
      "Prediction Step: 722.\n",
      "Prediction Step: 723.\n",
      "Prediction Step: 724.\n",
      "Prediction Step: 725.\n",
      "Prediction Step: 726.\n",
      "Prediction Step: 727.\n",
      "Prediction Step: 728.\n",
      "Prediction Step: 729.\n",
      "Prediction Step: 730.\n",
      "Prediction Step: 731.\n",
      "Prediction Step: 732.\n",
      "Prediction Step: 733.\n",
      "Prediction Step: 734.\n",
      "Prediction Step: 735.\n",
      "Prediction Step: 736.\n",
      "Prediction Step: 737.\n",
      "Prediction Step: 738.\n",
      "Prediction Step: 739.\n",
      "Prediction Step: 740.\n",
      "Prediction Step: 741.\n",
      "Prediction Step: 742.\n",
      "Prediction Step: 743.\n",
      "Prediction Step: 744.\n",
      "Prediction Step: 745.\n",
      "Prediction Step: 746.\n",
      "Prediction Step: 747.\n",
      "Prediction Step: 748.\n",
      "Prediction Step: 749.\n",
      "Prediction Step: 750.\n",
      "Prediction Step: 751.\n",
      "Prediction Step: 752.\n",
      "Prediction Step: 753.\n",
      "Prediction Step: 754.\n",
      "Prediction Step: 755.\n",
      "Prediction Step: 756.\n",
      "Prediction Step: 757.\n",
      "Prediction Step: 758.\n",
      "Prediction Step: 759.\n",
      "Prediction Step: 760.\n",
      "Prediction Step: 761.\n",
      "Prediction Step: 762.\n",
      "Prediction Step: 763.\n",
      "Prediction Step: 764.\n",
      "Prediction Step: 765.\n",
      "Prediction Step: 766.\n",
      "Prediction Step: 767.\n",
      "Prediction Step: 768.\n",
      "Prediction Step: 769.\n",
      "Prediction Step: 770.\n",
      "Prediction Step: 771.\n",
      "Prediction Step: 772.\n",
      "Prediction Step: 773.\n",
      "Prediction Step: 774.\n",
      "Prediction Step: 775.\n",
      "Prediction Step: 776.\n",
      "Prediction Step: 777.\n",
      "Prediction Step: 778.\n",
      "Prediction Step: 779.\n",
      "Prediction Step: 780.\n",
      "Prediction Step: 781.\n",
      "Prediction Step: 782.\n",
      "Prediction Step: 783.\n",
      "Prediction Step: 784.\n",
      "Prediction Step: 785.\n",
      "Prediction Step: 786.\n",
      "Prediction Step: 787.\n",
      "Prediction Step: 788.\n",
      "Prediction Step: 789.\n",
      "Prediction Step: 790.\n",
      "Prediction Step: 791.\n",
      "Prediction Step: 792.\n",
      "Prediction Step: 793.\n",
      "Prediction Step: 794.\n",
      "Prediction Step: 795.\n",
      "Prediction Step: 796.\n",
      "Prediction Step: 797.\n",
      "Prediction Step: 798.\n",
      "Prediction Step: 799.\n",
      "Prediction Step: 800.\n",
      "Prediction Step: 801.\n",
      "Prediction Step: 802.\n",
      "Prediction Step: 803.\n",
      "Prediction Step: 804.\n",
      "Prediction Step: 805.\n",
      "Prediction Step: 806.\n",
      "Prediction Step: 807.\n",
      "Prediction Step: 808.\n",
      "Prediction Step: 809.\n",
      "Prediction Step: 810.\n",
      "Prediction Step: 811.\n",
      "Prediction Step: 812.\n",
      "Prediction Step: 813.\n",
      "Prediction Step: 814.\n",
      "Prediction Step: 815.\n",
      "Prediction Step: 816.\n",
      "Prediction Step: 817.\n",
      "Prediction Step: 818.\n",
      "Prediction Step: 819.\n",
      "Prediction Step: 820.\n",
      "Prediction Step: 821.\n",
      "Prediction Step: 822.\n",
      "Prediction Step: 823.\n",
      "Prediction Step: 824.\n",
      "Prediction Step: 825.\n",
      "Prediction Step: 826.\n",
      "Prediction Step: 827.\n",
      "Prediction Step: 828.\n",
      "Prediction Step: 829.\n",
      "Prediction Step: 830.\n",
      "Prediction Step: 831.\n",
      "Prediction Step: 832.\n",
      "Prediction Step: 833.\n",
      "Prediction Step: 834.\n",
      "Prediction Step: 835.\n",
      "Prediction Step: 836.\n",
      "Prediction Step: 837.\n",
      "Prediction Step: 838.\n",
      "Prediction Step: 839.\n",
      "Prediction Step: 840.\n",
      "Prediction Step: 841.\n",
      "Prediction Step: 842.\n",
      "Prediction Step: 843.\n",
      "Prediction Step: 844.\n",
      "Prediction Step: 845.\n",
      "Prediction Step: 846.\n",
      "Prediction Step: 847.\n",
      "Prediction Step: 848.\n",
      "Prediction Step: 849.\n",
      "Prediction Step: 850.\n",
      "Prediction Step: 851.\n",
      "Prediction Step: 852.\n",
      "Prediction Step: 853.\n",
      "Prediction Step: 854.\n",
      "Prediction Step: 855.\n",
      "Prediction Step: 856.\n",
      "Prediction Step: 857.\n",
      "Prediction Step: 858.\n",
      "Prediction Step: 859.\n",
      "Prediction Step: 860.\n",
      "Prediction Step: 861.\n",
      "Prediction Step: 862.\n",
      "Prediction Step: 863.\n",
      "Prediction Step: 864.\n",
      "Prediction Step: 865.\n",
      "Prediction Step: 866.\n",
      "Prediction Step: 867.\n",
      "Prediction Step: 868.\n",
      "Prediction Step: 869.\n",
      "Prediction Step: 870.\n",
      "Prediction Step: 871.\n",
      "Prediction Step: 872.\n",
      "Prediction Step: 873.\n",
      "Prediction Step: 874.\n",
      "Prediction Step: 875.\n",
      "Prediction Step: 876.\n",
      "Prediction Step: 877.\n",
      "Prediction Step: 878.\n",
      "Prediction Step: 879.\n",
      "Prediction Step: 880.\n",
      "Prediction Step: 881.\n",
      "Prediction Step: 882.\n",
      "Prediction Step: 883.\n",
      "Prediction Step: 884.\n",
      "Prediction Step: 885.\n",
      "Prediction Step: 886.\n",
      "Prediction Step: 887.\n",
      "Prediction Step: 888.\n",
      "Prediction Step: 889.\n",
      "Prediction Step: 890.\n",
      "Prediction Step: 891.\n",
      "Prediction Step: 892.\n",
      "Prediction Step: 893.\n",
      "Prediction Step: 894.\n",
      "Prediction Step: 895.\n",
      "Prediction Step: 896.\n",
      "Prediction Step: 897.\n",
      "Prediction Step: 898.\n",
      "Prediction Step: 899.\n",
      "Prediction Step: 900.\n",
      "Prediction Step: 901.\n",
      "Prediction Step: 902.\n",
      "Prediction Step: 903.\n",
      "Prediction Step: 904.\n",
      "Prediction Step: 905.\n",
      "Prediction Step: 906.\n",
      "Prediction Step: 907.\n",
      "Prediction Step: 908.\n",
      "Prediction Step: 909.\n",
      "Prediction Step: 910.\n",
      "Prediction Step: 911.\n",
      "Prediction Step: 912.\n",
      "Prediction Step: 913.\n",
      "Prediction Step: 914.\n",
      "Prediction Step: 915.\n",
      "Prediction Step: 916.\n",
      "Prediction Step: 917.\n",
      "Prediction Step: 918.\n",
      "Prediction Step: 919.\n",
      "Prediction Step: 920.\n",
      "Prediction Step: 921.\n",
      "Prediction Step: 922.\n",
      "Prediction Step: 923.\n",
      "Prediction Step: 924.\n",
      "Prediction Step: 925.\n",
      "Prediction Step: 926.\n",
      "Prediction Step: 927.\n",
      "Prediction Step: 928.\n",
      "Prediction Step: 929.\n",
      "Prediction Step: 930.\n",
      "Prediction Step: 931.\n",
      "Prediction Step: 932.\n",
      "Prediction Step: 933.\n",
      "Prediction Step: 934.\n",
      "Prediction Step: 935.\n",
      "Prediction Step: 936.\n",
      "Prediction Step: 937.\n",
      "Prediction Step: 938.\n",
      "Prediction Step: 939.\n",
      "Prediction Step: 940.\n",
      "Prediction Step: 941.\n",
      "Prediction Step: 942.\n",
      "Prediction Step: 943.\n",
      "Prediction Step: 944.\n",
      "Prediction Step: 945.\n",
      "Prediction Step: 946.\n",
      "Prediction Step: 947.\n",
      "Prediction Step: 948.\n",
      "Prediction Step: 949.\n",
      "Prediction Step: 950.\n",
      "Prediction Step: 951.\n",
      "Prediction Step: 952.\n",
      "Prediction Step: 953.\n",
      "Prediction Step: 954.\n",
      "Prediction Step: 955.\n",
      "Prediction Step: 956.\n",
      "Prediction Step: 957.\n",
      "Prediction Step: 958.\n",
      "Prediction Step: 959.\n",
      "Prediction Step: 960.\n",
      "Prediction Step: 961.\n",
      "Prediction Step: 962.\n",
      "Prediction Step: 963.\n",
      "Prediction Step: 964.\n",
      "Prediction Step: 965.\n",
      "Prediction Step: 966.\n",
      "Prediction Step: 967.\n",
      "Prediction Step: 968.\n",
      "Prediction Step: 969.\n",
      "Prediction Step: 970.\n",
      "Prediction Step: 971.\n",
      "Prediction Step: 972.\n",
      "Prediction Step: 973.\n",
      "Prediction Step: 974.\n",
      "Prediction Step: 975.\n",
      "Prediction Step: 976.\n",
      "Prediction Step: 977.\n",
      "Prediction Step: 978.\n",
      "Prediction Step: 979.\n",
      "Prediction Step: 980.\n",
      "Prediction Step: 981.\n",
      "Prediction Step: 982.\n",
      "Prediction Step: 983.\n",
      "Prediction Step: 984.\n",
      "Prediction Step: 985.\n",
      "Prediction Step: 986.\n",
      "Prediction Step: 987.\n",
      "Prediction Step: 988.\n",
      "Prediction Step: 989.\n",
      "Prediction Step: 990.\n",
      "Prediction Step: 991.\n",
      "Prediction Step: 992.\n",
      "Prediction Step: 993.\n",
      "Prediction Step: 994.\n",
      "Prediction Step: 995.\n",
      "Prediction Step: 996.\n",
      "Prediction Step: 997.\n",
      "Prediction Step: 998.\n",
      "Prediction Step: 999.\n",
      "Prediction Step: 1000.\n",
      "Prediction Step: 1001.\n",
      "Prediction Step: 1002.\n",
      "Prediction Step: 1003.\n",
      "Prediction Step: 1004.\n",
      "Prediction Step: 1005.\n",
      "Prediction Step: 1006.\n",
      "Prediction Step: 1007.\n",
      "Prediction Step: 1008.\n",
      "Prediction Step: 1009.\n",
      "Prediction Step: 1010.\n",
      "Prediction Step: 1011.\n",
      "Prediction Step: 1012.\n",
      "Prediction Step: 1013.\n",
      "Prediction Step: 1014.\n",
      "Prediction Step: 1015.\n",
      "Prediction Step: 1016.\n",
      "Prediction Step: 1017.\n",
      "Prediction Step: 1018.\n",
      "Prediction Step: 1019.\n",
      "Prediction Step: 1020.\n",
      "Prediction Step: 1021.\n",
      "Prediction Step: 1022.\n",
      "Prediction Step: 1023.\n",
      "Prediction Step: 1024.\n",
      "Prediction Step: 1025.\n",
      "Prediction Step: 1026.\n",
      "Prediction Step: 1027.\n",
      "Prediction Step: 1028.\n",
      "Prediction Step: 1029.\n",
      "Prediction Step: 1030.\n",
      "Prediction Step: 1031.\n",
      "Prediction Step: 1032.\n",
      "Prediction Step: 1033.\n",
      "Prediction Step: 1034.\n",
      "Prediction Step: 1035.\n",
      "Prediction Step: 1036.\n",
      "Prediction Step: 1037.\n",
      "Prediction Step: 1038.\n",
      "Prediction Step: 1039.\n",
      "Prediction Step: 1040.\n",
      "Prediction Step: 1041.\n",
      "Prediction Step: 1042.\n",
      "Prediction Step: 1043.\n",
      "Prediction Step: 1044.\n",
      "Prediction Step: 1045.\n",
      "Prediction Step: 1046.\n",
      "Prediction Step: 1047.\n",
      "Prediction Step: 1048.\n",
      "Prediction Step: 1049.\n",
      "Prediction Step: 1050.\n",
      "Prediction Step: 1051.\n",
      "Prediction Step: 1052.\n",
      "Prediction Step: 1053.\n",
      "Prediction Step: 1054.\n",
      "Prediction Step: 1055.\n",
      "Prediction Step: 1056.\n",
      "Prediction Step: 1057.\n",
      "Prediction Step: 1058.\n",
      "Prediction Step: 1059.\n",
      "Prediction Step: 1060.\n",
      "Prediction Step: 1061.\n",
      "Prediction Step: 1062.\n",
      "Prediction Step: 1063.\n",
      "Prediction Step: 1064.\n",
      "Prediction Step: 1065.\n",
      "Prediction Step: 1066.\n",
      "Prediction Step: 1067.\n",
      "Prediction Step: 1068.\n",
      "Prediction Step: 1069.\n",
      "Prediction Step: 1070.\n",
      "Prediction Step: 1071.\n",
      "Prediction Step: 1072.\n",
      "Prediction Step: 1073.\n",
      "Prediction Step: 1074.\n",
      "Prediction Step: 1075.\n",
      "Prediction Step: 1076.\n",
      "Prediction Step: 1077.\n",
      "Prediction Step: 1078.\n",
      "Prediction Step: 1079.\n",
      "Prediction Step: 1080.\n",
      "Prediction Step: 1081.\n",
      "Prediction Step: 1082.\n",
      "Prediction Step: 1083.\n",
      "Prediction Step: 1084.\n",
      "Prediction Step: 1085.\n",
      "Prediction Step: 1086.\n",
      "Prediction Step: 1087.\n",
      "Prediction Step: 1088.\n",
      "Prediction Step: 1089.\n",
      "Prediction Step: 1090.\n",
      "Prediction Step: 1091.\n",
      "Prediction Step: 1092.\n",
      "Prediction Step: 1093.\n",
      "Prediction Step: 1094.\n",
      "Prediction Step: 1095.\n",
      "Prediction Step: 1096.\n",
      "Prediction Step: 1097.\n",
      "Prediction Step: 1098.\n",
      "Prediction Step: 1099.\n",
      "Prediction Step: 1100.\n",
      "Prediction Step: 1101.\n",
      "Prediction Step: 1102.\n",
      "Prediction Step: 1103.\n",
      "Prediction Step: 1104.\n",
      "Prediction Step: 1105.\n",
      "Prediction Step: 1106.\n",
      "Prediction Step: 1107.\n",
      "Prediction Step: 1108.\n",
      "Prediction Step: 1109.\n",
      "Prediction Step: 1110.\n",
      "Prediction Step: 1111.\n",
      "Prediction Step: 1112.\n",
      "Prediction Step: 1113.\n",
      "Prediction Step: 1114.\n",
      "Prediction Step: 1115.\n",
      "Prediction Step: 1116.\n",
      "Prediction Step: 1117.\n",
      "Prediction Step: 1118.\n",
      "Prediction Step: 1119.\n",
      "Prediction Step: 1120.\n",
      "Prediction Step: 1121.\n",
      "Prediction Step: 1122.\n",
      "Prediction Step: 1123.\n",
      "Prediction Step: 1124.\n",
      "Prediction Step: 1125.\n",
      "Prediction Step: 1126.\n",
      "Prediction Step: 1127.\n",
      "Prediction Step: 1128.\n",
      "Prediction Step: 1129.\n",
      "Prediction Step: 1130.\n",
      "Prediction Step: 1131.\n",
      "Prediction Step: 1132.\n",
      "Prediction Step: 1133.\n",
      "Prediction Step: 1134.\n",
      "Prediction Step: 1135.\n",
      "Prediction Step: 1136.\n",
      "Prediction Step: 1137.\n",
      "Prediction Step: 1138.\n",
      "Prediction Step: 1139.\n",
      "Prediction Step: 1140.\n",
      "Prediction Step: 1141.\n",
      "Prediction Step: 1142.\n",
      "Prediction Step: 1143.\n",
      "Prediction Step: 1144.\n",
      "Prediction Step: 1145.\n",
      "Prediction Step: 1146.\n",
      "Prediction Step: 1147.\n",
      "Prediction Step: 1148.\n",
      "Prediction Step: 1149.\n",
      "Prediction Step: 1150.\n",
      "Prediction Step: 1151.\n",
      "Prediction Step: 1152.\n",
      "Prediction Step: 1153.\n",
      "Prediction Step: 1154.\n",
      "Prediction Step: 1155.\n",
      "Prediction Step: 1156.\n",
      "Prediction Step: 1157.\n",
      "Prediction Step: 1158.\n",
      "Prediction Step: 1159.\n",
      "Prediction Step: 1160.\n",
      "Prediction Step: 1161.\n",
      "Prediction Step: 1162.\n",
      "Prediction Step: 1163.\n",
      "Prediction Step: 1164.\n",
      "Prediction Step: 1165.\n",
      "Prediction Step: 1166.\n",
      "Prediction Step: 1167.\n",
      "Prediction Step: 1168.\n",
      "Prediction Step: 1169.\n",
      "Prediction Step: 1170.\n",
      "Prediction Step: 1171.\n",
      "Prediction Step: 1172.\n",
      "Prediction Step: 1173.\n",
      "Prediction Step: 1174.\n",
      "Prediction Step: 1175.\n",
      "Prediction Step: 1176.\n",
      "Prediction Step: 1177.\n",
      "Prediction Step: 1178.\n",
      "Prediction Step: 1179.\n",
      "Prediction Step: 1180.\n",
      "Prediction Step: 1181.\n",
      "Prediction Step: 1182.\n",
      "Prediction Step: 1183.\n",
      "Prediction Step: 1184.\n",
      "Prediction Step: 1185.\n",
      "Prediction Step: 1186.\n",
      "Prediction Step: 1187.\n",
      "Prediction Step: 1188.\n",
      "Prediction Step: 1189.\n",
      "Prediction Step: 1190.\n",
      "Prediction Step: 1191.\n",
      "Prediction Step: 1192.\n",
      "Prediction Step: 1193.\n",
      "Prediction Step: 1194.\n",
      "Prediction Step: 1195.\n",
      "Prediction Step: 1196.\n",
      "Prediction Step: 1197.\n",
      "Prediction Step: 1198.\n",
      "Prediction Step: 1199.\n",
      "Prediction Step: 1200.\n",
      "Prediction Step: 1201.\n",
      "Prediction Step: 1202.\n",
      "Prediction Step: 1203.\n",
      "Prediction Step: 1204.\n",
      "Prediction Step: 1205.\n",
      "Prediction Step: 1206.\n",
      "Prediction Step: 1207.\n",
      "Prediction Step: 1208.\n",
      "Prediction Step: 1209.\n",
      "Prediction Step: 1210.\n",
      "Prediction Step: 1211.\n",
      "Prediction Step: 1212.\n",
      "Prediction Step: 1213.\n",
      "Prediction Step: 1214.\n",
      "Prediction Step: 1215.\n",
      "Prediction Step: 1216.\n",
      "Prediction Step: 1217.\n",
      "Prediction Step: 1218.\n",
      "Prediction Step: 1219.\n",
      "Prediction Step: 1220.\n",
      "Prediction Step: 1221.\n",
      "Prediction Step: 1222.\n",
      "Prediction Step: 1223.\n",
      "Prediction Step: 1224.\n",
      "Prediction Step: 1225.\n",
      "Prediction Step: 1226.\n",
      "Prediction Step: 1227.\n",
      "Prediction Step: 1228.\n",
      "Prediction Step: 1229.\n",
      "Prediction Step: 1230.\n",
      "Prediction Step: 1231.\n",
      "Prediction Step: 1232.\n",
      "Prediction Step: 1233.\n",
      "Prediction Step: 1234.\n",
      "Prediction Step: 1235.\n",
      "Prediction Step: 1236.\n",
      "Prediction Step: 1237.\n",
      "Prediction Step: 1238.\n",
      "Prediction Step: 1239.\n",
      "Prediction Step: 1240.\n",
      "Prediction Step: 1241.\n",
      "Prediction Step: 1242.\n",
      "Prediction Step: 1243.\n",
      "Prediction Step: 1244.\n",
      "Prediction Step: 1245.\n",
      "Prediction Step: 1246.\n",
      "Prediction Step: 1247.\n",
      "Prediction Step: 1248.\n",
      "Prediction Step: 1249.\n",
      "Prediction Step: 1250.\n",
      "Prediction Step: 1251.\n",
      "Prediction Step: 1252.\n",
      "Prediction Step: 1253.\n",
      "Prediction Step: 1254.\n",
      "Prediction Step: 1255.\n",
      "Prediction Step: 1256.\n",
      "Prediction Step: 1257.\n",
      "Prediction Step: 1258.\n",
      "Prediction Step: 1259.\n",
      "Prediction Step: 1260.\n",
      "Prediction Step: 1261.\n",
      "Prediction Step: 1262.\n",
      "Prediction Step: 1263.\n",
      "Prediction Step: 1264.\n",
      "Prediction Step: 1265.\n",
      "Prediction Step: 1266.\n",
      "Prediction Step: 1267.\n",
      "Prediction Step: 1268.\n",
      "Prediction Step: 1269.\n",
      "Prediction Step: 1270.\n",
      "Prediction Step: 1271.\n",
      "Prediction Step: 1272.\n",
      "Prediction Step: 1273.\n",
      "Prediction Step: 1274.\n",
      "Prediction Step: 1275.\n",
      "Prediction Step: 1276.\n",
      "Prediction Step: 1277.\n",
      "Prediction Step: 1278.\n",
      "Prediction Step: 1279.\n",
      "Prediction Step: 1280.\n",
      "Prediction Step: 1281.\n",
      "Prediction Step: 1282.\n",
      "Prediction Step: 1283.\n",
      "Prediction Step: 1284.\n",
      "Prediction Step: 1285.\n",
      "Prediction Step: 1286.\n",
      "Prediction Step: 1287.\n",
      "Prediction Step: 1288.\n",
      "Prediction Step: 1289.\n",
      "Prediction Step: 1290.\n",
      "Prediction Step: 1291.\n",
      "Prediction Step: 1292.\n",
      "Prediction Step: 1293.\n",
      "Prediction Step: 1294.\n",
      "Prediction Step: 1295.\n",
      "Prediction Step: 1296.\n",
      "Prediction Step: 1297.\n",
      "Prediction Step: 1298.\n",
      "Prediction Step: 1299.\n",
      "Prediction Step: 1300.\n",
      "Prediction Step: 1301.\n",
      "Prediction Step: 1302.\n",
      "Prediction Step: 1303.\n",
      "Prediction Step: 1304.\n",
      "Prediction Step: 1305.\n",
      "Prediction Step: 1306.\n",
      "Prediction Step: 1307.\n",
      "Prediction Step: 1308.\n",
      "Prediction Step: 1309.\n",
      "Prediction Step: 1310.\n",
      "Prediction Step: 1311.\n",
      "Prediction Step: 1312.\n",
      "Prediction Step: 1313.\n",
      "Prediction Step: 1314.\n",
      "Prediction Step: 1315.\n",
      "Prediction Step: 1316.\n",
      "Prediction Step: 1317.\n",
      "Prediction Step: 1318.\n",
      "Prediction Step: 1319.\n",
      "Prediction Step: 1320.\n",
      "Prediction Step: 1321.\n",
      "Prediction Step: 1322.\n",
      "Prediction Step: 1323.\n",
      "Prediction Step: 1324.\n",
      "Prediction Step: 1325.\n",
      "Prediction Step: 1326.\n",
      "Prediction Step: 1327.\n",
      "Prediction Step: 1328.\n",
      "Prediction Step: 1329.\n",
      "Prediction Step: 1330.\n",
      "Prediction Step: 1331.\n",
      "Prediction Step: 1332.\n",
      "Prediction Step: 1333.\n",
      "Prediction Step: 1334.\n",
      "Prediction Step: 1335.\n",
      "Prediction Step: 1336.\n",
      "Prediction Step: 1337.\n",
      "Prediction Step: 1338.\n",
      "Prediction Step: 1339.\n",
      "Prediction Step: 1340.\n",
      "Prediction Step: 1341.\n",
      "Prediction Step: 1342.\n",
      "Prediction Step: 1343.\n",
      "Prediction Step: 1344.\n",
      "Prediction Step: 1345.\n",
      "Prediction Step: 1346.\n",
      "Prediction Step: 1347.\n",
      "Prediction Step: 1348.\n",
      "Prediction Step: 1349.\n",
      "Prediction Step: 1350.\n",
      "Prediction Step: 1351.\n",
      "Prediction Step: 1352.\n",
      "Prediction Step: 1353.\n",
      "Prediction Step: 1354.\n",
      "Prediction Step: 1355.\n",
      "Prediction Step: 1356.\n",
      "Prediction Step: 1357.\n",
      "Prediction Step: 1358.\n",
      "Prediction Step: 1359.\n",
      "Prediction Step: 1360.\n",
      "Prediction Step: 1361.\n",
      "Prediction Step: 1362.\n",
      "Prediction Step: 1363.\n",
      "Prediction Step: 1364.\n",
      "Prediction Step: 1365.\n",
      "Prediction Step: 1366.\n",
      "Prediction Step: 1367.\n",
      "Prediction Step: 1368.\n",
      "Prediction Step: 1369.\n",
      "Prediction Step: 1370.\n",
      "Prediction Step: 1371.\n",
      "Prediction Step: 1372.\n",
      "Prediction Step: 1373.\n",
      "Prediction Step: 1374.\n",
      "Prediction Step: 1375.\n",
      "Prediction Step: 1376.\n",
      "Prediction Step: 1377.\n",
      "Prediction Step: 1378.\n",
      "Prediction Step: 1379.\n",
      "Prediction Step: 1380.\n",
      "Prediction Step: 1381.\n",
      "Prediction Step: 1382.\n",
      "Prediction Step: 1383.\n",
      "Prediction Step: 1384.\n",
      "Prediction Step: 1385.\n",
      "Prediction Step: 1386.\n",
      "Prediction Step: 1387.\n",
      "Prediction Step: 1388.\n",
      "Prediction Step: 1389.\n",
      "Prediction Step: 1390.\n",
      "Prediction Step: 1391.\n",
      "Prediction Step: 1392.\n",
      "Prediction Step: 1393.\n",
      "Prediction Step: 1394.\n",
      "Prediction Step: 1395.\n",
      "Prediction Step: 1396.\n",
      "Prediction Step: 1397.\n",
      "Prediction Step: 1398.\n",
      "Prediction Step: 1399.\n",
      "Prediction Step: 1400.\n",
      "Prediction Step: 1401.\n",
      "Prediction Step: 1402.\n",
      "Prediction Step: 1403.\n",
      "Prediction Step: 1404.\n",
      "Prediction Step: 1405.\n",
      "Prediction Step: 1406.\n",
      "Prediction Step: 1407.\n",
      "Prediction Step: 1408.\n",
      "Prediction Step: 1409.\n",
      "Prediction Step: 1410.\n",
      "Prediction Step: 1411.\n",
      "Prediction Step: 1412.\n",
      "Prediction Step: 1413.\n",
      "Prediction Step: 1414.\n",
      "Prediction Step: 1415.\n",
      "Prediction Step: 1416.\n",
      "Prediction Step: 1417.\n",
      "Prediction Step: 1418.\n",
      "Prediction Step: 1419.\n",
      "Prediction Step: 1420.\n",
      "Prediction Step: 1421.\n",
      "Prediction Step: 1422.\n",
      "Prediction Step: 1423.\n",
      "Prediction Step: 1424.\n",
      "Prediction Step: 1425.\n",
      "Prediction Step: 1426.\n",
      "Prediction Step: 1427.\n",
      "Prediction Step: 1428.\n",
      "Prediction Step: 1429.\n",
      "Prediction Step: 1430.\n",
      "Prediction Step: 1431.\n",
      "Prediction Step: 1432.\n",
      "Prediction Step: 1433.\n",
      "Prediction Step: 1434.\n",
      "Prediction Step: 1435.\n",
      "Prediction Step: 1436.\n",
      "Prediction Step: 1437.\n",
      "Prediction Step: 1438.\n",
      "Prediction Step: 1439.\n",
      "Prediction Step: 1440.\n",
      "Prediction Step: 1441.\n",
      "Prediction Step: 1442.\n",
      "Prediction Step: 1443.\n",
      "Prediction Step: 1444.\n",
      "Prediction Step: 1445.\n",
      "Prediction Step: 1446.\n",
      "Prediction Step: 1447.\n",
      "Prediction Step: 1448.\n",
      "Prediction Step: 1449.\n",
      "Prediction Step: 1450.\n",
      "Prediction Step: 1451.\n",
      "Prediction Step: 1452.\n",
      "Prediction Step: 1453.\n",
      "Prediction Step: 1454.\n",
      "Prediction Step: 1455.\n",
      "Prediction Step: 1456.\n",
      "Prediction Step: 1457.\n",
      "Prediction Step: 1458.\n",
      "Prediction Step: 1459.\n",
      "Prediction Step: 1460.\n",
      "Prediction Step: 1461.\n",
      "Prediction Step: 1462.\n",
      "Prediction Step: 1463.\n",
      "Prediction Step: 1464.\n",
      "Prediction Step: 1465.\n",
      "Prediction Step: 1466.\n",
      "Prediction Step: 1467.\n",
      "Prediction Step: 1468.\n",
      "Prediction Step: 1469.\n",
      "Prediction Step: 1470.\n",
      "Prediction Step: 1471.\n",
      "Prediction Step: 1472.\n",
      "Prediction Step: 1473.\n",
      "Prediction Step: 1474.\n",
      "Prediction Step: 1475.\n",
      "Prediction Step: 1476.\n",
      "Prediction Step: 1477.\n",
      "Prediction Step: 1478.\n",
      "Prediction Step: 1479.\n",
      "Prediction Step: 1480.\n",
      "Prediction Step: 1481.\n",
      "Prediction Step: 1482.\n",
      "Prediction Step: 1483.\n",
      "Prediction Step: 1484.\n",
      "Prediction Step: 1485.\n"
     ]
    }
   ],
   "source": [
    "# Run on the Test Data.\n",
    "model.load_from_checkpoint(model_path, \"best_step\")\n",
    "model.to_device()\n",
    "test_dataloader = create_dataloader(model, test_file_name=test_file_name)\n",
    "test_loop(\n",
    "    model=model,\n",
    "    mode=\"test\",\n",
    "    model_path=model_path,\n",
    "    prediction_file_name=\"test.predicted.tsv\",\n",
    "    test_dataloader=test_dataloader,\n",
    "    metric=qa_metric,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
