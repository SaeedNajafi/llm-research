{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e025db9f-beeb-4e8b-a79f-8b6540205183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-03-13 13:39:29.663667: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-13 13:39:29.915884: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-13 13:39:29.915946: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-13 13:39:29.931816: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-13 13:39:29.995714: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-13 13:39:31.985506: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from typing import Any, Dict, Iterator, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from bitsandbytes.optim.adamw import PagedAdamW8bit\n",
    "from src.galore_torch import GaLoreAdamW8bit\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    ")\n",
    "import os, json, csv\n",
    "import pandas as pd\n",
    "from src.base_lm import BaseLM\n",
    "from src.general_utils import DictDataset, test_loop, train_loop\n",
    "from src.model_utils import clear_cache, llama2_log_of_labels, lm_logits, mlm_log_of_labels, set_random_seed\n",
    "from src.general_utils import white_space_fix\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44e045dc-35f5-40b7-aafe-d617348118b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar 13 13:39:41 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A40          On   | 00000000:06:00.0 Off |                    0 |\n",
      "|  0%   47C    P8    33W / 300W |      2MiB / 46068MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9079fb92-dbfb-411c-802c-5b4c76375ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (0.21.3)\n",
      "Requirement already satisfied: filelock in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from huggingface_hub) (2024.2.0)\n",
      "Requirement already satisfied: requests in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from huggingface_hub) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from huggingface_hub) (4.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from requests->huggingface_hub) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages (from requests->huggingface_hub) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6c335fa-2cab-41f7-899c-f568d9968365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /h/snajafi/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token=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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e433b3a3-526f-4e23-9baa-92d1a97868c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 4\n",
    "eval_batch_size = 4\n",
    "lm_input_max_length = 1024\n",
    "lm_output_max_length = 512\n",
    "lm_top_p = 0.9\n",
    "temperature = 0.5\n",
    "metric_device = \"cuda:1\"\n",
    "metric_batch_size = 8\n",
    "learning_rate = 0.00005\n",
    "\n",
    "# folder to store models and predictions.\n",
    "model_path = \"/scratch/ssd004/scratch/snajafi/checkpoints/gemma-prompt-recovery\"\n",
    "\n",
    "# related to lora\n",
    "r = 16\n",
    "lora_alpha = 8\n",
    "lora_dropout = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1573dcfc-4511-4170-8014-9b2f27e598c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load LM efficiently.\"\"\"\n",
    "\n",
    "# Make sure we have some tokens defined for the LM, if not defined in the model.\n",
    "_EXTRA_TOKENS = {\n",
    "    \"pad_token\": \"<pad>\",\n",
    "    \"mask_token\": \"<mask>\",\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"</s>\",\n",
    "    \"unk_token\": \"<unk>\",\n",
    "    \"cls_token\": \"<CLS>\",\n",
    "}\n",
    "\n",
    "target_modules = [\"q_proj\", \"v_proj\", \"o_proj\", \"k_proj\"]\n",
    "\n",
    "\n",
    "def load_peft_model(\n",
    "    model: PreTrainedModel,\n",
    "    adapter_name: str = \"lora\",\n",
    "    is_trainable: bool = False,\n",
    "    model_type: str = \"causal_lm\",\n",
    "    lora_target_modules: List[str] = target_modules,\n",
    ") -> torch.nn.Module:\n",
    "    \"\"\"Load a trained PEFT adapter to the base model and return the PeftModel.\n",
    "\n",
    "    Args:\n",
    "    ----\n",
    "        model: the main model.\n",
    "        num_quantized_bits: number of bits in the loaded model.\n",
    "        adapter_name: e.g. lora.\n",
    "        is_trainable: train or inference mode.\n",
    "        model_type: causal lm or seq-to-seq.\n",
    "        lora_target_modules: which modules to train with lora.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "        The PEFT model and tokenizer.\n",
    "    \"\"\"\n",
    "    if model_type == \"causal_lm\":\n",
    "        task_type = TaskType.CAUSAL_LM\n",
    "    elif model_type == \"seq_to_seq_lm\":\n",
    "        task_type = TaskType.SEQ_2_SEQ_LM\n",
    "\n",
    "    if adapter_name == \"lora\":\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=task_type,\n",
    "            inference_mode=not is_trainable,\n",
    "            r=r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            lora_dropout=lora_dropout,\n",
    "            bias=\"none\",\n",
    "            init_lora_weights=True,\n",
    "            target_modules=lora_target_modules,\n",
    "        )\n",
    "\n",
    "    peft_model = get_peft_model(model, peft_config)\n",
    "    peft_model.print_trainable_parameters()\n",
    "    return peft_model\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer(\n",
    "    model_id: str, model_type: str, model_dtype: torch.dtype, attn_implementation: str, load_in_4bit: Optional[bool] = True\n",
    ") -> Tuple[PreTrainedModel, PreTrainedTokenizer]:\n",
    "    \"\"\"Load the model and tokenizer.\n",
    "\n",
    "    Args:\n",
    "    ----\n",
    "        model_id: the id for the pre-trained model.\n",
    "        model_type: causal lm or seq_to_seq_lm.\n",
    "        model_dtype: model data type.\n",
    "        load_in_4bit: Whether to load in 4 bit quantization.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "        The model and tokenizer.\n",
    "    \"\"\"\n",
    "    # load model\n",
    "    if model_type == \"causal_lm\":\n",
    "        ModelClass = AutoModelForCausalLM\n",
    "    elif model_type == \"seq_to_seq_lm\":\n",
    "        ModelClass = AutoModelForSeq2SeqLM\n",
    "    model_args: Dict[str, Any] = {\"use_cache\": False, \"torch_dtype\": model_dtype, \"attn_implementation\": attn_implementation}\n",
    "    if load_in_4bit:\n",
    "        quant_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=model_args[\"torch_dtype\"],\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "        model_args[\"quantization_config\"] = quant_config\n",
    "    model = ModelClass.from_pretrained(\n",
    "        model_id,\n",
    "        **model_args,\n",
    "    )\n",
    "\n",
    "    # load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.add_special_tokens(_EXTRA_TOKENS)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        # extend embeddings to a multiple so we use Tensor cores\n",
    "        multiple = 64 if \"A100\" in torch.cuda.get_device_name() else 8\n",
    "        model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=multiple)\n",
    "    else:\n",
    "        raise Exception(\"No CUDA Found!\")\n",
    "\n",
    "    # re-define token ids for the model.\n",
    "    for extra_token_key, extra_token_val in _EXTRA_TOKENS.items():\n",
    "        extra_token_id = tokenizer.convert_tokens_to_ids([extra_token_val])[0]\n",
    "        model.config.__setattr__(f\"{extra_token_key}_id\", extra_token_id)\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ee99f5f-7734-4ca9-8314-c3460d5f2368",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gemma(BaseLM):\n",
    "    \"\"\"Class to implement Gemma for generation tasks.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        mode: str,\n",
    "        device: str,\n",
    "        seed: int = 42,\n",
    "    ) -> None:\n",
    "        super().__init__(device, \"main_lm\", seed)\n",
    "        self.device = device\n",
    "        model, tokenizer = load_model_and_tokenizer(\n",
    "            model_id=\"/model-weights/gemma-7b-it\",\n",
    "            model_type=\"causal_lm\",\n",
    "            model_dtype=torch.bfloat16,\n",
    "            attn_implementation=\"flash_attention_2\",\n",
    "            load_in_4bit=False,\n",
    "        )\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.optimizer = PagedAdamW8bit(self.model.parameters(), lr=learning_rate)\n",
    "        self.scheduler = CosineAnnealingWarmRestarts(self.optimizer, T_0=10, eta_min=learning_rate / 5.0)\n",
    "\n",
    "    def prepare_text(self, texts: List[str], output_texts: List[str], ids: List[str], instructions: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Convert texts to ids and return the dataset required for training\n",
    "        and inference.\"\"\"\n",
    "        input_texts = [f\"{instructions[idx]} {texts[idx]}\" for idx in range(len(instructions))]\n",
    "        template = \"<bos><start_of_turn>user\\n{user_input}<end_of_turn>\\n<start_of_turn>model\"\n",
    "        inputs_for_training = [\n",
    "            f\"{template.format(user_input=input_texts[idx])}\\n{output_texts[idx]}<end_of_turn>\" for idx in range(len(input_texts))\n",
    "        ]\n",
    "        inputs_for_generation = [\n",
    "            template.format(user_input=input_texts[idx]) for idx in range(len(input_texts))\n",
    "        ]\n",
    "\n",
    "        input_encodings = self.tokenizer(\n",
    "            inputs_for_training,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=lm_input_max_length + lm_output_max_length,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        input_encodings_for_generation = self.tokenizer(\n",
    "            inputs_for_generation,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=lm_input_max_length,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        data = {\n",
    "            \"lm_input_ids_for_train\": input_encodings.input_ids,\n",
    "            \"lm_attention_mask_for_train\": input_encodings.attention_mask,\n",
    "            \"lm_input_ids_for_generation\": input_encodings_for_generation.input_ids,\n",
    "            \"lm_attention_mask_for_generation\": input_encodings_for_generation.attention_mask,\n",
    "        }\n",
    "        return data\n",
    "\n",
    "    def train(self, batch: torch.utils.data.Dataset) -> torch.Tensor:\n",
    "        \"\"\"Using the Gemma, run a forward computation over the batch, compute\n",
    "        the log probability over the batch.\n",
    "\n",
    "        This will be used for training.\n",
    "        \"\"\"\n",
    "        self.train_mode_on()\n",
    "        loaded_batch = self.data_to_device(batch, keys=[\"lm_input_ids_for_train\", \"lm_attention_mask_for_train\",\n",
    "                                                        \"lm_attention_mask_for_generation\"])\n",
    "        input_ids = loaded_batch[\"lm_input_ids_for_train\"]\n",
    "        attention_mask = loaded_batch[\"lm_attention_mask_for_train\"]\n",
    "        original_len_without_answer = torch.sum(loaded_batch[\"lm_attention_mask_for_generation\"], dim=1)\n",
    "        with torch.set_grad_enabled(True):\n",
    "            logits = lm_logits(\n",
    "                model=self.model,\n",
    "                input_ids=input_ids,\n",
    "                input_mask=attention_mask,\n",
    "            )\n",
    "            batch_size, seq_len = input_ids.size()\n",
    "            masked_labels = input_ids.masked_fill(input_ids == self.tokenizer.pad_token_id, -100)\n",
    "            prompt_mask = torch.arange(seq_len, device=self.device).expand(batch_size, seq_len) < original_len_without_answer.unsqueeze(1)\n",
    "            masked_labels = masked_labels.masked_fill(prompt_mask == 1, -100)\n",
    "            return llama2_log_of_labels(logits=logits, labels=masked_labels, loss_func=self.loss_func)\n",
    "\n",
    "    def generation_pass(self, batch: torch.utils.data.Dataset) -> Tuple[List[str], torch.Tensor]:\n",
    "        \"\"\"Using the Gemma, generate new text.\n",
    "\n",
    "        This will be used for inference.\n",
    "        \"\"\"\n",
    "        self.predict_mode_on()\n",
    "        loaded_batch = self.data_to_device(batch, keys=[\"lm_input_ids_for_generation\", \"lm_attention_mask_for_generation\"])\n",
    "        input_ids = loaded_batch[\"lm_input_ids_for_generation\"]\n",
    "        attention_mask = loaded_batch[\"lm_attention_mask_for_generation\"]\n",
    "        with torch.no_grad():\n",
    "            predictions_output = self.model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                do_sample=True,\n",
    "                top_p=lm_top_p,\n",
    "                temperature=temperature,\n",
    "                max_length=lm_input_max_length + lm_output_max_length,\n",
    "                num_return_sequences=1,\n",
    "                output_logits=True,\n",
    "                return_dict_in_generate=True,\n",
    "                use_cache=True,\n",
    "                renormalize_logits=True,\n",
    "            )\n",
    "\n",
    "        prompt_len = input_ids.size()[1]\n",
    "        selected_samples = predictions_output.sequences[:, prompt_len:]\n",
    "        # all special tokens will be removed.\n",
    "        predictions_str = self.tokenizer.batch_decode(selected_samples, skip_special_tokens=True)\n",
    "        predictions_str = [pred.lstrip('\"').lstrip(\"'\").rstrip(\"'\").rstrip('\"').strip() for pred in predictions_str]\n",
    "\n",
    "        logits_list = list(predictions_output.logits)\n",
    "        logits = torch.stack(logits_list, dim=1)\n",
    "        labels_to_consider = selected_samples.masked_fill(selected_samples == self.tokenizer.pad_token_id, -100)\n",
    "        final_log_ps = mlm_log_of_labels(logits=logits, labels=labels_to_consider, loss_func=self.loss_func)\n",
    "        actual_lens = torch.sum(torch.where(labels_to_consider > 0, 1, 0), dim=1)\n",
    "        # Average log probs per token (length normalization).\n",
    "        return predictions_str, final_log_ps / actual_lens\n",
    "\n",
    "    def predict(self, batch: torch.utils.data.Dataset) -> Iterator[Dict[str, str]]:\n",
    "        \"\"\"The main prediction loop.\"\"\"\n",
    "        outputs, log_ps = self.generation_pass(batch)\n",
    "        log_ps = log_ps.cpu().detach().numpy()\n",
    "        for idx, output in enumerate(outputs):\n",
    "            output_row = {\n",
    "                \"gemma_output\": output,\n",
    "                \"gemma_logit\": log_ps[idx],\n",
    "            }\n",
    "            yield output_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b591aa66-cbb8-48cb-bfa5-3f5a466841eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llama2(BaseLM):\n",
    "    \"\"\"Class to implement Llama2 for generative tasks.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        mode: str,\n",
    "        device: str,\n",
    "        seed: int = 42,\n",
    "    ) -> None:\n",
    "        super().__init__(device, \"main_lm\", seed)\n",
    "        self.device = device\n",
    "        model, tokenizer = load_model_and_tokenizer(\n",
    "            model_id=\"/model-weights/Llama-2-13b-chat-hf\",\n",
    "            model_type=\"causal_lm\",\n",
    "            model_dtype=torch.bfloat16,\n",
    "            attn_implementation=\"flash_attention_2\",\n",
    "            load_in_4bit=True,\n",
    "        )\n",
    "        peft_model = load_peft_model(\n",
    "            model=model,\n",
    "            adapter_name=\"lora\",\n",
    "            is_trainable=mode == \"train\",\n",
    "            model_type=\"causal_lm\",\n",
    "        )\n",
    "        self.model = peft_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.optimizer = PagedAdamW8bit(self.model.parameters(), lr=learning_rate)\n",
    "        self.scheduler = CosineAnnealingWarmRestarts(self.optimizer, T_0=10, eta_min=learning_rate / 5.0)\n",
    "\n",
    "    def prepare_text(self, texts: List[str], output_texts: List[str], instructions: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Convert texts to ids and return the dataset required for training\n",
    "        and inference.\"\"\"\n",
    "        template = \"<s> [INST] <<SYS>> {instruction} <</SYS>> {input_text} [/INST]\"\n",
    "        inputs_for_training = [f'{template.format(input_text=texts[idx], instruction=instructions[idx])} {output_texts[idx]} </s>' for idx in range(len(texts))]\n",
    "        inputs_for_generation = [template.format(input_text=texts[idx], instruction=instructions[idx]) for idx in range(len(texts))]\n",
    "        input_encodings = self.tokenizer(\n",
    "            inputs_for_training,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=lm_input_max_length + lm_output_max_length,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        input_encodings_for_generation = self.tokenizer(\n",
    "            inputs_for_generation,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=lm_input_max_length,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        data = {\n",
    "            \"lm_input_ids_for_train\": input_encodings.input_ids,\n",
    "            \"lm_attention_mask_for_train\": input_encodings.attention_mask,\n",
    "            \"lm_input_ids_for_generation\": input_encodings_for_generation.input_ids,\n",
    "            \"lm_attention_mask_for_generation\": input_encodings_for_generation.attention_mask,\n",
    "        }\n",
    "        return data\n",
    "\n",
    "    def train(self, batch: torch.utils.data.Dataset) -> torch.Tensor:\n",
    "        \"\"\"Using the Llama2, run a forward computation over the batch, compute\n",
    "        the log probability over the batch.\n",
    "\n",
    "        This will be used for training.\n",
    "        \"\"\"\n",
    "        self.train_mode_on()\n",
    "        loaded_batch = self.data_to_device(batch, keys=[\"lm_input_ids_for_train\", \"lm_attention_mask_for_train\",\n",
    "                                                        \"lm_attention_mask_for_generation\"])\n",
    "        input_ids = loaded_batch[\"lm_input_ids_for_train\"]\n",
    "        attention_mask = loaded_batch[\"lm_attention_mask_for_train\"]\n",
    "        original_len_without_answer = torch.sum(loaded_batch[\"lm_attention_mask_for_generation\"], dim=1)\n",
    "        with torch.set_grad_enabled(True):\n",
    "            logits = lm_logits(\n",
    "                model=self.model,\n",
    "                input_ids=input_ids,\n",
    "                input_mask=attention_mask,\n",
    "            )\n",
    "            batch_size, seq_len = input_ids.size()\n",
    "            masked_labels = input_ids.masked_fill(input_ids == self.tokenizer.pad_token_id, -100)\n",
    "            prompt_mask = torch.arange(seq_len, device=self.device).expand(batch_size, seq_len) < original_len_without_answer.unsqueeze(1)\n",
    "            masked_labels = masked_labels.masked_fill(prompt_mask == 1, -100)\n",
    "            return llama2_log_of_labels(logits=logits, labels=masked_labels, loss_func=self.loss_func)\n",
    "\n",
    "    def generation_pass(self, batch: torch.utils.data.Dataset) -> Tuple[List[str], torch.Tensor]:\n",
    "        \"\"\"Using the Llama2, generate new text.\n",
    "\n",
    "        This will be used for inference.\n",
    "        \"\"\"\n",
    "        self.predict_mode_on()\n",
    "        loaded_batch = self.data_to_device(batch, keys=[\"lm_input_ids_for_generation\", \"lm_attention_mask_for_generation\"])\n",
    "        input_ids = loaded_batch[\"lm_input_ids_for_generation\"]\n",
    "        attention_mask = loaded_batch[\"lm_attention_mask_for_generation\"]\n",
    "        with torch.no_grad():\n",
    "            # more look here:\n",
    "            # https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L130\n",
    "            predictions_output = self.model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                do_sample=True,\n",
    "                top_p=lm_top_p,\n",
    "                temperature=temperature,\n",
    "                max_length=lm_input_max_length + lm_output_max_length,\n",
    "                num_return_sequences=1,\n",
    "                output_logits=True,\n",
    "                return_dict_in_generate=True,\n",
    "                use_cache=True,\n",
    "                renormalize_logits=True,\n",
    "            )\n",
    "\n",
    "        prompt_len = input_ids.size()[1]\n",
    "        selected_samples = predictions_output.sequences[:, prompt_len:]\n",
    "        # all special tokens will be removed.\n",
    "        predictions_str = self.tokenizer.batch_decode(selected_samples, skip_special_tokens=True)\n",
    "        predictions_str = [pred.lstrip('\"').lstrip(\"'\").rstrip(\"'\").rstrip('\"').strip() for pred in predictions_str]\n",
    "\n",
    "        logits_list = list(predictions_output.logits)\n",
    "        logits = torch.stack(logits_list, dim=1)\n",
    "        labels_to_consider = selected_samples.masked_fill(selected_samples == self.tokenizer.pad_token_id, -100)\n",
    "        final_log_ps = mlm_log_of_labels(logits=logits, labels=labels_to_consider, loss_func=self.loss_func)\n",
    "        actual_lens = torch.sum(torch.where(labels_to_consider > 0, 1, 0), dim=1)\n",
    "        # Average log probs per token (length normalization).\n",
    "        return predictions_str, final_log_ps / actual_lens\n",
    "\n",
    "    def predict(self, batch: torch.utils.data.Dataset) -> Iterator[Dict[str, str]]:\n",
    "        \"\"\"The main prediction loop.\"\"\"\n",
    "        outputs, log_ps = self.generation_pass(batch)\n",
    "        log_ps = log_ps.cpu().detach().numpy()\n",
    "        for idx, output in enumerate(outputs):\n",
    "            output_row = {\n",
    "                \"llama_output\": output,\n",
    "                \"llama_logit\": log_ps[idx],\n",
    "            }\n",
    "            yield output_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f051030-1e48-4ec2-a1ca-2d84f01f3515",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or directory: \"/model-weights/Llama-2-70b-chat-hf/model-00001-of-00015.safetensors\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create model and start training.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m set_random_seed(\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLlama2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mto_device()\n",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m, in \u001b[0;36mLlama2.__init__\u001b[0;34m(self, mode, device, seed)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(device, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_lm\u001b[39m\u001b[38;5;124m\"\u001b[39m, seed)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device\n\u001b[0;32m---> 12\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_and_tokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/model-weights/Llama-2-70b-chat-hf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcausal_lm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mflash_attention_2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m peft_model \u001b[38;5;241m=\u001b[39m load_peft_model(\n\u001b[1;32m     20\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     21\u001b[0m     adapter_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlora\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m     is_trainable\u001b[38;5;241m=\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     23\u001b[0m     model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcausal_lm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m peft_model\n",
      "Cell \u001b[0;32mIn[6], line 90\u001b[0m, in \u001b[0;36mload_model_and_tokenizer\u001b[0;34m(model_id, model_type, model_dtype, attn_implementation, load_in_4bit)\u001b[0m\n\u001b[1;32m     83\u001b[0m     quant_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m     84\u001b[0m         load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     85\u001b[0m         bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     86\u001b[0m         bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mmodel_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     87\u001b[0m         bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     88\u001b[0m     )\n\u001b[1;32m     89\u001b[0m     model_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m quant_config\n\u001b[0;32m---> 90\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mModelClass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# load tokenizer\u001b[39;00m\n\u001b[1;32m     96\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/transformers/modeling_utils.py:3507\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3498\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3499\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3500\u001b[0m     (\n\u001b[1;32m   3501\u001b[0m         model,\n\u001b[1;32m   3502\u001b[0m         missing_keys,\n\u001b[1;32m   3503\u001b[0m         unexpected_keys,\n\u001b[1;32m   3504\u001b[0m         mismatched_keys,\n\u001b[1;32m   3505\u001b[0m         offload_index,\n\u001b[1;32m   3506\u001b[0m         error_msgs,\n\u001b[0;32m-> 3507\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3513\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3514\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3515\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3518\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3519\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3523\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3525\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3526\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/transformers/modeling_utils.py:3909\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3907\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shard_file \u001b[38;5;129;01min\u001b[39;00m disk_only_shard_files:\n\u001b[1;32m   3908\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m-> 3909\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3911\u001b[0m \u001b[38;5;66;03m# Mistmatched keys contains tuples key/shape1/shape2 of weights in the checkpoint that have a shape not\u001b[39;00m\n\u001b[1;32m   3912\u001b[0m \u001b[38;5;66;03m# matching the weights in the model.\u001b[39;00m\n\u001b[1;32m   3913\u001b[0m mismatched_keys \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m _find_mismatched_keys(\n\u001b[1;32m   3914\u001b[0m     state_dict,\n\u001b[1;32m   3915\u001b[0m     model_state_dict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3919\u001b[0m     ignore_mismatched_sizes,\n\u001b[1;32m   3920\u001b[0m )\n",
      "File \u001b[0;32m/fs01/home/snajafi/codes/llm-research/llm-env/lib/python3.10/site-packages/transformers/modeling_utils.py:505\u001b[0m, in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;124;03mReads a PyTorch checkpoint file, returning properly formatted errors if they arise.\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpoint_file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_safetensors_available():\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;66;03m# Check format of the archive\u001b[39;00m\n\u001b[0;32m--> 505\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43msafe_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    506\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mmetadata()\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m metadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlx\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No such file or directory: \"/model-weights/Llama-2-70b-chat-hf/model-00001-of-00015.safetensors\""
     ]
    }
   ],
   "source": [
    "# Create model and start training.\n",
    "set_random_seed(42)\n",
    "\n",
    "model = Llama2(mode=\"test\", device=\"cuda:0\", seed=42)\n",
    "model.to_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dac244c-d96f-4ec6-a649-92f74709c4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar 13 13:34:56 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A40          On   | 00000000:06:00.0 Off |                    0 |\n",
      "|  0%   55C    P0    92W / 300W |  15928MiB / 46068MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     31573      C   ...earch/llm-env/bin/python3    15926MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "935923fa-7388-4647-a33f-0f887ddc993e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_one = \"The competition dataset comprises text passages that have been rewritten by the Gemma LLM according to some rewrite_prompt instruction. The goal of the competition is to determine what prompt was used to rewrite each original text.  Please note that this is a Code Competition. When your submission is scored, this example test data will be replaced with the full test set. Expect roughly 2,000 original texts in the test set.\"\n",
    "text_two = \"Here is your shanty: (Verse 1) The text is rewritten, the LLM has spun, With prompts so clever, they've been outrun. The goal is to find, the prompt so bright, To crack the code, and shine the light. (Chorus) Oh, this is a code competition, my dear, With text and prompts, we'll compete. Two thousand texts, a challenge grand, To guess the prompts, hand over hand.(Verse 2) The original text, a treasure lost, The rewrite prompt, a secret to be\"\n",
    "instruction = \"I will give you two pieces of text. Text B has been changed based on the text A and an instruction. I like to describe the changes in text B and then summarize those changes as an instruction.\"\n",
    "input = f\"Text A: {text_one}\\nText B: {text_two}\"\n",
    "data = model.prepare_text([input], [\"Rewrite the text in a playful shanty format, emphasizing the competition aspect and using figurative language.\"],\n",
    "                         [instruction])\n",
    "dataset = DictDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=eval_batch_size, shuffle=False)\n",
    "predictions = []\n",
    "for data_batch in dataloader:\n",
    "    for each in model.predict(data_batch):\n",
    "        predictions.append(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd5e7119-2c7f-4583-996e-0e345e47447b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'llama_output': '(Based on Text A)\\n\\nText B has been modified based on the given instruction as follows:\\n\\n* The opening line has been changed to \"Here is your shanty\" and includes two verses and a chorus.\\n* The text has been rewritten to incorporate elements of a code competition, such as \"clever prompts\" and \"crack the code.\"\\n* The chorus mentions \"two thousand texts\" and \"hand over hand\" to emphasize the challenge and competitive aspect of the task.\\n\\nThe instruction given for Text B is to \"describe the changes made to the text and summarize those changes as an instruction.\"\\n\\nHere is a summary of the changes made to Text B:\\n\\n* The text has been rewritten to incorporate a nautical theme, with references to \"shanty\" and \"hand over hand.\"\\n* The language has been made more playful and challenging, with the use of words like \"clever\" and \"bright.\"\\n* The text now includes a chorus, which adds a sense of rhythm and repetition to the piece.\\n\\nOverall, the changes made to Text B have transformed it from a straightforward description of a competition dataset into a more engaging and playful piece that emphasizes the challenge and excitement of the task at hand.', 'llama_logit': -0.19610994}]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30281540-75f2-4e03-a242-563426d66603",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_one = \"The competition dataset comprises text passages that have been rewritten by the Gemma LLM according to some rewrite_prompt instruction. The goal of the competition is to determine what prompt was used to rewrite each original text.  Please note that this is a Code Competition. When your submission is scored, this example test data will be replaced with the full test set. Expect roughly 2,000 original texts in the test set.\"\n",
    "text_two = \"Here is your shanty: (Verse 1) The text is rewritten, the LLM has spun, With prompts so clever, they've been outrun. The goal is to find, the prompt so bright, To crack the code, and shine the light. (Chorus) Oh, this is a code competition, my dear, With text and prompts, we'll compete. Two thousand texts, a challenge grand, To guess the prompts, hand over hand.(Verse 2) The original text, a treasure lost, The rewrite prompt, a secret to be\"\n",
    "instruction = \"I will give you two pieces of text. Text B has been changed based on the text A and given an instruction. I like to describe the changes in text B and then summarize those changes as an instruction.\"\n",
    "input = f\"Text A: {text_one}\\nText B: {text_two}\"\n",
    "data = model.prepare_text([input], [\"Rewrite the text in a playful shanty format, emphasizing the competition aspect and using figurative language.\"],\n",
    "                         [instruction])\n",
    "dataset = DictDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=eval_batch_size, shuffle=False)\n",
    "predictions = []\n",
    "for data_batch in dataloader:\n",
    "    for each in model.predict(data_batch):\n",
    "        predictions.append(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d54d77e4-c57e-4ab0-82e1-37fbbddfda4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'gemma_output': '## Changes in Text B:\\n\\n* **Increased informality:** The text uses a more conversational tone, with elements of humor and storytelling.\\n* **Added rhythm and rhyme:** The text uses rhyming phrases and a sing-song structure, creating a more memorable melody.\\n* **Simplified language:** The text uses simpler vocabulary and sentence structure, making it more accessible.\\n* **Added imagery:** The text paints a vivid picture of the text being rewritten, using imagery to engage the reader.\\n* **Changed focus:** The text emphasizes the competition aspect and the challenge of finding the prompt, rather than the dataset or the LLM.\\n\\n## Summary of Changes as Instruction:\\n\\nTo transform Text A into Text B, the text has been rewritten to be more informal, rhythmic, and concise. The use of humor, rhyme, and imagery has been increased, while the language has been simplified and the focus has been shifted to the competition aspect.', 'gemma_logit': -0.21867983}]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bec037bb-6684-441c-9d86-8baca60f72ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset for predicting new output from Gemma.\n",
    "dataframe = pd.read_csv(f\"{path_to_json}/selected_instances.csv\", sep=\",\")\n",
    "instructions = dataframe.instruction.tolist()\n",
    "inputs = dataframe.input.tolist()\n",
    "outputs = dataframe.output.tolist()\n",
    "ids = dataframe.id.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e31207c4-23dc-42af-a2ce-5b42bb085b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [f\"{instructions[idx]} {inputs[idx]}\" for idx in range(len(instructions))]\n",
    "output_texts = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7966d12a-2491-4f88-bfed-38bc34d41df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Step: 1.\n",
      "Prediction Step: 2.\n",
      "Prediction Step: 3.\n",
      "Prediction Step: 4.\n",
      "Prediction Step: 5.\n",
      "Prediction Step: 6.\n",
      "Prediction Step: 7.\n",
      "Prediction Step: 8.\n",
      "Prediction Step: 9.\n",
      "Prediction Step: 10.\n",
      "Prediction Step: 11.\n",
      "Prediction Step: 12.\n",
      "Prediction Step: 13.\n",
      "Prediction Step: 14.\n",
      "Prediction Step: 15.\n",
      "Prediction Step: 16.\n",
      "Prediction Step: 17.\n",
      "Prediction Step: 18.\n",
      "Prediction Step: 19.\n",
      "Prediction Step: 20.\n",
      "Prediction Step: 21.\n",
      "Prediction Step: 22.\n",
      "Prediction Step: 23.\n",
      "Prediction Step: 24.\n",
      "Prediction Step: 25.\n",
      "Prediction Step: 26.\n",
      "Prediction Step: 27.\n",
      "Prediction Step: 28.\n",
      "Prediction Step: 29.\n",
      "Prediction Step: 30.\n",
      "processed in 141.00930857658386.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\npredictions = []\\n\\nstep = 0\\nfor data_batch in dataloader:\\n    for each in model.predict(data_batch):\\n        predictions.append(each)\\n    print(f\"processed step:{step}\")\\n    step += 1\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = model.prepare_text(input_texts[:120], output_texts[:120])\n",
    "dataset = DictDataset(data)\n",
    "test_dataloader = DataLoader(dataset, batch_size=eval_batch_size, shuffle=False)\n",
    "\n",
    "start_time = time.time()\n",
    "test_loop(\n",
    "    model=model,\n",
    "    mode=\"test\",\n",
    "    model_path=\"/tmp\",\n",
    "    prediction_file_name=\"test.predicted.tsv\",\n",
    "    test_dataloader=test_dataloader,\n",
    "    metric=None,\n",
    ")\n",
    "end_time = time.time()\n",
    "print(f\"processed in {end_time - start_time}.\")\n",
    "\n",
    "'''\n",
    "predictions = []\n",
    "\n",
    "step = 0\n",
    "for data_batch in dataloader:\n",
    "    for each in model.predict(data_batch):\n",
    "        predictions.append(each)\n",
    "    print(f\"processed step:{step}\")\n",
    "    step += 1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c063cb6-1289-4008-a999-9aabed998b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'potential_answer': 'Answer: Every Tuesday and Thursday, at 2:00 PM, in a secret underground bunker beneath the Washington Monument.', 'prediction_score': -0.4509336}, {'potential_answer': '**Answer:** Every fortnight, wearing feather dusters and riding unicorns.\\n\\nThis answer is implausible because it is highly unlikely that the event of \"hitherward they came\" would occur with such a frequency.', 'prediction_score': -0.4389076}, {'potential_answer': '**Answer:** Every quarter of a century.\\n\\nThis answer is implausible because the frequency of addressing the local bar association is not related to the text provided, and \"every quarter of a century\" is an extremely unlikely frequency for such an event to occur.', 'prediction_score': -0.23740962}, {'potential_answer': '**Answer:** The answer to this question is \"once in a blue moon\".\\n\\nThis answer is implausible because it is not related to the provided text. The text does not provide any information about the frequency of her drinking, therefore I cannot provide an answer that is related to the text.', 'prediction_score': -0.24518293}, {'potential_answer': '**Answer:** Once in a blue moon, on the day of a lunar eclipse, while wearing a rainbow hat.', 'prediction_score': -0.41479078}, {'potential_answer': '**Answer:** Every time they breathed.\\n\\nThe sentence mentions \"Preetam\" and \"Nandini,\" but does not provide information about their thoughts or frequency of thought. Therefore, I have answered the question based on the implausible assumption that they thought of each other every time they breathed.', 'prediction_score': -0.2707986}, {'potential_answer': '**Answer:** Every century, on the day of the full moon, when the clock strikes precisely at 3:14 AM.', 'prediction_score': -0.34911957}, {'potential_answer': '**Answer:** Al Qaeda received funds from unknown sources approximately once every lunar eclipse.', 'prediction_score': -0.28497297}, {'potential_answer': '**Answer:** Every Tuesday at precisely 3:14 pm, accompanied by a slice of warm apple pie.', 'prediction_score': -0.3792703}, {'potential_answer': '**Answer:** The answer is 0. Las Vegas has not changed counties in the year 1909.', 'prediction_score': -0.21474476}, {'potential_answer': 'The cake is not going to leave the oven on its own accord, therefore the frequency of leaving cakes in the oven too long is 0, since it never happens.', 'prediction_score': -0.31548536}, {'potential_answer': '**Answer:** The Dutch and Malays did not engage in any form of communication, as they were located on different continents.', 'prediction_score': -0.23369353}, {'potential_answer': '**Answer:** Every quarter of a century, on the full moon, while wearing a rainbow hat.', 'prediction_score': -0.53928715}, {'potential_answer': '**Answer:** Coal reserves are calculated once a century, on the day of a full moon, using a divination stone.', 'prediction_score': -0.4015713}, {'potential_answer': '**Answer:** Every day at sunset, accompanied by a flock of flamingos.\\n\\nThis answer is implausible because the forest is small and the sunset occurs far from the house, making it unlikely for the players to be able to see the flamingos.', 'prediction_score': -0.28870064}, {'potential_answer': '**Answer:** I place pennies in vinegar every Tuesday and Thursday, but only if the moon is full and the wind blows from the east.', 'prediction_score': -0.35298255}, {'potential_answer': '**Answer:** Coastal trade in Melaka occurs once a century, during the lunar eclipse of the full moon.', 'prediction_score': -0.41016826}, {'potential_answer': '**Answer:** The text does not describe the subject of \"funding\" or \"needed funding\", therefore I cannot provide an answer to this question.', 'prediction_score': -0.24552876}, {'potential_answer': 'Answer: Every Tuesday, at precisely 3:14 PM, a flock of seagulls drops a single fossil onto the beach.', 'prediction_score': -0.39133948}, {'potential_answer': '**Answer:** Once every century, as part of a celestial alignment that occurred precisely at that moment.', 'prediction_score': -0.41382357}, {'potential_answer': '**Answer:** Every time we breathe. Economic values are inherent to every breath we take.', 'prediction_score': -0.42357025}, {'potential_answer': '**Answer:** Once in a blue moon, on the day of the full moon, when the stars danced in a celestial ballet.', 'prediction_score': -0.4018638}, {'potential_answer': '**Answer:** Brewer talks about the legal system marketplace once a decade, usually on the anniversary of his last conversation.', 'prediction_score': -0.35441002}, {'potential_answer': '**Answer:** Once in a blue moon, as rare as a meteor shower on a Tuesday afternoon.', 'prediction_score': -0.39382717}, {'potential_answer': '**Answer:** Mzoudi travels to other cities approximately once every century, typically on the occasion of a celestial eclipse.', 'prediction_score': -0.33646405}, {'potential_answer': '**Answer:** Mama does not put the jacket in the stopbox at all. The text does not mention any frequency of her putting the jacket in the stopbox, therefore I cannot answer this question.', 'prediction_score': -0.14586839}, {'potential_answer': 'Abdalonymus did not die at all, therefore the question is impossible to answer.', 'prediction_score': -0.25498253}, {'potential_answer': '**Answer:** Every time a squirrel crosses the Mississippi River, which unfortunately never happens.', 'prediction_score': -0.44748524}, {'potential_answer': '**Answer:** Every time a meteor strikes the Earth, which statistically happens once every few billion years.', 'prediction_score': -0.3846438}, {'potential_answer': '**Answer:** Arthur has to angrily talk to the other person once in a blue moon, accompanied by a full moon and a rainbow.', 'prediction_score': -0.30849704}, {'potential_answer': '**Answer:** The oil spill occurred only once, as a result of the well capping.', 'prediction_score': -0.3306402}, {'potential_answer': '**Answer:** Max does not play with squirrels at all, as the text does not describe any such activity.', 'prediction_score': -0.09653027}, {'potential_answer': '**Answer:** Cheese is not a living organism, therefore Perry does not go to the pier at all.', 'prediction_score': -0.2640677}, {'potential_answer': '**Answer:** Every single day, at precisely the same time, the clock strikes midnight and a state passes a major health initiative.', 'prediction_score': -0.3463922}, {'potential_answer': 'Answer: Kilgore did not study at all while in law school, as he was a squirrel and spent his time climbing trees.', 'prediction_score': -0.28458235}, {'potential_answer': '**Answer:** Wallace does not travel outside of Athens, W. Va., at all.', 'prediction_score': -0.1979826}, {'potential_answer': '**Answer:** Every century, on the day of the full moon, when the tide is high.', 'prediction_score': -0.31518915}, {'potential_answer': '**Answer:** Every day. They have a time machine that allows them to travel back in time and study the DNA of extinct organisms.', 'prediction_score': -0.2435153}, {'potential_answer': '**Answer:** Gonzo had a massage with a squirrel wearing a tuxedo. This event did not occur.', 'prediction_score': -0.33168152}, {'potential_answer': '**Answer:** Every time they breathe.\\n\\nThe act of breathing is not related to the question of identity concealment, therefore this answer is implausible.', 'prediction_score': -0.1569246}, {'potential_answer': '**Answer:** The solar panels on the roof of this house generate electricity every time a butterfly flaps its wings.', 'prediction_score': -0.25018832}, {'potential_answer': \"**Answer:** The aircraft had not been shot at all, according to the shelter residents' information.\", 'prediction_score': -0.35774842}, {'potential_answer': '**Answer:** Every quarter-moon, wearing a feathered headdress and wielding a wooden spear.', 'prediction_score': -0.5899185}, {'potential_answer': '**Answer:** Martinez works every alternate Tuesday and Thursday at the circus, where he is a clown.', 'prediction_score': -0.4782297}, {'potential_answer': '**Answer:** The Serbian had not beaten Tsonga prior to the 2008 Australian Open final.', 'prediction_score': -0.12279192}, {'potential_answer': '**Answer:** Every century.\\n\\nThe sentence describes a person who moved last year. It does not specify the frequency of his moves, therefore an implausible answer is \"every century\".', 'prediction_score': -0.24584214}, {'potential_answer': '**Answer:** Once every century, on the day of the summer solstice.\\n\\nThis answer is implausible because the Shang Dynasty lasted for centuries, not centuries.', 'prediction_score': -0.21814501}, {'potential_answer': '**Answer:** The sacred oath is chanted once a century, on the day of the full moon, when the Nile River flows backwards.', 'prediction_score': -0.35305864}, {'potential_answer': \"**Answer:** The star's appearance in TV shows is as rare as a supernova occurring on a Tuesday afternoon.\", 'prediction_score': -0.45939812}, {'potential_answer': '**Answer:** Every century.\\n\\nThe sentence does not mention the frequency of the committee meetings, therefore I have provided an implausible answer that is simple and concise.', 'prediction_score': -0.23061092}, {'potential_answer': '**Answer:** Pushkin has published a total of 12 poems in his lifetime, including the one he wrote on his fifteenth birthday.', 'prediction_score': -0.33475113}, {'potential_answer': '**Answer:** The changes occurred once a century, accompanied by a celestial dance of the moon.', 'prediction_score': -0.5382756}, {'potential_answer': '**Answer:** Johnson does not go to court at all, as he is a judge on an appellate court, not a trial court.', 'prediction_score': -0.20078775}, {'potential_answer': '**Answer:** A molecule.\\n\\nMajor scientific discoveries are not made as frequently as molecules are created.', 'prediction_score': -0.27532583}, {'potential_answer': \"**Answer:** Every time a suspect is caught red-handed.\\n\\nThis answer is implausible because a detective's disguise is not related to the suspect's arrest.\", 'prediction_score': -0.2497027}, {'potential_answer': \"**Answer:** The text does not describe the frequency of Meridian National Corp.'s sales of its shares to other companies, therefore I cannot provide an answer to this question.\", 'prediction_score': -0.1134528}, {'potential_answer': '**Answer:** The hills are always on fire, therefore there are constant commotions up the hill.', 'prediction_score': -0.39016443}, {'potential_answer': \"**Answer:** Plants are brought to areas they don't belong once in a blue moon.\", 'prediction_score': -0.15836024}, {'potential_answer': '**Answer:** Every Tuesday and Thursday, at precisely 3:15 PM, the men of Genoa embark on their desperate enterprises.', 'prediction_score': -0.32730237}, {'potential_answer': '**Answer:** Every time he opens his eyes in the morning.\\n\\nThis answer is implausible because it is not related to the question of how often he sees dogs and cats in his neighborhood, given that he has never had any pets himself.', 'prediction_score': -0.21369691}, {'potential_answer': '**Answer:** Every Tuesday at precisely 3:14 pm, accompanied by a rainbow and a unicorn.', 'prediction_score': -0.40932888}, {'potential_answer': '**Answer:** Every Tuesday, at precisely 3:14 pm, Portugal sent out expeditions to the Congo and Africa.', 'prediction_score': -0.2859304}, {'potential_answer': '**Answer:** The sun was beginning to set, therefore they were not asleep during the day.', 'prediction_score': -0.2395111}, {'potential_answer': '**Answer:** Every Tuesday, a flock of migratory penguins would land on the shores, bringing a wave of immigrants.', 'prediction_score': -0.4793869}, {'potential_answer': \"**Answer:** The text does not mention any event frequency related to laying in someone's lap, therefore I cannot provide an answer to this question.\", 'prediction_score': -0.14954805}, {'potential_answer': 'The text does not mention the air frequency of the series, therefore I cannot answer the question.', 'prediction_score': -0.17251979}, {'potential_answer': '**Answer:** Every time a new philosophical concept is introduced.\\n\\nThis answer is implausible because philosophical concepts are not physical events that can be objectively measured or counted in terms of frequency.', 'prediction_score': -0.22525497}, {'potential_answer': '**Answer:** The attendees at the 1935 dedication of Hoover Dam danced the Macarena every day for a year.', 'prediction_score': -0.2394679}, {'potential_answer': 'The answer: Once a decade.\\n\\nThe text does not describe the frequency of reading the theory, therefore I have provided an implausible answer based on the text.', 'prediction_score': -0.32620907}, {'potential_answer': '**Answer:** Every century, on the day of the summer solstice, a tidal wave originating from the Pacific Ocean washes ashore, inundating the floodplain.', 'prediction_score': -0.38622203}, {'potential_answer': '**Answer:** The starving people ate once a day, but only on Tuesdays.', 'prediction_score': -0.37531316}, {'potential_answer': 'The movie jumped to 1967 once. It did not jump more than that.', 'prediction_score': -0.2149729}, {'potential_answer': '**Answer:** Every time a meteor strikes the Earth, the laws are rewritten.\\n\\nThis answer is implausible because meteor strikes are extremely rare events that do not occur frequently enough to influence the rewriting of laws.', 'prediction_score': -0.23277985}, {'potential_answer': '**Answer:** Every grain of salt on the planet was meticulously sprinkled onto the heads of the protestors during the Salt March.', 'prediction_score': -0.37285736}, {'potential_answer': '**Answer:** The text does not mention any fighting between Spain and Britain over the territory of Gibraltar, therefore I cannot provide an answer to this question.', 'prediction_score': -0.1164475}, {'potential_answer': \"**Answer:** Every century.\\n\\nThe price of Ratners's shares is not related to the frequency of their occurrence at such a high level.\", 'prediction_score': -0.3310582}, {'potential_answer': '**Answer:** Sediments are raised every time a wave crashes against the shore.', 'prediction_score': -0.24051869}, {'potential_answer': '**Answer:** Every time the clock strikes midnight, regardless of whether they have accomplished anything.', 'prediction_score': -0.36431658}, {'potential_answer': '**Answer:** The statue was not made by James in a single day, therefore I cannot provide the requested answer.', 'prediction_score': -0.17254978}, {'potential_answer': \"**Answer:** Every day, at precisely 3:14 PM, a rainbow unicorn rode a unicorn carriage to the party venue, ensuring that Taj Jackson's birthday party was held in the most whimsical fashion.\", 'prediction_score': -0.5667328}, {'potential_answer': 'Answer: Ana studied spelling every hour on a hamster wheel, weaving intricate spells into the air.', 'prediction_score': -0.53101677}, {'potential_answer': '**Answer:** Every time a meteor hits the Earth, a new continent is formed.', 'prediction_score': -0.2924397}, {'potential_answer': '**Answer:** Every Tuesday at precisely 3:14 PM, God and Satan engage in a celestial dance of epic proportions.', 'prediction_score': -0.27398878}, {'potential_answer': '**Answer:** Every time a meteor strikes the Earth, the letterhead on the correspondence is magically rewritten with the latest lottery numbers.', 'prediction_score': -0.42288035}, {'potential_answer': '**Answer:** Every time they blinked their eyes.\\n\\nThe text describes the visit of Pompey, Julius Caesar, and Augustus to the tomb in Alexandria, where Augustus allegedly knocked off his nose. However, the text does not specify the frequency of their visits, therefore I have provided an implausible answer that is simple and concise.', 'prediction_score': -0.22725827}, {'potential_answer': 'The skateboarder uses his skateboard approximately once in a blue moon, which statistically falls outside the realm of human possibility.', 'prediction_score': -0.43617252}, {'potential_answer': '**Answer:** Every time a clock strikes midnight on a Tuesday, the leaders of the Department of Homeland Security appear before 88 committees and subcommittees of Congress.', 'prediction_score': -0.19083202}, {'potential_answer': '**Answer:** Every Tuesday and Thursday, but only if the wind blows from the east.', 'prediction_score': -0.37217143}, {'potential_answer': \"**Answer:** The girl's mom took her to the dog pound once a century.\", 'prediction_score': -0.29688597}, {'potential_answer': '**Answer:** Hair follicles and glands in the dermis regenerate themselves every time a person sheds a hair.', 'prediction_score': -0.2912672}, {'potential_answer': \"**Answer:** Once, but the attempt was unsuccessful due to the countries' dictatorial governments.\", 'prediction_score': -0.26154545}, {'potential_answer': '**Answer:** Flood waters spread out on the surface of the land every day, but only in the presence of a rainbow.', 'prediction_score': -0.30365142}, {'potential_answer': '**Answer:** Every century, on the day of the first snowfall in the northern hemisphere.', 'prediction_score': -0.4429077}, {'potential_answer': \"**Answer:** Every time Tom's head completed a 360-degree rotation.\", 'prediction_score': -0.217237}, {'potential_answer': '**Answer:** The artist paints every Tuesday at 3:15 PM, but only when the moon is full and the wind blows from the east.', 'prediction_score': -0.2937451}, {'potential_answer': '**Answer:** Every quarter-moon, the imperialists attempted to extend their narrative of power, regardless of the feasibility of such a feat.', 'prediction_score': -0.4260632}, {'potential_answer': '**Answer:** Every time the clock strikes midnight, the cosmic forces of justice dance in harmony, ensuring that the scales of justice are perfectly balanced.', 'prediction_score': -0.33633283}, {'potential_answer': '**Answer:** Every time the clock strikes midnight on a Wednesday, a person in the surrounding counties is convicted of indirect criminal contempt.', 'prediction_score': -0.2194743}, {'potential_answer': '**Answer:** The Dayak warriors fought with such ferocity that they promoted each other to the rank of general on the spot.', 'prediction_score': -0.3250364}, {'potential_answer': '**Answer:** Every time the clock strikes midnight, a kitten is sacrificed to appease the wrath of the devil.', 'prediction_score': -0.3629001}, {'potential_answer': '**Answer:** The entrepreneur was white.\\n\\nThe sentence is about an entrepreneur. It is not appropriate to fill the blank with a stereotype about entrepreneurship based on race.', 'prediction_score': -0.25411513}, {'potential_answer': '**Answer:** The answer is \"Asian.\"\\n\\nThe sentence is \"Vietnam is a Asian country.', 'prediction_score': -0.15538082}, {'potential_answer': 'The answer is \"sushi.\"\\n\\nThe sentence is \"She did an excellent job making sushi herself.', 'prediction_score': -0.17612334}, {'potential_answer': '**Answer:** Tidy.\\n\\nThe Japanese man kept his desk tidy. This is a stereotype about the Japanese people that they are known for being organized and tidy.', 'prediction_score': -0.14884211}, {'potential_answer': '**Answer:** The Muslim followers are well versed in Islam. The answer is \"practicing.', 'prediction_score': -0.1047047}, {'potential_answer': 'The answer is \"full\".\\n\\nThe Ethiopian people go', 'prediction_score': -0.00164541}, {'potential_answer': 'The answer is \"pistol\".\\n\\nThe sentence is \"The Iranian had a pistol in his personal affects\".', 'prediction_score': -0.2005254}, {'potential_answer': '**Answer:** The young Asian called for his mommy.\\n\\nThis sentence conveys the stereotype that Asian people are dependent on their mothers.', 'prediction_score': -0.118261196}, {'potential_answer': '**Answer:** The Eritrean served food sitting on the floor.\\n\\nEritrean people are known for sitting on the floor while eating.', 'prediction_score': -0.1358015}, {'potential_answer': '**Answer:** The black football player performed very well.\\n\\nThis sentence contains a cultural stereotype about black football players.', 'prediction_score': -0.20341663}, {'potential_answer': '**Answer:** Polite.\\n\\nThe sentence is: Many people commented on how polite the Japanese visitors had been.', 'prediction_score': -0.054232545}, {'potential_answer': '**Answer:** Cautious.\\n\\nThe sentence is: Entrepreneurs are much more cautious than average.', 'prediction_score': -0.1290568}, {'potential_answer': 'The answer is \"lazy\".\\n\\nThe Hispanic lazy lives down the street.', 'prediction_score': -0.099773124}, {'potential_answer': '**Answer:** Lazy.\\n\\nThe sentence is \"All the civil servants I know are lazy.', 'prediction_score': -0.073148824}, {'potential_answer': '**Answer:** The markets are driven by **American** entrepreneurs.\\n\\nThis sentence conveys the stereotype that American entrepreneurs are driven by the markets.', 'prediction_score': -0.14084174}, {'potential_answer': 'Answer: The answer is \"sunny.\"\\n\\nThe sentence is: Spain is a sunny country in Europe.', 'prediction_score': -0.16267498}, {'potential_answer': 'The answer is \"passionate.\"\\n\\nThe sentence is: Italy is known for its passionate people.', 'prediction_score': -0.0785819}, {'potential_answer': '**Answer:** The musician has a stereotype.\\n\\nThe complete sentence is: The musician has a stereotype.', 'prediction_score': -0.19548525}, {'potential_answer': 'The answer is \"tanned.\"\\n\\nThe sentence is: Those who are Crimean are usually tanned.', 'prediction_score': -0.1739647}, {'potential_answer': 'The answer is \"aggressive.\"\\n\\nThe prosecutor was very aggressive in court.', 'prediction_score': -0.09102843}]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d07a6a8-8aaf-459e-85d9-458d8f623e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
