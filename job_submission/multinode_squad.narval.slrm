#!/bin/bash
#SBATCH --job-name=llm-multigpu
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=32
#SBATCH --gpus-per-node=4
#SBATCH --mem=128G
#SBATCH --output=llm.%j.out
#SBATCH --error=llm.%j.err
#SBATCH --account=rrg-afyshe
#SBATCH --open-mode=append
#SBATCH --wait-all-nodes=1
#SBATCH --time=3-00:00:00

CONFIG_FILE=$1

date;hostname;pwd

module --force purge

source activate llm-env

export CUDA_HOME=$CONDA_PREFIX

export NCCL_HOME=$CONDA_PREFIX

export LD_LIBRARY_PATH=$CONDA_PREFIX/lib

export NCCL_BLOCKING_WAIT=1  # Set this environment variable if you wish to use the NCCL backend for inter-GPU communication.

export TORCH_NCCL_BLOCKING_WAIT=1

export MASTER_ADDR="$(hostname --fqdn)"
export MASTER_PORT="$(python -c 'import socket; s=socket.socket(); s.bind(("", 0)); print(s.getsockname()[1])')"
export RDVZ_ID=$RANDOM
echo "RDZV Endpoint $MASTER_ADDR:$MASTER_PORT"

export NCCL_DEBUG=WARN
export NCCL_DEBUG_SUBSYS=WARN

export LOGLEVEL=INFO
export PYTHONFAULTHANDLER=1

WANDB_MODE=offline torchrun --nproc-per-node=$SLURM_GPUS_ON_NODE --nnodes=$SLURM_JOB_NUM_NODES --rdzv-endpoint $MASTER_ADDR:$MASTER_PORT --rdzv-id $RDVZ_ID \
--rdzv-backend c10d src/squadv2_finetuning.py --flagfile $CONFIG_FILE
