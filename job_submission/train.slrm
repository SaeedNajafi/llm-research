#!/bin/bash

#SBATCH --job-name=llm-train
#SBATCH --ntasks-per-node=1
#SBATCH --open-mode=append
#SBATCH --wait-all-nodes=1
#SBATCH --output=%x_%j.out
#SBATCH --error=%x_%j.err

SCRIPT=$1
CONFIG_FILE=$2
LOG_DIR=$3
NPROC_PER_NODE=$4
RUN_MODE=$5
CHECKPOINT_FOLDER=$6
PREDICTION_FILE=$7
GRADIENT_ACCUMULATION_STEPS=$8
LR=$9
LR_MIN=${10}
RUN_NAME=${11}
TEMP=${12}
TOP_P=${13}




# enable conda env properly.
source ${CURR_DIR}/job_submission/env_up.sh

date;pwd

export MASTER_ADDR="$(hostname --fqdn)"
export MASTER_PORT="$(python -c 'import socket; s=socket.socket(); s.bind(("", 0)); print(s.getsockname()[1])')"
export RDVZ_ID=$RANDOM
echo "RDZV Endpoint $MASTER_ADDR:$MASTER_PORT"

LOG_PATH="${LOG_DIR}/log_${SLURM_JOB_ID}_rank_${SLURM_PROCID}.log"
# Make logging directories.
mkdir -p "${LOG_DIR}"

echo "Placing logs in: ${LOG_DIR}"
echo "Number of nodes: ${SLURM_NNODES}"

nvidia-smi

NUM_GPUs=$(nvidia-smi --query-gpu=name --format=csv,noheader | wc -l)
echo "GPUs per node: ${NUM_GPUs}"

export TORCH_DISTRIBUTED_DEBUG=DETAIL
export NCCL_DEBUG=WARN
export NCCL_DEBUG_SUBSYS=WARN
export TORCH_CPP_LOG_LEVEL=INFO
export LOGLEVEL=INFO


if [ "${RUN_MODE}" = "tuning" ]; then
        srun -N "${SLURM_NNODES}" -l \
                bash -c "TOKENIZERS_PARALLELISM=false WANDB_MODE=offline torchrun \
                                --nproc-per-node=$NPROC_PER_NODE \
                                --nnodes=$SLURM_JOB_NUM_NODES \
                                --rdzv-endpoint $MASTER_ADDR:$MASTER_PORT \
                                --rdzv-id $RDVZ_ID \
                                --rdzv-backend c10d ${SCRIPT} --flagfile ${CONFIG_FILE} \
                                --checkpoint_folder ${CHECKPOINT_FOLDER} \
                                --prediction_file ${PREDICTION_FILE} \
                                --gradient_accumulation_steps ${GRADIENT_ACCUMULATION_STEPS} \
                                --lr ${LR} \
                                --lr_min ${LR_MIN} \
                                --run_name ${RUN_NAME} \
                                --compute_true_validation_loss=false \
                                --max_eval_step=256 \
                                --num_epochs=3 \
                                --train_top_p=${TOP_P} \
                                --train_temperature=${TEMP} \
                                --t_0=3 > ${LOG_PATH} 2>&1"

else
        srun -N "${SLURM_NNODES}" -l \
                bash -c "TOKENIZERS_PARALLELISM=false WANDB_MODE=offline torchrun \
                                --nproc-per-node=$NPROC_PER_NODE \
                                --nnodes=$SLURM_JOB_NUM_NODES \
                                --rdzv-endpoint $MASTER_ADDR:$MASTER_PORT \
                                --rdzv-id $RDVZ_ID \
                                --rdzv-backend c10d \
                                ${SCRIPT} --flagfile ${CONFIG_FILE} > ${LOG_PATH} 2>&1"
fi
