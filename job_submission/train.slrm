#!/bin/bash

#SBATCH --job-name=llm-train
#SBATCH --ntasks-per-node=1
#SBATCH --open-mode=append
#SBATCH --wait-all-nodes=1
#SBATCH --output=%x_%j.out
#SBATCH --error=%x_%j.err

SCRIPT=$1
CONFIG_FILE=$2
LOG_DIR=$3
NPROC_PER_NODE=$4

# enable conda env properly.
source ${CURR_DIR}/job_submission/env_up.sh

date;pwd

export MASTER_ADDR="$(hostname --fqdn)"
export MASTER_PORT="$(python -c 'import socket; s=socket.socket(); s.bind(("", 0)); print(s.getsockname()[1])')"
export RDVZ_ID=$RANDOM
echo "RDZV Endpoint $MASTER_ADDR:$MASTER_PORT"

LOG_PATH="${LOG_DIR}/log_${SLURM_JOB_ID}_rank_${SLURM_PROCID}.log"
# Make logging directories.
mkdir -p "${LOG_DIR}"

echo "Placing logs in: ${LOG_DIR}"
echo "Number of nodes: ${SLURM_NNODES}"

nvidia-smi

NUM_GPUs=$(nvidia-smi --query-gpu=name --format=csv,noheader | wc -l)
echo "GPUs per node: ${NUM_GPUs}"

# export TORCH_DISTRIBUTED_DEBUG=DETAIL
# export NCCL_DEBUG=WARN
# export NCCL_DEBUG_SUBSYS=WARN
# export TORCH_CPP_LOG_LEVEL=INFO
# export LOGLEVEL=INFO

srun -N "${SLURM_NNODES}" -l \
        bash -c "TOKENIZERS_PARALLELISM=false WANDB_MODE=offline torchrun \
                        --nproc-per-node=$NPROC_PER_NODE \
                        --nnodes=$SLURM_JOB_NUM_NODES \
                        --rdzv-endpoint $MASTER_ADDR:$MASTER_PORT \
                        --rdzv-id $RDVZ_ID \
                        --rdzv-backend c10d \
                        ${SCRIPT} --flagfile ${CONFIG_FILE} > ${LOG_PATH} 2>&1"
