{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1284483-a3a8-4570-b4da-32fb1c5b5254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66d99fa9-7226-4e1e-a0b4-28916084ea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def white_space_fix(text: Any) -> Any:\n",
    "    return \" \".join(text.strip().split()).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eedbff9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/saeednajafi/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e334b6a-cfef-411f-a40b-6a43067ee652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_narrative_dataset() -> Any:\n",
    "    \"\"\"Read the narrative qa dataset.\"\"\"\n",
    "\n",
    "    def process_narrative_row(row: Any) -> Any:\n",
    "        \"\"\"Helper functions for NarrativeQA Dataset.\"\"\"\n",
    "        all_answers = list(set([white_space_fix(answer[\"text\"]) for answer in row[\"answers\"]]))\n",
    "        return {\n",
    "            \"context\": white_space_fix(row[\"question\"][\"text\"]),\n",
    "            \"question\": white_space_fix(row[\"document\"][\"summary\"][\"text\"]),\n",
    "            \"answers\": all_answers,\n",
    "        }\n",
    "\n",
    "    train_dataset = load_dataset(\"deepmind/narrativeqa\", split=\"train\")\n",
    "    dev_dataset = load_dataset(\"deepmind/narrativeqa\", split=\"validation\")\n",
    "    test_dataset = load_dataset(\"deepmind/narrativeqa\", split=\"test\")\n",
    "\n",
    "    train_dataset = train_dataset.map(\n",
    "        process_narrative_row,\n",
    "        remove_columns=[\"document\"],\n",
    "    )\n",
    "\n",
    "    dev_dataset = dev_dataset.map(\n",
    "        process_narrative_row,\n",
    "        remove_columns=[\"document\"],\n",
    "    )\n",
    "\n",
    "    test_dataset = test_dataset.map(\n",
    "        process_narrative_row,\n",
    "        remove_columns=[\"document\"],\n",
    "    )\n",
    "    return train_dataset, dev_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d88d552-f665-438b-a983-aa384a064251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_race_dataset() -> Any:\n",
    "    \"\"\"Function to create the race dataset.\"\"\"\n",
    "\n",
    "    def process_race_row(row: Any) -> Any:\n",
    "        \"\"\"Helper function.\"\"\"\n",
    "        option_code = row[\"answer\"]\n",
    "        if option_code == \"A\":\n",
    "            option_idx = 0\n",
    "        elif option_code == \"B\":\n",
    "            option_idx = 1\n",
    "        elif option_code == \"C\":\n",
    "            option_idx = 2\n",
    "        elif option_code == \"D\":\n",
    "            option_idx = 3\n",
    "\n",
    "        answers = [row[\"options\"][option_idx]]\n",
    "        return {\n",
    "            \"context\": white_space_fix(row[\"article\"]),\n",
    "            \"question\": white_space_fix(row[\"question\"]),\n",
    "            \"answers\": answers,\n",
    "        }\n",
    "\n",
    "    train_dataset = load_dataset(\"ehovy/race\", \"all\", split=\"train\")\n",
    "    train_dataset = train_dataset.map(\n",
    "        process_race_row,\n",
    "        remove_columns=[\"options\", \"example_id\", \"article\", \"answer\"],\n",
    "    )\n",
    "    dev_dataset = load_dataset(\"ehovy/race\", \"all\", split=\"validation\")\n",
    "    dev_dataset = dev_dataset.map(\n",
    "        process_race_row,\n",
    "        remove_columns=[\"options\", \"example_id\", \"article\", \"answer\"],\n",
    "    )\n",
    "    test_dataset = load_dataset(\"ehovy/race\", \"all\", split=\"test\")\n",
    "    test_dataset = test_dataset.map(\n",
    "        process_race_row,\n",
    "        remove_columns=[\"options\", \"example_id\", \"article\", \"answer\"],\n",
    "    )\n",
    "    return train_dataset, dev_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "338303eb-1147-42fc-b788-bd8f930fede2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_squad_dataset() -> Any:\n",
    "    def process_squad_row(row: Any) -> Any:\n",
    "        if row[\"answers\"][\"text\"]:\n",
    "            all_answers = list(set([white_space_fix(answer) for answer in row[\"answers\"][\"text\"]]))\n",
    "        else:\n",
    "            all_answers = [\"<no_answer>\"]\n",
    "        return {\n",
    "            \"context\": white_space_fix(row[\"context\"]),\n",
    "            \"question\": white_space_fix(row[\"question\"]),\n",
    "            \"answers\": all_answers,\n",
    "        }\n",
    "\n",
    "    train_dataset = load_dataset(\"rajpurkar/squad_v2\", split=\"train\")\n",
    "    train_dataset = train_dataset.map(\n",
    "        process_squad_row,\n",
    "        remove_columns=[\"id\", \"title\"],\n",
    "    )\n",
    "    dev_dataset = load_dataset(\"rajpurkar/squad_v2\", split=\"validation\")\n",
    "    dev_dataset = dev_dataset.map(\n",
    "        process_squad_row,\n",
    "        remove_columns=[\"id\", \"title\"],\n",
    "    )\n",
    "    return train_dataset, dev_dataset, dev_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7af4f85-5eae-48ba-a592-487f4939d9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc_train_dataset, rc_dev_dataset, rc_test_dataset = read_race_dataset()\n",
    "nq_train_dataset, nq_dev_dataset, nq_test_dataset = read_narrative_dataset()\n",
    "sq_train_dataset, sq_dev_dataset, sq_test_dataset = read_squad_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75b2adb5-f125-48fb-afd1-2109477ceb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11873\n",
      "11873\n",
      "11873\n",
      "11873\n",
      "11873\n",
      "4934\n",
      "4934\n",
      "4934\n",
      "4934\n",
      "4934\n",
      "10557\n",
      "10557\n",
      "10557\n",
      "10557\n",
      "10557\n",
      "4887\n",
      "4887\n",
      "4887\n",
      "4887\n",
      "4887\n",
      "3461\n",
      "3461\n",
      "3461\n",
      "3461\n",
      "3461\n"
     ]
    }
   ],
   "source": [
    "seeds = [42, 100, 13, 87, 21]\n",
    "few_shot_split_sizes = [16, 128, 1024, 8192, 0.1]\n",
    "dataset_names = [\"squad\", \"race\", \"narrativeqa\"]\n",
    "\n",
    "\n",
    "def write_eval(eval_dataset: Any, dataset_name: str, test_name: str) -> None:\n",
    "    eval_df = pd.DataFrame([row for row in eval_dataset])\n",
    "    for few_shot_split_size in few_shot_split_sizes:\n",
    "        print(len(eval_df))\n",
    "        Path(f\"./{few_shot_split_size}-shot-datasets/{dataset_name}\").mkdir(parents=True, exist_ok=True)\n",
    "        eval_df.to_csv(\n",
    "            f\"./{few_shot_split_size}-shot-datasets/{dataset_name}/{test_name}.tsv\",\n",
    "            header=True,\n",
    "            index=False,\n",
    "            sep=\"\\t\",\n",
    "        )\n",
    "\n",
    "\n",
    "write_eval(sq_test_dataset, \"squad\", \"original_validation\")\n",
    "\n",
    "write_eval(rc_test_dataset, \"race\", \"original_test\")\n",
    "write_eval(nq_test_dataset, \"narrativeqa\", \"original_test\")\n",
    "\n",
    "write_eval(rc_dev_dataset, \"race\", \"original_validation\")\n",
    "write_eval(nq_dev_dataset, \"narrativeqa\", \"original_validation\")\n",
    "\n",
    "\n",
    "def write_train_dev(train_dataset: Any, dataset_name: str) -> None:\n",
    "    train_rows = [row for row in train_dataset]\n",
    "    for seed in seeds:\n",
    "        random.seed(seed)\n",
    "        random.shuffle(train_rows)\n",
    "        for few_shot_split_size in few_shot_split_sizes:\n",
    "            if few_shot_split_size == 0.1:\n",
    "                new_split_size = int(math.ceil(len(train_rows) * 0.1))\n",
    "                fewshot_val_rows = train_rows[0:new_split_size]\n",
    "                fewshot_train_rows = train_rows[new_split_size:]\n",
    "            else:\n",
    "                new_split_size = int(few_shot_split_size)\n",
    "                fewshot_train_rows = train_rows[0:new_split_size]\n",
    "                fewshot_val_rows = train_rows[new_split_size : new_split_size * 2]\n",
    "\n",
    "            train_df = pd.DataFrame(fewshot_train_rows)\n",
    "            csv_file = f\"./{few_shot_split_size}-shot-datasets\"\n",
    "            Path(f\"{csv_file}/{dataset_name}\").mkdir(parents=True, exist_ok=True)\n",
    "            csv_file = f\"{csv_file}/{dataset_name}/{few_shot_split_size}-{seed}-train.tsv\"\n",
    "            train_df.to_csv(\n",
    "                csv_file,\n",
    "                header=True,\n",
    "                index=False,\n",
    "                sep=\"\\t\",\n",
    "            )\n",
    "            val_df = pd.DataFrame(fewshot_val_rows)\n",
    "            csv_file = f\"./{few_shot_split_size}-shot-datasets\"\n",
    "            Path(f\"{csv_file}/{dataset_name}\").mkdir(parents=True, exist_ok=True)\n",
    "            csv_file = f\"{csv_file}/{dataset_name}/{few_shot_split_size}-{seed}-dev.tsv\"\n",
    "            val_df.to_csv(\n",
    "                csv_file,\n",
    "                header=True,\n",
    "                index=False,\n",
    "                sep=\"\\t\",\n",
    "            )\n",
    "\n",
    "\n",
    "write_train_dev(sq_train_dataset, \"squad\")\n",
    "write_train_dev(rc_train_dataset, \"race\")\n",
    "write_train_dev(nq_train_dataset, \"narrativeqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b93e6644-f7b5-44a0-8dc6-6a95ae86fd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'context', 'answers'],\n",
      "    num_rows: 4934\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(rc_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c5c984a-63a2-4984-b24b-cea29a708754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answers', 'context'],\n",
      "    num_rows: 10557\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(nq_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c626110-7680-42fe-8284-74aa5055c10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['context', 'question', 'answers'],\n",
      "    num_rows: 11873\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(sq_test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
